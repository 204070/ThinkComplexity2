% LaTeX source for ``Think Complexity, 2nd edition''
% Copyright (c)  2016  Allen B. Downey.

% Permission is granted to copy, distribute, transmit and adapt
% this work under a Creative Commons
% Attribution-NonCommercial-ShareAlike 4.0 International (CC BY-NC-SA 4.0)
% https://creativecommons.org/licenses/by-nc-sa/4.0/

% If you are interested in distributing a commercial version of this
% work, please contact Allen Downey.

% The LaTeX source for this book is available from
% http://greenteapress.com/complexity


\documentclass[12pt]{book}

\title{Think Complexity}
\author{Allen B. Downey}

\newcommand{\thetitle}{Think Complexity}
\newcommand{\thesubtitle}{Exploring Complexity Science in Python}
\newcommand{\theauthors}{Allen B. Downey}
\newcommand{\theversion}{2.0.4}

%%%% Both LATEX and PLASTEX

\usepackage{graphicx}
\usepackage{hevea}
\usepackage{makeidx}
\usepackage{setspace}

\makeindex

% automatically index glossary terms
\newcommand{\term}[1]{%
\item[#1:]\index{#1}}

\usepackage{amsmath}
\usepackage{amsthm}

% format end of chapter excercises
\newtheoremstyle{exercise}
  {12pt}        % space above
  {12pt}        % space below
  {}            % body font
  {}            % indent amount
  {\bfseries}   % head font
  {}            % punctuation
  {12pt}        % head space
  {}            % custom head
\theoremstyle{exercise}
\newtheorem{exercise}{Exercise}[chapter]

\newif\ifplastex
\plastexfalse

%%%% PLASTEX ONLY
\ifplastex

\usepackage{localdef}

\usepackage{url}

\newcount\anchorcnt
\newcommand*{\Anchor}[1]{%
  \@bsphack%
    \Hy@GlobalStepCount\anchorcnt%
    \edef\@currentHref{anchor.\the\anchorcnt}%
    \Hy@raisedlink{\hyper@anchorstart{\@currentHref}\hyper@anchorend}%
    \M@gettitle{}\label{#1}%
    \@esphack%
}

% code listing environments:
% we don't need these for plastex because they get replaced
% by preprocess.py
%\newenvironment{code}{\begin{code}}{\end{code}}
%\newenvironment{stdout}{\begin{code}}{\end{code}}

% inline syntax formatting
\newcommand{\py}{\verb}%}

%%%% LATEX ONLY
\else

\input{latexonly}

\fi

%%%% END OF PREAMBLE
\begin{document}

\frontmatter

%%%% PLASTEX ONLY
\ifplastex

\maketitle

%%%% LATEX ONLY
\else

\begin{latexonly}

%-half title--------------------------------------------------
\thispagestyle{empty}

\begin{flushright}
\vspace*{2.0in}

\begin{spacing}{3}
{\huge \thetitle}
\end{spacing}

\vspace{0.25in}

Version \theversion

\vfill

\end{flushright}

%--verso------------------------------------------------------

\newpage
\newpage
%\clearemptydoublepage
%\pagebreak
%\thispagestyle{empty}
%\vspace*{6in}

%--title page--------------------------------------------------
\pagebreak
\thispagestyle{empty}

\begin{flushright}
\vspace*{2.0in}

\begin{spacing}{3}
{\huge \thetitle}
\end{spacing}

\vspace{0.25in}

Version \theversion

\vspace{1in}


{\Large
\theauthors \\
}


\vspace{0.5in}

{\Large Green Tea Press}

{\small Needham, Massachusetts}

%\includegraphics[width=1in]{figs/logo1.eps}
\vfill

\end{flushright}


%--copyright--------------------------------------------------
\pagebreak
\thispagestyle{empty}

Copyright \copyright ~2016 \theauthors.



\vspace{0.2in}

\begin{flushleft}
Green Tea Press       \\
9 Washburn Ave \\
Needham MA 02492
\end{flushleft}

Permission is granted to copy, distribute, transmit and adapt
this work under a Creative Commons
Attribution-NonCommercial-ShareAlike 4.0 International License:
\url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.

If you are interested in distributing a commercial version of this
work, please contact the author.

The \LaTeX\ source for this book is available from

\begin{code}
      http://greenteapress.com/complexity
\end{code}

%--table of contents------------------------------------------

\cleardoublepage
\setcounter{tocdepth}{1}
\tableofcontents

\end{latexonly}


% HTML title page------------------------------------------

\begin{htmlonly}

\vspace{1em}

{\Large \thetitle}

{\large \theauthors}

Version \theversion

\vspace{1em}

Copyright \copyright ~2016 \theauthors.

Permission is granted to copy, distribute, and/or modify this work
under the terms of the Creative Commons
Attribution-NonCommercial-ShareAlike 4.0 International License, which is
available at \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.

\vspace{1em}

\setcounter{chapter}{-1}

\end{htmlonly}

% END OF THE PART WE SKIP FOR PLASTEX
\fi

\chapter{Preface}
\label{preface}

\section{Why I wrote this book}

This book is inspired by boredom and fascination: boredom with the
usual presentation of data structures and algorithms, and
fascination with complex systems.  The problem with data structures
is that they are often taught without a motivating context; the
problem with complexity science is that it is usually not taught at
all.

In 2005 I developed a new class at Olin College where students read
about topics in complexity, implemented experiments in Python, and learned
about algorithms and data structures.  I wrote the first draft of this
book when I taught the class again in 2008.

For the third offering, in 2011, I prepared the book for publication
and invited the students to submit their work, in the form of case
studies, for inclusion in the book.  I recruited 9 professors at Olin
to serve as a program committee and choose the reports that were ready
for publication.  The case studies that met the standard are included
in this book.  For the next edition, we invite additional submissions
from readers (see Appendix~\ref{submissions}).

\section{Suggestions for teachers}

This book is intended as a scaffold for an intermediate-level college
class in Python programming and algorithms.  My class uses the following
structure:

\begin{description}

\item[Reading] Complexity science is a collection of diverse topics.
  There are many interconnections, but it takes time to see them.  To
  help students see the big picture, I give them readings from
  popular presentations of work in the field.  My reading
  list, and suggestions on how to use it, are in
  Appendix~\ref{reading}.

\item[Exercises] This book presents a series of exercises; many of
  them ask students to reimplement seminal experiments and extend
  them.  One of the attractions of complexity is that the
  research frontier is accessible with moderate programming skills and
  undergraduate mathematics.

\item[Discussion] The topics in this book raise questions in the
  philosophy of science; these topics lend themselves to further
  reading and classroom discussion.

\item[Case studies] In my class, we spend almost half the semester on
  case studies.  Students participate in an idea generation process,
  form teams, and work for 6-7 weeks on a series of experiments, then
  present them in the form of a publishable 4-6 page report.

\end{description}

An outline of the course and my notes are available at
\url{https://sites.google.com/site/compmodolin}.


\section{Suggestions for autodidacts}

In 2009-10 I was a Visiting Scientist at Google, working in their
Cambridge office.  One of the things that impressed me about the
software engineers I worked with was their broad intellectual
curiosity and drive to expand their knowledge and skills.

I hope this book helps people like them explore a set of topics
and ideas they might not encounter otherwise, practice programming
skills in Python, and learn more about data structures and
algorithms (or review material that might have been less engaging
the first time around).

Some features of this book intended for autodidacts are:

\begin{description}

\item[Technical depth] There are many books about complex systems,
  but most are written for a popular audience.  They usually skip the
  technical details, which is frustrating for people who can handle
  it.  This book presents the mathematics and other technical content
  you need to really understand this work.

\item[Further reading] Throughout the book, I include pointers to
  further reading, including original papers (most of which are
  available electronically) and related articles from
  Wikipedia\footnote{Some professors have an allergic reaction to
    Wikipedia, on the grounds that students depend too heavily on an
    unreliable source.  Since many of my references are Wikipedia
    articles, I want to explain my thinking.  First, the articles on
    complexity science and related topics tend to be very good;
    second, they are written at a level that is accessible after you
    have read this book (but sometimes not before); and finally, they
    are freely available to readers all over the world.  If there is a
    danger in sending readers to these references, it is not that they
    are unreliable, but that the readers won't come back!  (see
    \url{http://xkcd.com/903}).} and other sources.

\item[Exercises and (some) solutions] For many of the exercises, I
  provide code to get you started, and solutions if you get stuck or
  want to compare your code to mine.

\item[Opportunity to contribute] If you explore a topic not covered in
  this book, reimplement an interesting experiment, or perform one of
  your own, I invite you to submit a case study for possible inclusion
  in the next edition of the book.  See Appendix~\ref{submissions} for
  details.

\end{description}


\section{Prerequisites}


\section{Using the code}



\begin{flushleft}
Allen B. Downey \newline
Professor of Computer Science \newline
Olin College of Engineering \newline
Needham, MA
\end{flushleft}


\section*{Contributor List}

\index{contributors}

If you have a suggestion or correction, please send email to
{\tt downey@allendowney.com}.  If I make a change based on your
feedback, I will add you to the contributor list
(unless you ask to be omitted).
\index{contributors}

If you include at least part of the sentence the
error appears in, that makes it easy for me to search.  Page and
section numbers are fine, too, but not quite as easy to work with.
Thanks!

\small

\begin{itemize}

\item Richard Hollands pointed out several typos.

\item John Harley, Jeff Stanton, Colden Rouleau and
Keerthik Omanakuttan are Computational Modeling students who
pointed out typos.

\item Muhammad Najmi bin Ahmad Zabidi caught some typos.

\item Phillip Loh, Corey Dolphin, Noam Rubin and Julian Ceipek
found typos and made helpful suggestions.

\item Jose Oscar Mur-Miranda found several typos.

\item I am grateful to the program committee that read and selected
the case studies included in this book: 
Sarah Spence Adams,
John Geddes,
Stephen Holt,
Vincent Manno,
Robert Martello,
Amon Millner,
Jos\'{e} Oscar Mur-Miranda,
Mark Somerville, and
Ursula Wolz.

\item Sebastian Sch\"{o}ner sent two pages of typos!

\item Jonathan Harford found a code error.

\item Philipp Marek sent a number of corrections.

\item Alex Hantman found a missing word.

% ENDCONTRIB

\end{itemize}

\normalsize

\mainmatter

\chapter{Complexity Science}
\label{overview}

\section{What is this book about?}

This book is about data structures and algorithms, intermediate
programming in Python, computational modeling and the philosophy of
science:

\begin{description}

\item[Data structures and algorithms:] A data structure is a
  collection of data elements organized in a way that supports
  particular operations.  For example, a Python dictionary organizes
  key-value pairs in a way that provides fast mapping from keys to
  values, but mapping from values to keys is slower.
\index{data structure}
\index{algorithm}

An algorithm is a mechanical process for performing a computation.
Designing efficient programs often involves the co-evolution of data
structures and the algorithms that use them.  For example, in the
first few chapters I present graphs, data structures that
implement graphs, and graph algorithms based on those data structures.
\index{graph algorithm}

\item[Python programming:] This book picks up where {\em Think Python}
  leaves off.  I assume that you have read that book or have
  equivalent knowledge of Python.  I try to emphasize fundamental ideas
  that apply to programming in many languages, but along the way you
  will learn some useful features that are specific to Python.
\index{Think Python@{\em Think Python}}

\item[Computational modeling:] A model is a simplified
  description of a system used for simulation or analysis.
  Computational models are designed to take advantage of cheap, fast
  computation.
\index{modeling}

\item[Philosophy of science:] The experiments and results in this book
  raise questions relevant to the philosophy of science,
  including the nature of scientific laws, theory choice, realism and
  instrumentalism, holism and reductionism, and epistemology.
\index{philosophy}

\end{description}

This book is also about {\bf complexity science}, which is an
interdisciplinary field---at the intersection of mathematics, computer
science and natural science---that focuses on discrete models of
physical systems.  \index{complexity science}
In particular, it focuses on {\bf complex systems}, which are
systems with many interacting components.  
\index{complexity science}
\index{complex systems}

Complex systems include networks and graphs, cellular automata,
agent-based models and swarms, fractals and self-organizing
systems, chaotic systems and cybernetic systems.  These terms
might not mean much to you at this point.  We will get to them
soon, but you can get a preview at
\url{http://en.wikipedia.org/wiki/Complex_systems}.


\section{A new kind of science}

In 2002 Stephen Wolfram published {\em A New Kind of Science} where he
presents his and others' work on cellular automata and describes a
scientific approach to the study of computational systems.  We'll get
back to Wolfram in Chapter~\ref{automata}, but I want to borrow his
title for something a little broader.
\index{Wolfram, Stephen}
\index{New Kind of Science@{\it A New Kind of Science}}

I think complexity is a ``new kind of science'' not because
it applies the tools of science to a new subject, but because it
uses different tools, allows different kinds of work, and ultimately
changes what we mean by ``science.''

To demonstrate the difference, I'll start with an example of classical
science: suppose someone asked you why planetary orbits are
elliptical.  You might invoke Newton's law of universal
gravitation and use it to write a differential equation that describes
planetary motion.  Then you could solve the differential equation and
show that the solution is an ellipse.  QED!
\index{gravitation}
\index{planetary motion}

Most people find this kind of explanation satisfying.  It includes a
mathematical derivation---so it has some of the rigor of a proof---and
it explains a specific observation, elliptical orbits, by appealing to
a general principle, gravitation.
\index{proof}
\index{natural law}

Let me contrast that with a different kind of explanation.  Suppose
you move to a city like Detroit that is racially segregated, and you
want to know why it's like that.  If you do some research, you might
find a paper by Thomas Schelling called ``Dynamic Models of
Segregation,'' which proposes a simple model of racial segregation
(a copy is available from \url{http://statistics.berkeley.edu/~aldous/157/Papers/Schelling_Seg_Models.pdf}).
\index{Schelling}
\index{segregation}
\index{Detroit}

Here is a summary of the paper (from Chapter~\ref{agent-based}):

\begin{quote}
The Schelling model of the city is an array of cells where each cell
represents a house.  The houses are occupied by two kinds of
``agents,'' labeled red and blue, in roughly equal numbers.  About
10\% of the houses are empty.  \index{Schelling, Thomas}
\index{agent}

At any point in time, an agent might be happy or unhappy, depending
on the other agents in the neighborhood.
In one version of the model, agents are happy if they have at least
two neighbors like themselves, and unhappy if they have one or zero.
\index{agent-based model}

The simulation proceeds by choosing an agent at random and checking
to see whether it is happy.  If so, nothing happens; if not,
the agent chooses one of the unoccupied cells at
random and moves.
\end{quote}

If you start with a simulated city that is entirely unsegregated and
run the model for a short time, clusters of similar agents appear.  As
time passes, the clusters grow and coalesce until there are a small
number of large clusters and most agents live in homogeneous
neighborhoods. \index{segregation}

The degree of segregation in the model is surprising, and it suggests
an explanation of segregation in real cities.  Maybe Detroit is
segregated because people prefer not to be greatly outnumbered and
will move if the composition of their neighborhoods makes them
unhappy.  \index{racism} \index{xenophobia}

Is this explanation satisfying in the same way as the explanation of
planetary motion?  Most people would say not, but why?

Most obviously, the Schelling model is highly abstract, which is to
say not realistic.  It is tempting to say that people are more complicated
than planets, but when you think about it, planets are just as complicated
as people (especially the ones that {\em have} people).
\index{abstract model}

Both systems are complicated, and both models are based on
simplifications; for example, in the model of planetary motion we
include forces between the planet and its sun, and ignore interactions
between planets.
\index{simplification}

The important difference is that, for planetary motion, we can defend
the model by showing that the forces we ignore are smaller than the
ones we include.  And we can extend the model to include other
interactions and show that the effect is small.  For Schelling's model
it is harder to justify the simplifications.
\index{justification}

To make matters worse, Schelling's model doesn't appeal to any
physical laws, and it uses only simple computation, not mathematical
derivation.  Models like Schelling's don't look like classical
science, and many people find them less compelling, at least at first.
But as I will try to demonstrate, these models do useful work,
including prediction, explanation, and design.  One of the goals of
this book is to explain how.
\index{modeling}


\section{Paradigm shift?}

When I describe this book to people, I am often asked if this new kind
of science is a paradigm shift.  I don't think so, and here's why.
\index{paradigm shift}

Thomas Kuhn introduced the term ``paradigm shift'' in {\em The
Structure of Scientific Revolutions} in 1962.  It refers to a process
in the history of science where the basic assumptions of a field
change, or where one theory is replaced by another.
He presents as examples the Copernican revolution, the displacement
of phlogiston by the oxygen model of combustion, and the emergence
of relativity.
\index{Kuhn, Thomas}
\index{Structure of Scientific Revolutions@{\it The Structure of Scientific Revolutions}}

The development of complexity science is not the replacement of
an older model, but (in my opinion) a gradual shift in the criteria
models are judged by, and in the kinds of models that are considered
acceptable.
\index{complexity science}

For example, classical models tend to be law-based, expressed in the
form of equations, and solved by mathematical derivation.  Models that
fall under the umbrella of complexity are often rule-based,
expressed as computations, and simulated rather than analyzed.

Not everyone finds these models satisfactory.  For example, in
{\em Sync}, Steven Strogatz writes about his model of spontaneous
synchronization in some species of fireflies.  He presents a
simulation that demonstrates the phenomenon, but then writes:
\index{Strogatz, Steven}
\index{Sync@{\it Sync}}
\index{fireflies}
\index{synchronization}

\begin{quote}
I repeated the simulation dozens of times, for other random
initial conditions and for other numbers of oscillators.  Sync
every time. [...] The challenge now was to prove it.  Only an
ironclad proof would demonstrate, in a way that no computer ever
could, that sync was inevitable; and the best kind of proof would
clarify {\em why} it was inevitable.
\end{quote}

Strogatz is a mathematician, so his enthusiasm for proofs is
understandable, but his proof doesn't address what is, to me, the
most interesting part the phenomenon.  In order to prove that ``sync
was inevitable,'' Strogatz makes several simplifying assumptions, in
particular that each firefly can see all the others.
\index{proof}

In my opinion, it is more interesting to explain how an entire valley
of fireflies can synchronize {\em despite the fact that they cannot
  all see each other}.  How this kind of global behavior emerges from
local interactions is the subject of Chapter~\ref{agent-based}.
Explanations of these phenomena often use agent-based models, which
explore (in ways that would be difficult or impossible with
mathematical analysis) the conditions that allow or prevent
synchronization.

I am a computer scientist, so my enthusiasm for computational models
is probably no surprise.  I don't mean to say that Strogatz is wrong,
but rather that people disagree about what questions to ask and what
tools to use to answer them.  These decisions are based on value
judgments, so there is no reason to expect agreement.
\index{computational model}

Nevertheless, there is rough consensus among scientists
about which models are considered good science, and which others
are fringe science, pseudoscience, or not science at all.
\index{fringe science}
\index{pseudoscience}

I claim, and this is a central thesis of this book, that the
criteria this consensus is based on change over time, and that
the emergence of complexity science reflects a gradual shift in
these criteria.


\section{The axes of scientific models}

I have described classical models as based on physical laws, expressed
in the form of equations, and solved by mathematical analysis;
conversely, models of complexity systems are often based on simple
rules and implemented as computations.
\index{criteria for models}

We can think of this trend as a shift over time along two axes:

\begin{description}

\item[Equation-based $\rightarrow$ simulation-based] \quad

\item[Analysis $\rightarrow$ computation] \quad

\end{description}

The new kind of science is different in several other
ways.  I present them here so you know what's coming, but some of them
might not make sense until you have seen the examples later in the
book.

\begin{description}

\item[Continuous $\rightarrow$ discrete] Classical models tend to be
  based on continuous mathematics, like calculus; models of complex
  systems are often based on discrete mathematics, including graphs and
  cellular automata.
\index{continuous}
\index{discrete}

\item[Linear $\rightarrow$ non-linear] Classical models are often
  linear, or use linear approximations to non-linear systems;
  complexity science is more friendly to non-linear models.  One example
  is chaos theory\footnote{Chaos is not covered in this book, but you can
  read about it at \url{http://en.wikipedia.org/wiki/Chaos}.}.
\index{linear}
\index{non-linear}

\item[Deterministic $\rightarrow$ stochastic] Classical models are
  usually deterministic, which may reflect underlying philosophical
  determinism, discussed in Chapter~\ref{automata}; complex models
  often feature randomness.
\index{deterministic}
\index{stochastic}

\item[Abstract $\rightarrow$ detailed] In classical models, planets are
  point masses, planes are frictionless, and cows are
  spherical (see \url{http://en.wikipedia.org/wiki/Spherical_cow}).
  Simplifications like these are often necessary for analysis,
  but computational models can be more realistic.
\index{spherical cow}
\index{cow, spherical}

\item[One, two $\rightarrow$ many] In celestial mechanics, the
  two-body problem can be solved analytically; the three-body problem
  cannot.  Where classical models are often limited to
  small numbers of interacting elements, complexity science works with
  larger complexes (which is where the name comes from).
\index{one, two, many}

\item[Homogeneous $\rightarrow$ composite] In classical models, the
  elements tend to be interchangeable; complex models more often
  include heterogeneity.
\index{homogeneous}
\index{composite}

\end{description}

These are generalizations, so we should not take them
too seriously.  And I don't mean to deprecate
classical science.  A more complicated model is not necessarily better;
in fact, it is usually worse.

Also, I don't mean to say that these changes are abrupt or complete.
Rather, there is a gradual migration in the frontier of what is
considered acceptable, respectable work.  Some tools that used
to be regarded with suspicion are now common, and some 
models that were widely accepted are now regarded with scrutiny.

For example, when Appel and Haken proved the four-color theorem in
1976, they used a computer to enumerate 1,936 special cases that were,
in some sense, lemmas of their proof.  At the time, many
mathematicians did not consider the theorem truly proved.  Now
computer-assisted proofs are common and generally (but not
universally) accepted.
\index{Appel, Kenneth}
\index{Hacken, Wolfgang}
\index{four-color theorem}

Conversely, a substantial body of economic analysis is based on a
model of human behavior called ``Economic man,'' or, with tongue in
cheek, {\it Homo economicus}.  Research based on this model was
highly regarded for several decades, especially if it involved
mathematical virtuosity.  More recently, this model is treated with
skepticism, and models that include imperfect information and
bounded rationality are hot topics.
\index{economic man}
\index{Homo economicus}
\index{economics}

\section{A new kind of model}

Complex models are often appropriate for different purposes and
interpretations:
\index{complex model}

\begin{description}

\item[Predictive $\rightarrow$ explanatory] Schelling's model
of segregation might shed light on a complex social phenomenon, but
it is not useful for prediction.  On the other hand, a simple model
of celestial mechanics can predict solar eclipses, down to the second,
years in the future.
\index{predictive model}
\index{explanatory model}

\item[Realism $\rightarrow$ instrumentalism] Classical models lend
  themselves to a realist interpretation; for example, most people
  accept that electrons are real things that exist.  Instrumentalism
  is the view that models can be useful even if the entities they
  postulate don't exist.  George Box wrote what might be the motto of
  instrumentalism: ``All models are wrong, but some are useful."
\index{realism}
\index{instrumentalism}

\item[Reductionism $\rightarrow$ holism] Reductionism is the view that
  the behavior of a system can be explained by understanding its
  components.  For example, the periodic table of the elements is a
  triumph of reductionism, because it explains the chemical behavior
  of elements with a simple model of the electrons in an atom.  Holism
  is the view that some phenomena that appear at the system level do
  not exist at the level of components, and cannot be explained in
  component-level terms.
\index{reductionism}
\index{holism}

\end{description}

We get back to explanatory models in Chapter~\ref{scale-free},
instrumentalism in Chapter~\ref{life}, and holism in Chapter~\ref{soc}.


\section{A new kind of engineering}

I have been talking about complex systems in the context of science,
but complexity is also a cause, and effect, of
changes in engineering and the organization of social
systems:
\index{engineering}

\begin{description}

\item[Centralized $\rightarrow$ decentralized] Centralized systems are
  conceptually simple and easier to analyze, but decentralized systems
  can be more robust.  For example, in the World Wide Web clients send
  requests to centralized servers; if the servers are down, the
  service is unavailable.  In peer-to-peer networks, every node is
  both a client and a server.  To take down the service, you have to
  take down {\em every} node.
\index{centralized}
\index{decentralized}
\index{client-server architecture}
\index{peer-to-peer architecture}
  
\item[Isolation $\rightarrow$ interaction] In classical engineering,
  the complexity of large systems is managed by isolating components
  and minimizing interactions.  This is still an important engineering
  principle; nevertheless, the availability of cheap computation makes
  it increasingly feasible to design systems with complex interactions
  between components.
\index{isolation}
\index{interaction}

\item[One-to-many $\rightarrow$ many-to-many] In many communication
  systems, broadcast services are being augmented, and sometimes
  replaced, by services that allow users to communicate with each
  other and create, share, and modify content.  
\index{broadcast service}

\item[Top-down $\rightarrow$ bottom-up] In social, political and
  economic systems, many activities that would normally be centrally
  organized now operate as grassroots movements.  Even armies, which
  are the canonical example of hierarchical structure, are moving
  toward devolved command and control.
\index{top-down}
\index{bottom-up}
\index{grassroots}

\item[Analysis $\rightarrow$ computation] In classical engineering,
  the space of feasible designs is limited by our capability for
  analysis.  For example, designing the Eiffel Tower was possible
  because Gustave Eiffel developed novel analytic techniques, in
  particular for dealing with wind load.  Now tools for computer-aided
  design and analysis make it possible to build almost anything that
  can be imagined.  Frank Gehry's Guggenheim Museum Bilbao is my
  favorite example.
\index{analysis}
\index{computation}
\index{Eiffel Tower}
\index{Eiffel, Gustave}
\index{Gehry, Frank}
\index{Guggenheim Museum Bilbao}

\item[Design $\rightarrow$ search] Engineering is sometimes described
  as a search for solutions in a landscape of possible designs.
  Increasingly, the search process can be automated.  For example,
  genetic algorithms explore large design spaces and discover
  solutions human engineers would not imagine (or like).  The ultimate
  genetic algorithm, evolution, notoriously generates designs that
  violate the rules of human engineering.
\index{design}
\index{search}

\end{description}


\section{A new kind of thinking}

We are getting farther afield now, but the shifts I am postulating
in the criteria of scientific modeling are related to 20th Century
developments in logic and epistemology.
\index{logic}
\index{epistemology}

\begin{description}

\item[Aristotelian logic $\rightarrow$ many-valued logic] In
  traditional logic, any proposition is either true or false.  This
  system lends itself to math-like proofs, but fails (in dramatic
  ways) for many real-world applications.  Alternatives include
  many-valued logic, fuzzy logic, and other systems designed to handle
  indeterminacy, vagueness, and uncertainty.  Bart
  Kosko discusses some of these systems in {\em Fuzzy
    Thinking}.
\index{Aristotelian logic}
\index{many-valued logic}
\index{Kosko, Bart}
\index{Fuzzy Thinking@{\it Fuzzy Thinking}}
\index{uncertainty}

\item[Frequentist probability $\rightarrow$ Bayesianism] Bayesian
  probability has been around for centuries, but was not widely used
  until recently, facilitated by the availability of cheap computation
  and the reluctant acceptance of subjectivity
  in probabilistic claims.  Sharon Bertsch McGrayne presents this
  history in {\em The Theory That Would Not Die}.
\index{frequentist}
\index{Bayesian}
\index{McGrayne, Sharon Bertsch}
\index{Theory That Would Not Die, The@{\it The Theory That Would Not Die}}

\item[Objective $\rightarrow$ subjective] The Enlightenment, and
  philosophic modernism, are based on belief in objective truth; that
  is, truths that are independent of the people that hold them.  20th
  Century developments including quantum mechanics, G\"{o}del's
  Incompleteness Theorem, and Kuhn's study of the history of science
  called attention to seemingly unavoidable subjectivity in
  even ``hard sciences'' and mathematics.  Rebecca Goldstein presents
  the historical context of G\"{o}del's proof in {\it Incompleteness}.
\index{objective}
\index{subjective}
\index{Kuhn, Thomas}
\index{Godel's Incompleteness Theorem@G\"{o}del's Incompleteness Theorem}
\index{incompleteness}
\index{Goldstein, Rebecca}
\index{Incompleteness@{\it Incompleteness}}

\item[Physical law $\rightarrow$ theory $\rightarrow$ model]
  Some people distinguish between laws, theories, and models, but
  I think they are the same thing.
  People who use ``law'' are likely to
  believe that it is objectively true and immutable; people who use
  ``theory'' concede that it is subject to revision; and ``model''
  concedes that it is based on simplification and approximation.
\index{physical law}
\index{theory}
\index{model}

  Some concepts that are called ``physical laws'' are really
  definitions; others are, in effect, the assertion that a model
  predicts or explains the behavior of a system particularly well.
  We come back to the nature of physical laws 
  in Sections~\ref{model1}, ~\ref{model3} and \ref{model2}.

\item[Determinism $\rightarrow$ indeterminism] Determinism is the view
  that all events are caused, inevitably, by prior events.  Forms of
  indeterminism include randomness, probabilistic causation, and
  fundamental uncertainty.  We come back to this
  topic in Section~\ref{determinism} and \ref{free.will}
\index{determinism}
\index{indeterminism}
\index{free will}

\end{description}

These trends are not universal or complete, but the center of
opinion is shifting along these axes.  As evidence, consider the
reaction to Thomas Kuhn's {\em The Structure of Scientific
  Revolutions}, which was reviled when it was published and
now considered almost uncontroversial.
\index{Kuhn, Thomas}
\index{Structure of Scientific Revolutions@{\it The Structure of Scientific Revolutions}}

These trends are both cause and effect of complexity science.  For
example, highly abstracted models are more acceptable now because of
the diminished expectation that there should be a unique, correct model
for every system.  Conversely, developments in complex systems
challenge determinism and the related concept of physical law.

This chapter is an overview of the themes coming up in the book, but
not all of it will make sense before you see the examples.  When you
get to the end of the book, you might find it helpful to read this
chapter again.


\chapter{Graphs}

\newcommand{\Erdos}{Erd\H{o}s}
\newcommand{\Renyi}{R\'{e}nyi}

The first three chapters of this book are about models that describe
systems that are made up of components and connections between components.
For example, in an ecological food web, the components are species
and the connections represent predator-prey relationships.

In this chapter, I introduce NetworkX, a Python package for building
and studying these models.  We start with the \Erdos-\Renyi~model,
which has some interesting mathematical properties.  In the next
chapter we move on to models that are more useful for explaining
real-world systems.

The code for this chapter is in {\tt chap02.ipynb} in the respository
for this book.  More information about working with the code is
in Section~\ref{???}.


\section{What is a graph?}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap02-1.pdf}}
\caption{A directed graph that represents a social network.}
\label{chap02-1}
\end{figure}

To most people a graph is a visual representation of a data set, like
a bar chart or a plot of stock prices over time.  That's not what this
chapter is about.  \index{graph}

In this chapter, a {\bf graph} is a representation of
a system that contains discrete, interconnected elements.  The
elements are represented by {\bf nodes}
and the interconnections are represented by {\bf edges}.
\index{node}
\index{edge}
\index{vertex}

For example, you could represent a road map with a node for each
city and an edge for each road between cities.  Or you could
represent a social network using a node for each person, with an
edge between two people if they are friends and no edge otherwise.
\index{road network}
\index{social network}

In some graphs, edges have attributes like length, cost, or weight.
For example, in a road map, the length of an edge might represent the
distance between two cities, or the travel time.  In a
social network there might be different kinds of edges to represent
different kinds of relationships: friends, business associates, etc.
\index{edge weight} \index{weight}

Edges may be {\bf directed} or {\bf undirected}, depending on whether
the relationships they represent are asymmetric or symmetric.  In a
road map, you might represent a one-way street with a directed edge
and a two-way street with an undirected edge.  In some social
networks, like Facebook, friendship is symmetric: if $A$ is friends
with $B$ then $B$ is friends with $A$.  But in Twitter, for example,
the ``follows'' relationship is not symmetric; if $A$ follows $B$,
that doesn't imply that $B$ follows $A$.  So you might use undirected
edges to represent a Facebook network and directed edges for Twitter.
\index{directed graph} \index{undirected graph}

Graphs have interesting mathematical properties, and
there is a branch of mathematics called {\bf graph theory}
that studies them.
\index{graph theory}

Graphs are also useful, because there are many real world
problems that can be solved using {\bf graph algorithms}.
For example, Dijkstra's shortest path algorithm is an efficient
way to find the shortest path from a node to all
other nodes in a graph.  A {\bf path} is a sequence of nodes
with an edge between each consecutive pair.
\index{graph algorithm}
\index{path}

Graphs are usually drawn with squares or circles for nodes and lines
for edges.  For example, the directed graph in Figure~\ref{chap02-1}
might represent three people who ``follow'' each other on Twitter.
The thick part of the line indicates edge direction.
In this example, Alice and Bob follow each other and both follow
Chuck, but Chuck follows no one.
\index{representing graphs}

The undirected graph in Figure~\ref{chap02-2} shows four cities
in the northeast United States; the labels on the edges
indicate driving time in hours.
In this example the placement of the nodes corresponds
roughly to the geography of the cities, but in general the layout
of a graph is arbitrary.
\index{graph layout}


\section{NetworkX}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap02-2.pdf}}
\caption{An undirected graph that represents cities and highways.}
\label{chap02-2}
\end{figure}

To represent graphs, we'll use a package called NetworkX,
which is the most commonly used network library in Python.
You can read more about it at \url{https://networkx.github.io/},
but I'll explain it as we go along.

We can create a directed graph by importing NetworkX and instantiating
\py{nx.DiGraph}:

\begin{code}
import networkx as nx
G = nx.DiGraph()
\end{code}

It is conventional to import NetworkX as \py{nx}.  At this
point, \py{G} is a \py{DiGraph} object that contains no nodes
and no edges.  We can add nodes using the \py{add_node}method:

\begin{code}
G.add_node('Alice')
G.add_node('Bob')
G.add_node('Chuck')
\end{code}

Now we can use the \py{nodes} method to get the list of nodes:

\begin{code}
>>> G.nodes()
['Alice', 'Bob', 'Chuck']
\end{code}

Adding edges works pretty much the same way:

\begin{code}
G.add_edge('Alice', 'Bob')
G.add_edge('Alice', 'Chuck')
G.add_edge('Bob', 'Alice')
G.add_edge('Bob', 'Chuck')
\end{code}

And we can use \py{edges} to get the list of edges:

\begin{code}
>>> G.edges()
[('Alice', 'Bob'), ('Alice', 'Chuck'), 
 ('Bob', 'Alice'), ('Bob', 'Chuck')]
\end{code}

NetworkX provides several functions for drawing graphs;
\py{draw_circular}arranges the nodes in a circle and connects them
with edges:

\begin{code}
nx.draw_circular(G, 
                 node_color=COLORS[0], 
                 node_size=2000, 
                 with_labels=True)
\end{code}

And that's the code I use to generate Figure~\ref{chap02-1}.
The option \py{with_labels}causes the nodes to be labeled;
in the next example we'll see how to label the edges.

To generate Figure~\ref{chap02-2}, I start with a dictionary
that maps from each city name to its approximate longitude
and latitude:

\begin{code}
pos = dict(Albany=(-74, 43),
           Boston=(-71, 42),
           NYC=(-74, 41),
           Philly=(-75, 40))
\end{code}

Since this is an undirected graph, I instantiate \py{nx.Graph}:

\begin{code}
G = nx.Graph()
\end{code}

Then I can use \py{add_nodes_from}to iterate the keys of
\py{pos} and add them as nodes:

\begin{code}
G.add_nodes_from(pos)
\end{code}

Next I'll make a dictionary that maps from each edge to the corresponding
driving time:

\begin{code}
drive_times = {('Albany', 'Boston'): 3,
               ('Albany', 'NYC'): 4,
               ('Boston', 'NYC'): 4,
               ('NYC', 'Philly'): 2}
\end{code}

Now I can use \py{add_edges_from}, which iterates the keys of
\py{drive_times} and adds them as edges:

\begin{code}
G.add_edges_from(drive_times)
\end{code}

Now instead of \py{draw_circular}, which arranges the nodes in 
a circle, I'll use \py{draw}, which takes \py{pos} as the second
parameter:

\begin{code}
nx.draw(G, pos, 
        node_color=COLORS[1], 
        node_shape='s', 
        node_size=2500, 
        with_labels=True)
\end{code}

\py{pos} is a dictionary that maps from each city to its coordinates;
\py{draw} uses it to determine the locations of the nodes.

To add the edge labels, we use \py{draw_networkx_edge_labels}:

\begin{code}
nx.draw_networkx_edge_labels(G, pos, 
                             edge_labels=drive_times)
\end{code}

\py{drive_times} is a dictionary that maps from each edge, represented
by a pair of city names, to the driving distance between them.
And that's how I generated Figure~\ref{chap02-2}.

In both of these examples, the nodes are strings, but in general they
can be any hashable type.


\section{Random graphs}

A random graph is just what it sounds like: a graph with nodes and edges
generated at random.  Of course, there are many random processes that
can generate graphs, so there are many kinds of random graphs.
\index{random graph}

One of the more interesting kinds is the \Erdos-\Renyi~model, studied
by Paul \Erdos~and Alfr\'{e}d \Renyi~in the 1960s.
\index{Renyi, Alfred@\Renyi, Afr\'{e}d}
\index{Erdos, Paul@\Erdos, Paul}

An \Erdos-\Renyi~graph (ER graph) is characterized by two parameters:
$n$ is the number of nodes and $p$ is the probability that there
is an edge between any two nodes.
See \url{http://en.wikipedia.org/wiki/Erdos-Renyi_model}.
\index{Erdos-Renyi model@\Erdos-\Renyi~model}

\Erdos~and \Renyi~studied the properties of these random graphs;
one of their surprising results is the existence of
abrupt changes in the properties of random graphs as
random edges are added.
\index{threshold value}

One of the properties that displays this kind of transition is
connectivity.  An undirected graph is {\bf connected} if there is a
path from every node to every other node.

In an ER graph, the probability that the graph is connected is very
low when $p$ is small and nearly 1 when $p$ is large.  Between these
two regimes, there is a rapid transition at a particular value of
$p$, denoted $p^*$.

\Erdos~and \Renyi~showed that this critical value is
$p^* = \ln n / n$, where $n$ is the number of nodes.
A random graph, $G(n, p)$, is unlikely to be connected
if $p < p^*$ and very likely to be connected if $p > p^*$.
\index{critical value}

To test this claim, we'll develop algorithms to generate random
graphs and check whether they are connected.


\section{Generating graphs}
\label{generating}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap02-3.pdf}}
\caption{A complete graph with 10 nodes.}
\label{chap02-3}
\end{figure}

I'll start by generating a {\bf complete} graph, which is a graph
where every node is connected to every other.

Here's a generator function that takes a list of nodes and enumerates
all distinct pairs.  If you are not familiar with generator functions,
you might want to read Appendix~\ref{???} and then come back.

\begin{code}
def all_pairs(nodes):
    for i, u in enumerate(nodes):
        for j, v in enumerate(nodes):
            if i>j:
                yield u, v
\end{code}

We can use \py{all_pairs} to construct a complete graph:

\begin{code}
def make_complete_graph(n):
    G = nx.Graph()
    nodes = range(n)
    G.add_nodes_from(nodes)
    G.add_edges_from(all_pairs(nodes))
    return G
\end{code}

\py{make_complete_graph} takes the number of nodes, \py{n}, and
returns a new \py{Graph} with \py{n} nodes and edges between all
pairs of nodes.

The following code makes a complete graph with 10 nodes and draws it.

\begin{code}
complete = make_complete_graph(10)
nx.draw_circular(complete, 
                 node_color=COLORS[2], 
                 node_size=1000, 
                 with_labels=True)
\end{code}

Figure~\ref{chap02-3} shows the result.
Soon we will modify this code to generate ER graphs, but first
we'll develop functions to check whether a graph is connected.


\section{Connected graphs}
\label{connected}

A graph is {\bf connected} if there is a path from every node to every
other node.  
See \url{http://en.wikipedia.org/wiki/Connectivity_(graph_theory)}.
\index{connected graph}
\index{path}

For many applications involving graphs, it is useful to
check whether a graph is connected.  Fortunately, there is a
simple algorithm that does it.

You can start at any node and check whether you can reach all
other nodes.  If you can reach a node, $v$, you can reach any
of the {\bf neighbors} of $v$, which is any node connected by
$v$ by an edge.

The \py{Graph} class provides a method called \py{neighbors}
that returns a list of neighbors for a given node.  For
example, in the complete graph we generated in the previous section:

\begin{code}
>>> complete.neighbors(0)
[1, 2, 3, 4, 5, 6, 7, 8, 9]
\end{code}

Suppose we start at node $s$.  We can mark $s$ as ``seen'',
then we can mark its neighbors.
Then we mark the neighbor's neighbors, and so
on, until you can't reach any more nodes.  If all nodes are
seen, the graph is connected.

Here's what that looks like in Python:

\begin{code}
def reachable_nodes(G, start):
    seen = set()
    stack = [start]
    while stack:
        node = stack.pop()
        if node not in seen:
            seen.add(node)
            stack.extend(G.neighbors(node))
    return seen
\end{code}

\py{reachable_nodes} takes a \py{Graph} and a starting node, {\tt
  start}, and returns the set of nodes that can be reached from {\tt
  start}.

Initially the set, \py{seen}, is empty, and we create a
list called \py{stack} that keeps track of nodes we have
discovered but not yet processed.  Initially the stack contains
a single node, \py{start}.

Now, each time through the loop, we

\begin{enumerate}

\item Remove one node from the stack.

\item If the node is already in \py{seen}, we go back
to Step 1.

\item Otherwise, we add the node to \py{seen} and add its
neighbors to the stack.

\end{enumerate}

When the stack is empty, we can't reach any more nodes, so we
break out of the loop and return \py{seen}.

As an example, we can find all nodes in the complete graph that
are reachable from node 0:

\begin{code}
>>> reachable_nodes(complete, 0)
{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}
\end{code}

Initially, the stack contains node 0 and \py{seen} is empty.
The first time through the loop, node 0 is added to \py{seen}
and all the other nodes are added to the stack (since they are all
neighbors of node 0).

The next time through the loop, \py{pop} returns the last element
in the stack, which is node 9.  So node 9 gets added to \py{seen}
and its neighbors get added to the stack.  

Notice that the same node can appear more than once in the stack;
in fact, a node with $k$ neighbors will be added to the stack
$k$ times.  Later we will look for ways to make this algorithm
more efficient.

We can use \py{reachable_nodes} to write \py{is_connected}:

\begin{code}
def is_connected(G):
    start = next(G.nodes_iter())
    reachable = reachable_nodes(G, start)
    return len(reachable) == len(G)
\end{code}

\py{is_connected} chooses a starting node by calling
\py{nodes_iter}, which returns an iterator object, and passing the
result to \py{next}, which returns the first node.

\py{seen} gets the set of nodes that can be reached from
\py{start}.  If the size of this set is the same as the size
of the graph, that means we can reach all nodes, which means the
graph is connected.

A complete graph is, not surprisingly, connected:

\begin{code}
>>> is_connected(complete)
True
\end{code}

In the next section we will generate ER graphs and check whether they
are connected.


\section{Generating ER graphs}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap02-4.pdf}}
\caption{An ER graph with \py{n=10} and \py{p=0.3}.}
\label{chap02-4}
\end{figure}

The ER graph $G(n, p)$ contains $n$ nodes, and each pair of nodes is
connected by an edge with probability $p$.  Generating an ER graph is
similar to generating a complete graph.

The following generator function enumerates all possible edges and
uses a helper function, \py{flip}, to choose which ones should be
added to the graph:

\begin{code}
def random_pairs(nodes, p):
    for i, u in enumerate(nodes):
        for j, v in enumerate(nodes):
            if i>j and flip(p):
                yield u, v
\end{code}

\py{flip} returns \py{True} with the
given probability, \py{p}, and \py{False} with the complementary
probability \py{1-p}:

\begin{code}
from numpy.random import random

def flip(p):
    return random() < p
\end{code}

Finally, \py{make_random_graph} generates and returns the 
ER graph $G(n, p)$.

\begin{code}
def make_random_graph(n, p):
    G = nx.Graph()
    nodes = range(n)
    G.add_nodes_from(nodes)
    G.add_edges_from(random_pairs(nodes, p))
    return G
\end{code}

\py{make_random_graph} is almost identical to \py{make_complete_graph};
the only difference is that it uses \py{random_pairs} instead of
\py{all_pairs}.

Here's an example with \py{p=0.3}:

\begin{code}
random_graph = make_random_graph(10, 0.3)
\end{code}

Figure~\ref{chap02-4} shows the result.  This graph turns out to be
connected; in fact, most ER graphs with $n=10$ and
$p=0.3$ are connected.  In the next section, we'll see how many.



\section{The probability of connectivity}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap02-5.pdf}}
\caption{Probability of connectivity with $n=10$ and a range of $p$.
The vertical line shows the predicted critical value.}
\label{chap02-5}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap02-6.pdf}}
\caption{Probability of connectivity for several values of $n$ and a range of $p$.}
\label{chap02-6}
\end{figure}

For given values of $n$ and $p$, we would like to know the probability
that $G(n, p)$ is connected.  We can estimate it by generating
a large number of random graphs and counting how many are connected.
Here's how:

\begin{code}
def prob_connected(n, p, iters=100):
    count = 0
    for i in range(iters):
        random_graph = make_random_graph(n, p)
        if is_connected(random_graph):
            count += 1
    return count/iters
\end{code}

\py{iters} is the number of random graphs we generate.  As we
increase \py{iters}, the estimated probability gets more precise.

\begin{code}
>>> prob_connected(10, 0.3, iters=10000)
0.6454
\end{code}

Out of 10000 ER graphs with these parameters, 6498 are connected, so
we estimate that 65\% of them are connected.  So $0.3$ is near the
critical value where the probability of connectivity goes from near 0
to near 1.  According to \Erdos~and \Renyi, $p^* = \ln n / n = 0.23$.

We can get a clearer view of the transition by estimating the probability
of connectivity for a range of values of $p$:

\begin{code}
import numpy as np

n = 10
ps = np.logspace(-2.5, 0, 11)
ys = [prob_connected(n, p) for p in ps]
\end{code}

This is the first example we've seen using NumPy.  Following convention,
I import NumPy as {\tt np}.  The function \py{logspace} returns an
array of 11 values from $10^{-2.5}$ to $10^0 = 1$, equally spaced
on a logarithmic scale.

Figure~\ref{chap02-5} shows the results, with
a vertical line at $p^*$.  As expected, the transition from 0 to 1
occurs near the predicted critical value.  With $p$ on a log scale,
the transition is roughly symmetric.

Figure~\ref{chap02-6} shows
similar results for larger values of $n$.  As $n$ increases, the
critical value gets smaller and the transition gets more abrupt.

These experiments are consistent with the results \Erdos~and
\Renyi~proved in their papers.


\section{Analysis of graph algorithms}
\label{graphanalysis}

I think chapter I presented an algorithm for checking whether
a graph is connected; in the next few chapters, we will see
a few more graph algorithms.  And we will want to analyze the
performance of those algorithms, figuring out how their run times
grow as the size of the graphs increases.

If you are not already familiar with analysis of algorithms,
you should read Appendix~\ref{alganalysis} before you continue.

\newcommand{\V}{n}
\newcommand{\E}{m}

The order of growth for graph algorithms is usually expressed
as a function of $\V$, the number of vertices, and $\E$, the number
of edges.
\index{analysis of graph algorithms}
\index{graph algorithm}

As an example, let's analyze \py{reachable_nodes} from
Section~\ref{connected}:

\begin{code}
def reachable_nodes(G, start):
    seen = set()
    stack = [start]
    while stack:
        node = stack.pop()
        if node not in seen:
            seen.add(node)
            stack.extend(G.neighbors(node))
    return seen
\end{code}

Each time through the loop, we pop a node off the stack; by default,
\py{pop} removes and returns the last element of a list, which is
a constant time operation.

Next we check whether the node is in \py{seen}, which is a set,
so checking membership is constant time.

If the node is not already in \py{seen}, we add it, which is
constant time, and then add the neighbors to the stack, which is
linear in the number of neighbors.

To express the run time in terms of $\V$ and $\E$, we can add up
the total number of times each node is added \py{seen}
and \py{stack}.

Each node is only added to \py{seen} once, so the total number
of additions is $\V$.

But nodes might be added to \py{stack} many times, depending on
how many neighbors they have.  If a node has $k$ neighbors, it
is added to \py{stack} $k$ times.  Of course, if it has $k$ neighbors,
that means it is connected to $k$ edges.

So the total number
of additions to \py{stack} is the total number of edges, $\E$,
doubled because we consider every edge twice.

Therefore, the order of growth for this function is $O(\V + \E)$,
which is a convenient way to say that the run time grows in proportion
to either $\V$ or $\E$, whichever is ``bigger.''  \index{breadth first
  search} \index{BFS}

If we know the relationship between $\V$ and $\E$, we can simplify
this expression.  For example, in a complete graph the number of edges
is $n(n-1)/2$, which is in $O(\V^2)$.  So for a complete graph,
\py{reachable_nodes} is quadratic in $\V$.
\index{quadratic}


\section{Exercises}

The code for this chapter is in \py{chap02.ipynb}, which is a
Jupyter notebook in the repository for this book.  For more information
about working with this code, see Section~\ref{???}.

\begin{exercise}
Launch \py{chap02.ipynb} and run the code.  There are a few short
exercises embedded in the notebook that you might want to try.
\end{exercise}

\begin{exercise}
In Section~\ref{graphanalysis} we analyzed the performance of
\py{reachable_nodes} and classified it in $O(n + m)$, where $n$ is the
number of nodes and $m$ is the number of edges.  Continuing the
analysis, what is the order of growth for \py{is_connected}?

\begin{code}
def is_connected(G):
    start = next(G.nodes_iter())
    reachable = reachable_nodes(G, start)
    return len(reachable) == len(G)
\end{code}


\end{exercise}

\begin{exercise}
In my implementation of \py{reachable_nodes}, you might be bothered by
the apparent inefficiency of adding {\em all} neighbors to the stack
without checking whether they are already in \py{seen}.  Write a
version of this function that checks the neighbors before adding them
to the stack.  Does this ``optimization'' change the order of growth?
Does it make the function faster?
\end{exercise}


\begin{exercise}

There are actually two kinds of ER graphs.  The one we generated in
this chapter, $G(n, p)$, is characterized by two parameters, the number
of nodes and the probability of an edge between nodes.

An alternative definition, denoted $G(n, m)$, is also characterized by
two parameters: the number of nodes, $n$, and the number of edges,
$m$.  Under this definition, the number of edges is fixed, but their
location is random.

Repeat the experiments we did in this chapter using this alternative
definition.  Here are a few suggestions for how to proceed:

1. Write a function called \py{m_pairs} that takes a list of nodes
and the number of edges, $m$, and returns a random selection of $m$
edges.  A simple way to do that is to generate a list of all possible
edges and use \py{random.sample}.

2. Write a function called \py{make_m_graph} that takes $n$ and
$m$ and returns a random graph with $n$ nodes and $m$ edges.

3. Make a version of \py{prob_connected} that uses
\py{make_m_graph} instead of \py{make_random_graph}.

4. Compute the probability of connectivity for a range of values of $m$.

How do the results of this experiment compare to the results using the
first type of ER graph?

\end{exercise}


\chapter{Small world graphs}

Many networks in the real world, including social networks, have
the ``small world property'', which is that the average distance
between nodes, measured in number of edges on the shortest path,
is much smaller than expected.

In this chapter, I present Stanley Milgram's famous Small World
Experiment, which was the first scientific demonstration of
the small world property in a real social network.  Then we'll
consider Watts-Strogatz graphs, which are intended as a model of
small world graphs.
I'll replicate the experiment Watts and Strogatz performed and
explain what it is intended to show.

Along the way, we'll see two new graph algorithms: breadth-first
search (BFS) and Dijkstra's algorithm for computing the shortest
path between nodes in a graph.


\section{Stanley Milgram}

Stanley Milgram was an American social psychologist who conducted
two of the most famous experiments in social science, the
Milgram experiment, which studied people's obedience to authority
(\url{http://en.wikipedia.org/wiki/Milgram_experiment})
and the Small World Experiment, which studied
the structure of social networks
(\url{http://en.wikipedia.org/wiki/Small_world_phenomenon}).
\index{Milgram, Stanley}
\index{small world experiment}

In the Small World Experiment, Milgram sent a package to several
randomly-chosen people in Wichita, Kansas, with instructions asking
them to forward an enclosed letter to a target person, identified by
name and occupation, in Sharon, Massachusetts (which is the town near
Boston where I grew up).  The subjects were told that they could mail
the letter directly to the target person only if they knew him
personally; otherwise they were instructed to send it, and the same
instructions, to a relative or friend they thought would be more
likely to know the target person.
\index{Kansas}
\index{Wichita, Kansas}
\index{Massachusetts}
\index{Sharon, Massachusetts}

Many of the letters were never delivered, but for the ones that
were the average path length---the number of
times the letters were forwarded---was about six.  This result
was taken to confirm previous observations (and speculations) that
the typical distance between any two people in a social network
is about ``six degrees of separation.''
\index{six degrees}

This conclusion is surprising because most people expect social
networks to be localized---people tend to live near their
friends---and in a graph with local connections, path lengths tend to
increase in proportion to geographical distance.  For example, most of
my friends live nearby, so I would guess that the average distance
between nodes in a social network is about 50 miles.  Wichita is about
1600 miles from Boston, so if Milgram's letters traversed typical
links in the social network, they should have taken 32 hops, not six.
\index{hop}
\index{social network}
\index{local connection}


\section{Watts and Strogatz}

In 1998 Duncan Watts and Steven Strogatz published a paper in {\em
  Nature}, ``Collective dynamics of 'small-world' networks'', that
proposed an explanation for the small world phenomenon.  You can
download it from
\url{http://www.nature.com/nature/journal/v393/n6684/abs/393440a0.html}.
\index{Watts, Duncan} \index{Strogatz, Steven} \index{small world
  network}

Watts and Strogatz started with two kinds of graph that were well
understood: random graphs and regular graphs.  They looked at two
properties of these graphs, clustering and path length.
\index{random graph}
\index{regular graph}
\index{clustering}
\index{path length}

\begin{description}

\item Clustering is a measure of the ``cliquishness'' of the graph.
In a graph, a {\bf clique} is a subset of nodes that are
all connected to each other; in a social network, a clique is
a set of people who are all friends with each other.  Watts and Strogatz
defined a clustering coefficient that quantifies the likelihood
that two nodes that are connected to the same node are also
connected to each other.
\index{clique}

\item Path length is a measure of the average distance between
two nodes, which corresponds to the degrees of separation in
a social network.

\end{description}

Their initial result was what you might expect: regular graphs
have high clustering and high path lengths;
random graphs with the same size tend to have low clustering
and low path lengths.  So neither of these is a good model of
social networks, which combine high clustering with
short path lengths.

Their goal was to create a {\bf generative model} of a social
network.  A generative model tries to explain a phenomenon by
modeling the process that builds or leads to the phenomenon.  In
this case Watts and Strogatz proposed a process for building
small-world graphs:
\index{generative model}

\begin{enumerate}

\item Start with a regular graph with $n$ nodes and degree $k$.  
  Watts and Strogatz use a ring lattice, which is
  a kind of regular graph (details in the next section).

\item Choose a subset of the edges and ``rewire'' them by
  replacing them with random edges.
  \index{rewire}

\end{enumerate}

The probability that an edge is rewired is a parameter, $p$,
that controls how random the graph is.  With $p=0$, the graph
is regular; with $p=1$ it is random.
\index{parameter}

Watts and Strogatz found that small values of $p$ yield graphs
with high clustering, like a regular graph, and low path
lengths, like a random graph.

In this chapter I will replicate the Watts and Strogatz experiment
in the following steps:

\begin{enumerate}

\item We'll start by constructing a ring lattice, which is a kind
of regular graph.

\item Then we'll rewire it as Watts and Strogatz did.

\item We'll write a function to measure the degree of clustering
and use a NetworkX function to compute path lengths.

\item Then we'll compute the degree of clustering and path length for
a range of values of $p$.

\item Finally, I'll present an efficient algorithm for computing shortest
paths, Dijkstra's algorithm.

\end{enumerate}


\section{Ring lattices}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap03-1.pdf}}
\caption{A ring lattice with $n=10$ and $k=4$.}
\label{chap03-1}
\end{figure}

A {\bf regular} graph is a graph where each node has the same number
of neighbors; the number of neighbors is also called the {\bf degree}
of the node.

A ring lattice is a kind of regular graph, which Watts and Strogatz
use as the basis of their model.
In a ring lattice with $n$ nodes, the nodes can be arranged in a circle
with each node connected the $k$ nearest neighbors.

For example, a ring lattice with $n=3$
and $k=2$ would add the following edges: $(0, 1)$, $(1, 2)$, and
$(2, 0)$.  Notice that the edges ``wrap around'' from the
highest-numbered node back to 0.

More generally, we can enumerate the edges like this:

\begin{code}
def adjacent_edges(nodes, halfk):
    n = len(nodes)
    for i, u in enumerate(nodes):
        for j in range(i+1, i+halfk+1):
            v = nodes[j % n]
            yield u, v
\end{code}

\py{adjacent_edges} takes a list of nodes and a parameter,
\py{halfk}, which is half of $k$.  It is a generator function that
yields one edge at a time.  It uses the modulus operator, \verb"%",
to wrap around from the highest-numbered node to the lowest.

We can test it like this:

\begin{code}
>>> nodes = range(3)
>>> for edge in adjacent_edges(nodes, 1):
...     print(edge)
(0, 1)
(1, 2)
(2, 0)
\end{code}

Now we can use \py{adjacent_edges} to make a ring lattice:

\begin{code}
def make_ring_lattice(n, k):
    G = nx.Graph()
    nodes = range(n)
    G.add_nodes_from(nodes)
    G.add_edges_from(adjacent_edges(nodes, k//2))
    return G
\end{code}

Notice that \py{make_ring_lattice} uses floor division to compute
\py{halfk}, so if \py{k} is odd, it will round down and generate
a ring lattice with degree \py{k-1}.  That's probably not what we
want, but it is good enough for now.

We can test the function like this:

\begin{code}
lattice = make_ring_lattice(10, 4)
\end{code}

Figure~\ref{chap03-1} shows the result.


\section{WS graphs}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap03-2.pdf}}
\caption{WS graphs with $n=20$, $k=4$, and $p=0$ (left), $p=0.2$ (middle),
and $p=1$ (right).}
\label{chap03-2}
\end{figure}

To make a Watts-Strogatz (WS) graph, we start with a ring lattice and
``rewire'' some of the edges.  In their paper, Watts and Strogatz
consider the edges in a particular order and rewire each one with
probability $p$.  If an edge is rewired, they leave the first node
unchanged and choose the second node at random.  They don't allow self
loops or multiple edges; that is, you can't have a edge from a node to
itself, and you can't have more than one edge between the same two
nodes.

Here is my implementation of this process.

\begin{code}
def rewire(G, p):
    nodes = set(G.nodes())
    for edge in G.edges():
        if flip(p):
            u, v = edge
            choices = nodes - {u} - set(G[u])
            new_v = choice(tuple(choices))
            G.remove_edge(u, v)
            G.add_edge(u, new_v)
\end{code}

The parameter \py{p} is the probability of rewiring an edge.  The
\py{for} loop enumerates the edges and uses \py{flip}, which returns
\py{True} with probability \py{p}, to choose which ones get rewired.

If we are rewiring an edge from node \py{u} to node \py{v}, we
have to choose a replacement for \py{v}, called \py{new_v}.
To compute the possible choices,
we start with \py{nodes}, which is a set,
and subtract off \py{u} and its neighbors, which avoids self
loops and multiple edges.  

Then we choose \py{new_v} from \py{choices}, remove the existing
edge from \py{u} to \py{v}, and add a new edge from \py{u} to
\py{new_v}.

As an aside, the expression \py{G[u]} returns a
dictionary that contains the neighbors of \py{u} as keys.  In
this case it is a little faster than using \py{G.neighbors}.

This function does not consider the edges in the order specified
by Watts and Strogatz, but it doesn't seem to affect
the results.

Figure~\ref{chap03-2} shows WS graphs with $n=20$, $k=4$, and 
a range of values of $p$.  When $p=0$, the graph is a ring lattice.
When $p=1$, it is completely random.  As we'll see, the interesting
things happen in between.


\section{Clustering}
\label{clustering}

The next step is to compute the clustering coefficient, which
quantifies the tendency for the nodes to form cliques.
A {\bf clique} is a set of nodes that are completely connected;
that is, there are edges between all pairs of nodes in the set.

Suppose a particular node, $u$, has $k$ neighbors.  If all of the
neighbors are connected to each other, there would be $k(k-1)/2$
edges among them.  The fraction of those edges that actually exist
is the local clustering coefficient for $u$, denoted $C_u$.
It is called
a ``coefficient'' because it is always between 0 and 1.

If we compute the average of $C_u$ over all nodes, we get the
``network average clustering coefficient'', denoted $\bar{C}$.

Here is a function that computes it. 

\begin{code}
def node_clustering(G, u):
    neighbors = G[u]
    k = len(neighbors)
    if k < 2:
        return 0
        
    total = k * (k-1) / 2
    exist = 0    
    for v, w in all_pairs(neighbors):
        if G.has_edge(v, w):
            exist +=1
    return exist / total
\end{code}

Again I use the expression \py{G[u]}, which returns a dictionary
with the neighbors of \py{node} as keys.  If a node has fewer than 2
neighbors, the clustering coefficient is undefined, but for simplicity
\py{node_clustering} returns 0.

Otherwise we compute the number of possible edges among the neighbors,
\py{total}, and then count the number of those edges that actually
exist.  The result is the fraction of all edges that exist.

We can test the function like this:
   
\begin{code}
>>> lattice = make_ring_lattice(10, 4)
>>> node_clustering(lattice, 1)
0.5
\end{code}

In a ring lattice with $k=4$, the clusting coefficient for each node
is 0.5 (if you are not convinced, take another look at
Figure~\ref{chap03-1}).

Now we can compute the network average clustering coefficient like this:
   
\begin{code}
def clustering_coefficient(G):
    cc = np.mean([node_clustering(G, node) for node in G])
    return cc
\end{code}

\py{np.mean} is a NumPy function that computes the mean of the numbers
in a list or array.

And we can test it like this:

\begin{code}
>>> clustering_coefficient(lattice)
0.5
\end{code}

In this graph, the local clustering coefficient for all nodes is 0.5,
so the average across nodes is 0.5.  Of course, we expect this value
to be different for WS graphs.


\section{Shortest path lengths}
\label{pathlength}

The next step is to compute the characteristic path length, $L$, which
is the average length of the shortest path between each pair of nodes.
To compute it, I'll start with a function provided by NetworkX,
\py{shortest_path_length}.  I'll use it to replicate the Watts and
Strogatz experiment, then I'll explain how it works.

Here's a function that takes a graph and returns a list of shortest
path lengths, one for each pair of nodes.

\begin{code}
def path_lengths(G):
    length_map = nx.shortest_path_length(G)
    lengths = [length_map[u][v] for u, v in all_pairs(G)]
    return lengths
\end{code}

The return value from \py{nx.shortest_path_length} is a dictionary
of dictionaries.  The outer dictionary maps from each node, \py{u},
to a dictionary that maps from each node, \py{v}, to the length of
the shortest path from \py{u} to \py{v}.

With the list of lengths from \py{path_lengths}, we can compute $L$
like this:

\begin{code}
def characteristic_path_length(G):
    return np.mean(path_lengths(G))
\end{code}

And we can test it with a small ring lattice:

\begin{code}
>>> lattice = make_ring_lattice(3, 2)
>>> characteristic_path_length(lattice)
1.0
\end{code}

In this example, all 3 nodes are connected to each other, so the
mean path length is 1.


\section{The WS experiment}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/chap03-3.pdf}}
\caption{Clustering coefficient (C) and characteristic path length (L) for
WS graphs with $n=1000$, $k=10$, and a range of $p$.}
\label{chap03-3}
\end{figure}

Now we are ready to replicate the WS experiment, which shows that for
a range of values of $p$, a WS graph has high clustering like a
regular graph and short path lengths like a random graph.

I'll start with \py{run_one_graph}, which takes \py{n}, \py{k}, and \py{p}; it
generates a WS graph with the given parameters and computes the
mean path length, \py{mpl}, and clustering coefficient \py{cc}:

\begin{code}
def run_one_graph(n, k, p):
    ws = make_ws_graph(n, k, p)    
    mpl = characteristic_path_length(ws)
    cc = clustering_coefficient(ws)
    print(mpl, cc)
    return mpl, cc
\end{code}

Watts and Strogatz ran their experiment with \py{n=1000} and \py{k=10}.
With these parameters, \py{run_one_graph} takes about one second on
my computer; most of that time is spent computing the
mean path length.

Now we need to compute these values for a range of \py{p}.  I'll use
the NumPy function \py{logspace} again to compute \py{ps}:

\begin{code}
ps = np.logspace(-4, 0, 9)
\end{code}

For each value of \py{p}, I generate 3 random graphs and we'll average the
results.  Here's the function that runs the experiment:

\begin{code}
def run_experiment(ps, n=1000, k=10, iters=3):
    res = {}
    for p in ps:
        print(p)
        res[p] = []
        for _ in range(iters):
            res[p].append(run_one_graph(n, k, p))
    return res
\end{code}

The result is a dictionary that maps from each value of \py{p} to
a list of \py{(mpl, cc)} pairs.

The last step is to aggregate the results:

\begin{code}
L = []
C = []
for p, t in sorted(res.items()):
    mpls, ccs = zip(*t)
    mpl = np.mean(mpls)
    cc = np.mean(ccs)
    L.append(mpl)
    C.append(cc)
\end{code}

Each time through the loop, we get a value of $p$ and a list of
\py{(mpl, cc)} pairs.  We use \py{zip} to extracts two lists, \py{mpls} and
\py{ccs}, then compute their means and append them to \py{L} and \py{C}, which
are lists of path lengths and clustering coefficients.

In order to plot \py{L} and \py{C} on the same axes, we standardize them
by dividing through by the first element:

\begin{code}
L = np.array(L) / L[0]
C = np.array(C) / C[0]
\end{code}

Figure~\ref{chap03-3} shows the results.  As $p$ increases, the mean
path length drops quickly, because even a small number of randomly
rewired edges provide shortcuts between regions of the graph that
are far apart in the lattice.  On the other hand, removing local links
decreases the clustering coefficient, but much more slowly.

As a result, there is a wide range of $p$ where a WS graph has the
properties of a small world graph, high clustering and low path
lengths.

And that's why Watts and Strogatz propose WS graphs as a
model for real-world networks that exhibit the small world phenomenon.


\section{What kind of explanation is {\em that}?}

If you ask me why planetary orbits are elliptical,
I might start by modeling a planet and a star as point masses; I
would look up the law of universal gravitation at
\url{http://en.wikipedia.org/wiki/Newton's_law_of_universal_gravitation}
and use it to write a differential equation for the motion of
the planet.  Then I would either derive the orbit equation or,
more likely, look it up at \url{http://en.wikipedia.org/wiki/Orbit_equation}.
With a little algebra, I could derive the conditions that
yield an elliptical orbit.  Then I would argue that the objects
we consider planets satisfy these conditions.
\index{planetary motion}
\index{universal gravitation}

People, or at least scientists, are generally satisfied with
this kind of explanation.  One of the reasons for its appeal
is that the assumptions and approximations in the model seem
reasonable.  Planets and stars are not really point masses,
but the distances between them are so big that their actual
sizes are negligible.  Planets in the same solar system can
affect each others' orbits, but the effect is usually small.
And we ignore relativistic effects, again on the assumption that
they are small.
\index{explanatory model}

This explanation is also appealing because it is equation-based.
We can express the orbit equation in a closed form, which means
that we can compute orbits efficiently.  It also means that
we can derive general expressions for the orbital velocity,
orbital period, and other quantities.
\index{equation-based model}

Finally, I think this kind of explanation is appealing because
it has the form of a mathematical proof.  It starts from a
set of axioms and derives the result by logic and analysis.
But it is important to remember that the proof pertains to the
model and not the real world.  That is, we can prove that
an idealized model of a planet yields an elliptic orbit, but
we can't prove that the model pertains to actual planets (in
fact, it does not).
\index{mathematical proof}
\index{proof}

By comparison, Watts and Strogatz's explanation of the small
world phenomenon may seem less satisfying.  First, the model
is more abstract, which is to say less realistic.  Second,
the results are generated by simulation, not by mathematical
analysis.  Finally, the results seem less like a proof and
more like an example.
\index{abstract model}

Many of the models in this book are like the Watts and Strogatz model:
abstract, simulation-based and (at least superficially) less formal
than conventional mathematical models.  One of the goals of this book
it to consider the questions these models raise:

\index{simulation-based model}

\begin{itemize}

\item What kind of work can these models do: are they predictive, or
  explanatory, or both?

\item Are the explanations these models offer less satisfying than
  explanations based on more traditional models?  Why?

\item How should we characterize the differences between these and
  more conventional models?  Are they different in kind or only in
  degree?

\end{itemize}

Over the course of the book I will offer my answers
to these questions, but they are tentative and sometimes
speculative.  I encourage you to consider them skeptically
and reach your own conclusions.


\section{Breadth-First Search}

When we computed shortest paths, we used a function provided by
NetworkX, but I have not explained how it works.  To do that, I'll
start with breadth-first search, which is the basis of Dijkstra's
algorithm for computing shortest paths.

In Section~\ref{connected} I presented \py{reachable_nodes}, which finds all
the nodes that can be reached from a given starting node:

\begin{code}
def reachable_nodes(G, start):
    seen = set()
    stack = [start]
    while stack:
        node = stack.pop()
        if node not in seen:
            seen.add(node)
            stack.extend(G.neighbors(node))
    return seen
\end{code}

I didn't say so at the time, but \py{reachable_nodes} performs a
depth-first search (DFS).  Now we'll modify it to perform breadth-first
search (BFS).

To understand the difference, imagine you are exploring a castle.
You start in a room with three doors marked A, B, and C.  You open
door C and discover another room, with doors marked D, E, and F.

Which door do you open next?  If you are feeling adventurous, you might
want to go deeper into the castle and choose D, E, or F.  That would be
a depth-first search.

But if you wanted to be more systematic, you might go back and explore
A and B before D, E, and F.  That would be a breadth-first search.

In \py{reachable_nodes}, we use \py{list.pop} to choose the next node to
``explore''.  By default, \py{pop} returns the last element of the list,
which is the last one we added.  In the example, that would be door F.

If we want to perform a BFS instead, the simplest solution is to
pop the first element off the stack:

\begin{code}
        node = stack.pop(0)
\end{code}

That works, but it is slow.  In Python, popping the last element
of a list takes constant time, but popping the first element is linear
in the length of the list.  In the worst case, the length of the
stack is $O(\V)$, which makes this implementation of BFS $O(\V\E)$,
which is much worse than what it should be, $O(\V + \E)$.

We can solve this problem with a double-ended queue, also known
as a {\bf deque}.  The important feature of a deque is that you
can add and remove elements from the beginning or end in constant time.
To see how it is implemented, see \url{https://en.wikipedia.org/wiki/Double-ended_queue}.

Python provides a \py{deque} in the \py{collections} module, so we can
import it like this:

\begin{code}
from collections import deque
\end{code}

And we can use it to write an efficient BFS:

\begin{code}
def reachable_nodes_bfs(G, start):
    seen = set()
    queue = deque([start])
    while queue:
        node = queue.popleft()
        if node not in seen:
            seen.add(node)
            queue.extend(G.neighbors(node))
    return seen
\end{code}

The differences are:

\begin{itemize}

\item I replaced the list called \py{stack} with a deque called \py{queue}.

\item I replaced \py{pop} with \py{popleft}, which removes and returns
leftmost element of the queue, which was the first to be added.

\end{itemize}

This version is back to being $O(\V + \E)$.  Now we're ready to
find shortest paths.


\section{Dijkstra's algorithm}
\label{dijkstra}

Edsger W. Dijkstra was a Dutch computer scientist who invented an
efficient shortest-path algorithm (see
\url{http://en.wikipedia.org/wiki/Dijkstra's_algorithm}).  He also invented the
semaphore, which is a data structure used to coordinate programs that
communicate with each other (see
\url{http://en.wikipedia.org/wiki/Semaphore_(programming)} and Downey, {\em The
  Little Book of Semaphores}).

\index{Dijkstra, Edsger}
\index{Little Book of Semaphores@{\em The Little Book of Semaphores}}

Dijkstra is famous (and notorious) as the author of a series
of essays on computer science.
Some, like ``A Case against the GO TO Statement,'' have
had a profound effect on programming practice.
Others, like
``On the Cruelty of Really Teaching Computing Science,'' are
entertaining in their cantankerousness, but less effective.

{\bf Dijkstra's algorithm} solves the ``single source shortest path
problem,'' which means that it finds the minimum distance from a given
``source'' node to every other node in the graph (or at least every
connected node).

\index{shortest path}
\index{single source shortest path} 
\index{Dijkstra's algorithm}

We start with a simplified version of the algorithm that
considers all edges the same length.  The more general version
works with any non-negative edge lengths.

The simplified version is similar to the breadth-first search
in Section~\ref{bfs} except that we replace the set called
\py{seen} with a dictionary called \py{dist}, which maps from each
node to its distance from the source:

\begin{code}
def shortest_path_dijkstra(G, start):
    dist = {start: 0}
    queue = deque([start])
    while queue:
        node = queue.popleft()
        new_dist = dist[node] + 1

        neighbors = set(G[node]) - set(dist)
        for n in neighbors:
            dist[n] = new_dist
        
        queue.extend(neighbors)
    return dist
\end{code}

Here's how it works:

\begin{itemize}

\item Initially, \py{queue} contains a single element, \py{start}, and \py{dist}
maps from \py{start} to distance 0 (which is the distance from \py{start} to
itself).

\item Each time through the loop, we use \py{popleft} to select nodes
in the order they were added to the queue.

\item Next we find all neighbors of \py{node} that are not already in
\py{dist}.

\item Since the distance from \py{start} to \py{node} is \py{dist[node]}, the
distance to any of the undiscovered neighbors is \py{dist[node]+1}.

\item For each neighbor, we add an entry to \py{dist}, then we add
the neighbors to the queue.

\end{itemize}

This algorithm only works if we use BFS, not DFS.  Why?

The first time through the loop \py{node} is \py{start}, and \py{new_dist}
is 1.  So the neighbors of \py{start} get distance 1 and they
go in the queue.

When we process the neighbors of \py{start}, all of {\em their}
neighbors get distance 2.  We know that none of them can have
distance 1, because if they did, we would have discovered
them during the first iteration.

Similarly, when we process the nodes with distance 2, we give
their neighbors distance 3.  We know that none of them can
have distance 1 or 2, because if they did, we would have
discovered them during a previous iteration.

And so on.  If you are familiar with proof by induction, you
can see where this is going.

But this argument only works if we process all nodes with distance
1 before we start processing nodes with distance 2, and so on.
And that's exactly what BFS does.

In the exercises at the end of this chapter, you'll write a version
of Dijkstra's algorithm using DFS, so you'll have a chance to see
what goes wrong.


\section{Exercises}

\begin{exercise}

In a ring lattice, every node has the same number of neighbors.  The
number of neighbors is called the {\bf degree} of the node, and a
graph where all nodes have the same degree is called a {\bf regular
  graph}.

All ring lattices are regular, but not all regular graphs are ring
lattices.  In particular, if \py{k} is odd, we can't construct a ring
lattice, but we might be able to construct a regular graph.

Write a function called \py{make_regular_graph} that takes \py{n} and \py{k}
and returns a regular graph that contains \py{n} nodes, where every node
has \py{k} neighbors.  If it's not possible to make a regular graph with
the given values of \py{n} and \py{k}, the function should raise a
\py{ValueError}.

\end{exercise}


\begin{exercise}

My implementation of \py{reachable_nodes_bfs} is efficient in the sense
that it is in $O(\V + \E)$, but it incurs a lot of overhead adding nodes
to the queue and removing them.  NetworkX provides a simple, fast
implementation of BFS, available from the NetworkX repository on
  GitHub at \url{https://github.com/networkx/networkx/blob/master/networkx/algorithms/components/connected.py}.

%TODO: tinyurl

Here is a version I modified to return a set of nodes:

\begin{code}
def _plain_bfs(G, source):
    seen = set()
    nextlevel = {source}
    while nextlevel:
        thislevel = nextlevel
        nextlevel = set()
        for v in thislevel:
            if v not in seen:
                seen.add(v)
                nextlevel.update(G[v])
    return seen
\end{code}

Compare this function to \py{reachable_nodes_bfs} and see which is
faster.  Then see if you can modify this function to implement a
faster version of \py{shortest_path_dijkstra}

\end{exercise}


\begin{exercise}

The following implementation of a BFS
contains two performance errors.  What are
they?  What is the actual order of growth for this algorithm?
\index{order of growth}
\index{performance error}

\begin{code}
def bfs(top_node, visit):
    """Breadth-first search on a graph, starting at top_node."""
    visited = set()
    queue = [top_node]
    while len(queue):
        curr_node = queue.pop(0)    # Dequeue
        visit(curr_node)            # Visit the node
        visited.add(curr_node)

        # Enqueue non-visited and non-enqueued children
        queue.extend(c for c in curr_node.children
                     if c not in visited and c not in queue)
\end{code}

\end{exercise}


\begin{exercise}
In Section~\ref{dijkstra}, I claimed that Dijkstra's algorithm does
not work unless it uses BFS.  Write a version of
\py{shortest_path_dijkstra} that uses DFS and test it on a few examples
to see what goes wrong.
\end{exercise}



\begin{exercise}

A natural question about the Watts and Strogatz paper is
whether the small world phenomenon is specific to their
generative model or whether other similar models yield
the same qualitative result (high clustering and low path lengths).
\index{small world phenomenon}

To answer this question, choose a variation of the
Watts and Strogatz model and repeat the experiment.
There are two kinds of variation you might consider:

\begin{itemize}

\item Instead of starting with a regular graph, start with
another graph with high clustering.  For example, you could
put nodes at random locations in a 2-D space
and connect each node to its nearest $k$ neighbors.

\item Experiment with different kinds of rewiring.

\end{itemize}

If a range of similar models yield similar behavior, we
say that the results of the paper are {\bf robust}.
\index{robust}

\end{exercise}


\begin{exercise}

Dijkstra's algorithm solves the ``single source shortest path''
problem, but to compute the characteristic path length of a graph,
we actually want to solve the ``all pairs shortest path'' problem.

Of course, one option is to run Dijkstra's algorithm $n$ times,
once for each starting node.  And for some applications, that's
probably good enough.  But there are are more efficient alternatives.

Find an algorithm for the all-pairs shortest path problem and
implement it.  See
\url{https://en.wikipedia.org/wiki/Shortest_path_problem#All-pairs_shortest_paths}.

Compare the run time of your implementation with running
Dijkstra's algorithm $n$ times.  Which algorithm is better in
theory?  Which is better in practice?  Which one does NetworkX
use?

% https://github.com/networkx/networkx/blob/master/networkx/algorithms/shortest_paths/unweighted.py

\end{exercise}



\chapter{Scale-free networks}
\label{scale-free}

\section{Social network data}

Watts-Strogatz graphs are intended to model networks in the natural
and social sciences.  In their original paper, Watts and Strogatz
looked at the network of film actors (connected if they have appeared
in a movie together); the electrical power grid in the western United
States; and the network of neurons in the brain of the roundworm
{\it C. elegans}.  They found that all of these networks had the
high connectivity and low path lengths characteristic of small world
graphs.

In this section we'll perform the same analysis with a different dataset,
a set Facebook users and their friends.  If you are
not familiar with Facebook, users
who are connected on Facebook are called ``friends'', regardless of
the nature of their relationship in the real world.

I'll use data from the Stanford Network Analysis Project (SNAP), which
shares large datasets from online social networks and other sources.
Specifically, I'll use their Facebook dataset\footnote{J. McAuley and
  J. Leskovec. Learning to Discover Social Circles in Ego
  Networks. NIPS, 2012.}, which includes 4039 users and 88,234
friend relationships among them.  This dataset is in the repository
for this book, but it is also available from the SNAP web site at
\url{https://snap.stanford.edu/data/egonets-Facebook.html}

The data file contains one line per edge, with users identified by
integers from 0 to 4038.  Here's the code that reads the file:

\begin{code}
def read_graph(filename):
    G = nx.Graph()
    array = np.loadtxt(filename, dtype=int)
    G.add_edges_from(array)
    return G
\end{code}

NumPy provides a function called `loadtext` that reads the
given file and returns the contents as a NumPy array.  The
parameter `dtype` specifies the type of the elements of the array.

Then we can use `add_edges_from` to iterate the rows of the array
and make edges.  Here are the results:

\begin{code}
>>> fb = read_graph('facebook_combined.txt.gz')
>>> n = len(fb)
>>> m = len(fb.edges())
>>> n, m
(4039, 88234)
\end{code}

The number of nodes and edges are consistent with the documentation
of the dataset.

Now we can check whether this dataset has the characteristics of
a small world graph: high clustering and low path lengths.
I'll use `clustering_coefficient` from Section~\ref{clustering}

\begin{code}
from thinkcomplexity import clustering_coefficient
C = clustering_coefficient(fb)
C
\end{code}

Alternatively, you could use `average_clustering`, which is provided
by NetworkX:

\begin{code}
nx.average_clustering(fb)
\end{code}

Either way, the clustering coefficient is 0.61, which is high,
as we expect if this network has the small world property.

To compute the average path length, I'll use `path_lengths` from
Section~\ref{pathlength}

\begin{code}
from thinkcomplexity import path_lengths
pls = path_lengths(fb)
L = np.mean(pls)
\end{code}

The average path length between any two users is 3.7, which is
quite small in a network of more than 4000 users.  It's a small
world after all.

Now let's see if we can construct a WS graph that has the same
characteristics as this network.

\section{WS Model}

In this network, the ratio of edges to nodes is about 44:

\begin{code}
>>> k = int(round(2*m/n))
>>> k
44
\end{code}

We can make a WS graph with `n=4039` and `k=44`.  When `p=0`, we
get a ring lattice.

\begin{code}
lattice = nx.watts_strogatz_graph(n, k, 0)
\end{code}

In this graph, clustering is high: `C` is 0.73, compared to 0.61
in the dataset.  But the average path length is 46, much higher
than in the dataset!

With `p=1` we get a random graph:

\begin{code}
random = nx.watts_strogatz_graph(n, k, 1)
\end{code}

In the random graph, the average path length is 2.6, even shorter than
in the dataset (3.7), but the clustering coefficient is only 0.011.

By trial and error, we find that when `p=0.05` we get a WS graph with
high clustering and low path length:

\begin{code}
ws = nx.watts_strogatz_graph(n, k, 0.05, seed=15)
\end{code}

In this graph `C` is 0.63, a bit higher than
in the dataset, and `L` is 3.2, a bit lower than in the dataset.
So this graph models the small world characteristics of the dataset
well.

So far, so good.


\section{Degree}

Recall that the degree of a node is the number of edges it is connected
to.  If the WS graph is a good model for the Facebook data, it
should have the same total (or average) degree, and ideally the same
variance in degree, across nodes.

This function returns a list of degrees in a graph, one for each node:

\begin{code}
def degrees(G):
    return [G.degree(u) for u in G]
\end{code}


Results from WS and FB...



\section{Zipf's Law}

Zipf's law describes a relationship between the frequencies and ranks
of words in natural languages; see
  \url{http://en.wikipedia.org/wiki/Zipf's_law}.  The ``frequency'' of
a word is the number of times it appears in a body of work.
The ``rank'' of a word is its position in a list of words
sorted by frequency: the most common word has rank 1, the
second most common has rank 2, etc.
\index{Zipf's law}

Specifically, Zipf's Law
predicts that the frequency, $f$, of the word with rank $r$ is:
\index{frequency}
\index{rank}

\[ f = c r^{-s} \]
%
where $s$ and $c$ are parameters that depend on the language and the
text.
\index{parameter}

If you take the logarithm of both sides of this equation, you get:

\index{logarithm}

\[ \log f = \log c - s \log r \]
%
So if you plot $\log f$ versus $\log r$, you should get
a straight line with slope $-s$ and intercept $\log c$.
\index{log-log plot}

\begin{exercise}

Write a program that reads a text from a file, counts word
frequencies, and prints one line for each word, in descending order of
frequency.  You can test it by downloading an out-of-copyright book in
plain text format from \py{gutenberg.net}.  You might want to remove
punctuation from the words.

If you need some help getting started, you can download
\url{thinkcomplex.com/Pmf.py}, which provides an
object named \py{Hist} that maps from value to frequencies.
\index{Hist@\py{Hist}}

Plot the results and check whether they form
a straight line.  For plotting suggestions, see Section~\ref{pyplot}.
Can you estimate the value of $s$?

You can download my solution from
\url{thinkcomplex.com/Zipf.py}

\end{exercise}



\section{Cumulative distributions}

A distribution is a statistical description of a set of values.
For example, if you collect the population of every city and town
in the U.S., you would have a set of about 14,000 integers.
\index{cumulative distribution}

The simplest description of this set is a list of numbers, which
would be complete but not very informative.  A more concise description
is a statistical summary like the mean and variation, but
that is not a complete description because there are many sets
of values with the same summary statistics.
\index{summary statistic}

One alternative is a histogram, which divides the range of
possible values into ``bins'' and counts the number of values that
fall in each bin.  Histograms are common, so they are
easy to understand, but it is tricky to get the bin size right.  If the bins
are too small, the number of values in each bin is also small,
so the histogram doesn't give much insight.  If the bins are
too large, they lump together a wide range of values, which
obscures details that might be important. \index{histogram} \index{bin size}

A better alternative is a {\bf cumulative distribution function}
(CDF), which maps from a value, $x$, to the fraction of values less
than or equal to $x$.  If you choose a value at random, $CDF(x)$
is the probability that the value you get is less
than or equal to $x$.
\index{cumulative distribution function}
\index{CDF}

For a list of $n$ values, the simplest way to compute CDF is to
sort the values.  Then the CDF of the $i$th value (starting from 1)
is $i/n$.

I have written a class called \py{Cdf} that provides functions
for creating and manipulating CDFs.  You can download it from
\url{thinkcomplex.com/Cdf.py}.
\index{Cdf@\py{Cdf}}

As an example, we'll compute the CDF for the values \{1,2,2,4,5\}:

\begin{code}
import Cdf
cdf = Cdf.MakeCdfFromList([1,2,2,4,5])
\end{code}

\py{MakeCdfFromList} can take any sequence or iterator.
Once you have the \py{Cdf}, you can find the probability, $CDF(x)$, for
a given value:

\begin{code}
prob = cdf.Prob(2)
\end{code}

The result is 0.6, because 3/5 of the values are less than or equal to 2.
You can also compute the value for a given probability:

\begin{code}
value = cdf.Value(0.5)
\end{code}

The value with probability 0.5 is the median, which in this example is 2.

\begin{figure}
\centerline{\includegraphics[width=3.0in]{figs/cdf_example.pdf}}
\caption{CDF of the values \{1,2,2,4,5\}.\label{fig.cdf}}
\end{figure}

To plot the \py{Cdf}, you can use \py{Render}, which returns
a list of value-probability pairs.

\begin{code}
    xs, ps = cdf.Render()
    for x, p in zip(xs, ps):
        print x, p
\end{code}

The result is:

\begin{code}
1 0.0
1 0.2
2 0.2
2 0.6
4 0.6
4 0.8
5 0.8
5 1.0
\end{code}

Each value appears twice.  That way when we plot the
CDF, we get a stair-step pattern.
\index{pyplot@\py{pyplot}}
\index{plotting CDFs}

\begin{code}
import matplotlib.pyplot as pyplot

    xs, ps = cdf.Render()

    pyplot.plot(xs, ps, linewidth=3)
    pyplot.axis([0.9, 5.1, 0, 1])
    pyplot.title('CDF')
    pyplot.xlabel('value, x')
    pyplot.ylabel('probability, CDF(x)')
    pyplot.show()
\end{code}

Figure~\ref{fig.cdf} shows the cumulative distribution
function (CDF), for the values $(1,2,2,4,5)$.

I drew vertical lines at each of the values, which is not
mathematically correct.  To be more rigorous, I should draw
a discontinuous function.

\begin{exercise}

Read the code in \py{Cdf.py}.  What is the order of growth for
\py{MakeCdfFromList} and the methods \py{Prob} and \py{Value}?
\index{order of growth}
\end{exercise}


\section{Continuous distributions}

The distributions we have seen so far are sometimes called
{\bf empirical distributions} because they are based on a
dataset that comes from some kind of empirical observation.
\index{empirical distribution}

An alternative is a {\bf continuous distribution},
which is characterized by a CDF that is a continuous function.
Some of these distributions, like the
Gaussian or normal
distribution, are well known, at least to people who have studied
statistics.  Many real world phenomena can be approximated by
continuous distributions, which is why they are useful.
\index{continuous distribution}
\index{normal distribution}
\index{Gaussian distribution}

For example, if you observe a mass of radioactive material with
an instrument that can detect decay events, the distribution
of times between events will most likely fit an exponential
distribution.  The same is true for any series where
an event is equally likely at any time.
\index{exponential distribution}

The CDF of the exponential distribution is:

\[ CDF(x) = 1 - e^{-\lambda x} \]

The parameter, $\lambda$, determines the mean and variance
of the distribution.  This equation can be used to derive
a simple visual test for whether a dataset can be well
approximated by an exponential distribution.  All you
have to do is plot the {\bf complementary distribution}
on a log-$y$ scale.
\index{parameter}
\index{complementary distribution}

The complementary distribution (CCDF) is just $1 - CDF(x)$;
if you plot the complementary distribution of a dataset
that you think is exponential, you expect to see a function
like:

\[ y = 1 - CDF(x) \sim e^{-\lambda x} \]

If you take the log of both sides of this equation, you get:

\[ \log y \sim -\lambda x \]

So on a log-$y$ scale the CCDF should look like a straight line
with slope $-\lambda$.

\begin{exercise}

Write a function called \py{plot_ccdf} that takes
a list of values and the corresponding list of probabilities
and plots the CCDF on a log-$y$ scale.

To test your function, use \py{expovariate} from the \py{random}
module to generate 100 values from an exponential distribution.  Plot
the CCDF on a log-$y$ scale and see if it falls on a straight line.
\index{random module@\py{random} module}

\end{exercise}


\section{Pareto distributions}

The Pareto distribution is named after the economist Vilfredo
Pareto, who used it to describe the distribution of wealth;
see \url{http://en.wikipedia.org/wiki/Pareto_distribution}.  Since then,
people have used it to describe
phenomena in the natural and social sciences
including sizes of cities and towns, sand particles
and meteorites, forest fires and earthquakes.
\index{Pareto distribution}
\index{Pareto, Vilfredo}

The Pareto distribution is characterized by a CDF with the following
form:

\[ CDF(x) = 1- \left( \frac{x}{x_m} \right) ^{-\alpha} \]

The parameters $x_m$ and $\alpha$ determine the location and shape of
the distribution.  $x_m$ is the minimum possible quantity.
\index{parameter}

Values from a Pareto distribution often have these properties:

\begin{description}

\item[Long tail:] Pareto distributions contain many small values
and a few very large ones.
\index{long tail}

\item[80/20 rule:] The large values in a Pareto distribution are
so large that they make up a disproportionate share of the total.
In the context of wealth, the 80/20 rule says that 20\% of the
people own 80\% of the wealth.
\index{80/20 rule}

\item[Scale free:] Short-tailed distributions are centered around
a typical size, which is called a ``scale.''  For example, the
great majority of adult humans are between 100 and 200 cm in height,
so we could say that the scale of human height is a few hundred
centimeters.  But for long-tailed distributions, there is no
similar range (bounded by a factor of two) that contains the
bulk of the distribution.  So we say that these distributions
are ``scale-free.''
\index{scale}
\index{scale-free}

\end{description}

To get a sense of the difference between the Pareto and Gaussian
distributions, imagine what the world would be like if the
distribution of human height were Pareto.

In Pareto World, the shortest person is 100 cm,
and the median is 150 cm, so that part of the distribution is not
very different from ours.
\index{Pareto World}

But if you generate 6 billion values from this distribution
distribution, the tallest person might
be 100 km---that's what it means to
be scale-free!

There is a simple visual test that indicates whether an empirical
distribution is well-characterized by a Pareto distribution: on a
log-log scale, the CCDF looks like a straight line.  The derivation is
similar to what we saw in the previous section.

The equation for the CCDF is:

\[ y = 1 - CDF(x) \sim \left( \frac{x}{x_m} \right) ^{-\alpha} \]

Taking the log of both sides yields:

\[ \log y \sim -\alpha (\log x - \log x_m ) \]

So if you plot $\log y$ versus $\log x$, it should look like a
straight line with slope $-\alpha$ and intercept $\alpha \log x_m$.
\index{log-log plot}

\begin{exercise}

Write a version of \py{plot_ccdf} that plots the complementary
CCDF on a log-log scale.

To test your function, use \py{paretovariate} from the \py{random}
module to generate 100 values from a Pareto distribution.  Plot
the CCDF on a log-$y$ scale and see if it falls on a straight line.
What happens to the curve as you increase the number of values?
\index{random module@\py{random} module}

\end{exercise}


\begin{exercise}

The distribution of populations for cities and towns has been proposed
as an example of a real-world phenomenon that can be described
with a Pareto distribution.
\index{population}

The U.S. Census Bureau publishes data on the population of every
incorporated city and town in the United States.  I wrote a
small program that downloads this data and converts it into a
convenient form.  You can download it from
\url{thinkcomplex.com/populations.py}.
\index{U.S. Census Bureau}

Read over the program to make sure you know what it does and then
write a program that computes and plots the distribution of
populations for the 14593 cities and towns in the dataset.

Plot the CDF on linear and log-$x$ scales so you can get a sense of
the shape of the distribution.  Then plot the CCDF on a log-log scale
to see if it has the characteristic shape of a Pareto distribution.

What conclusion do you draw about the distribution of sizes
for cities and towns?

\end{exercise}

\newcommand{\Barabasi}{Barab\'{a}si}

\section{\Barabasi~and Albert}
\label{scale.free}

In 1999 \Barabasi~and Albert published a paper in {\em Science},
``Emergence of Scaling in Random Networks,'' that characterizes the
structure (also called ``topology'') of several real-world networks,
including graphs that represent the interconnectivity of movie actors,
world-wide web (WWW) pages, and elements in the electrical power grid
in the western United States.  You can download the paper from
\url{http://www.sciencemag.org/content/286/5439/509}.
\index{topology} \index{movie actor} \index{world-wide web}
\index{electrical power grid}

They measure the degree (number of connections) of each node and
compute $P(k)$, the probability that a vertex has degree $k$; then
they plot $P(k)$ versus $k$ on a log-log scale.  The tail of the plot
fits a straight line, so they conclude that it obeys a {\bf power
  law}; that is, as $k$ gets large, $P(k)$ is asymptotic to
$k^{- \gamma}$, where $\gamma$ is a parameter that determines the rate
of decay.
\index{degree}
\index{power law}

They also propose a model that generates random graphs with the same
property.  The essential features of the model, which distinguish it
from the \Erdos-\Renyi~ model and the Watts-Strogatz model, are:
\index{generative model}

\begin{description}

\item[Growth:]  Instead of starting with a fixed number of vertices,
\Barabasi~and Albert start with a small graph and add vertices gradually.

\item[Preferential attachment:] When a new edge is created, it is
more likely to connect to a vertex that already has a large number
of edges.  This ``rich get richer'' effect is characteristic of
the growth patterns of some real-world networks.
\index{preferential attachment}
\index{rich get richer}

\end{description}

Finally, they show that graphs generated by this model have a
distribution of degrees that obeys a power law.  Graphs that
have this property are sometimes called {\bf scale-free networks};
see \url{http://en.wikipedia.org/wiki/Scale-free_network}.
That name can be confusing because it is the distribution
of degrees that is scale-free, not the network.
\index{scale-free network}

In order to maximize confusion, distributions that obey the power law
are sometimes called {\bf scaling distributions} because they are
invariant under a change of scale.  That means that if you change the
units the quantities are expressed in, the slope parameter, $\gamma$,
doesn't change.  You can read \url{http://en.wikipedia.org/wiki/Power_law} for
the details, but it is not important for what we are doing here.
\index{scaling distribution}


\begin{exercise}

This exercise asks you to make connections between the Watts-Strogatz (WS)
and \Barabasi-Albert (BA) models:
\index{Watts-Strogatz model}
\index{Barabasi-Albert model@\Barabasi-Albert model}

\begin{enumerate}

\item Read \Barabasi~and Albert's paper and implement their algorithm
for generating graphs.  See if you can replicate their Figure 2(A),
which shows $P(k)$ versus $k$ for a graph with 150 000 vertices.

\item Use the WS model to generate the largest graph you can in
a reasonable amount of time.  Plot $P(k)$ versus $k$ and see if
you can characterize the tail behavior.

\item Use the BA model to generate a graph with about 1000 vertices
and compute the characteristic length and clustering coefficient
as defined in the Watts and Strogatz paper.  Do scale-free networks
have the characteristics of a small-world graph?

\end{enumerate}

\end{exercise}


\section{Zipf, Pareto and power laws}


At this point we have seen three phenomena that yield a straight line
on a log-log plot:
\index{log-log plot}

\begin{description}

\item[Zipf plot:] Frequency as a function of rank.
\index{Zipf plot}

\item[Pareto CCDF:] The complementary CDF of a Pareto distribution.
\index{complementary CDF}

\item[Power law plot:] A histogram of frequencies.
\index{power law}

\end{description}

The similarity in these plots is not a coincidence; these
visual tests are closely related.

Starting with a power-law distribution, we have:

\[ P(k) \sim k^{- \gamma} \]

If we choose a random node in a scale free network,
$P(k)$ is the probability that its degree equals $k$.
\index{degree}

The cumulative distribution function, $CDF(k)$, is the probability
that the degree is less than or equal to $k$, so we can
get that by summation:

\[ CDF(k) = \sum_{i=0}^k P(i) \]

For large values of $k$ we can approximate the summation with
an integral:

\[ \sum_{i=0}^k i^{- \gamma} \sim \int_{i=0}^k i^{- \gamma} =
\frac{1}{\gamma -1} (1 - k^{-\gamma + 1}) \]

To make this a proper CDF we could normalize it so that it
goes to 1 as $k$ goes to infinity, but that's not necessary,
because all we need to know is:

\[ CDF(k) \sim 1 - k^{-\gamma + 1} \]

Which shows that the distribution of $k$ is asymptotic to a
Pareto distribution with $\alpha = \gamma - 1$.

Similarly, if we start with a straight line on a Zipf plot,
we have\footnote{This derivation follows
Adamic, ``Zipf, power law and
Pareto---a ranking tutorial,'' available at
\url{www.hpl.hp.com/research/idl/papers/ranking/ranking.html}}:

\[ f = c r^{-s} \]

Where $f$ is the frequency of the word with rank $r$.  Inverting
this relationship yields:
\index{rank}

\[ r = (f/c)^{-{1/s}} \]

Now subtracting 1 and dividing through by the number of different
words, $n$, we get

\[ \frac{r-1}{n} = \frac{(f/c)^{-{1/s}}}{n} - \frac{1}{n} \]

Which is only interesting because if $r$ is the rank of a word,
then $(r-1)/n$ is the fraction of words with lower ranks, which is
the fraction of words with higher frequency, which is the
CCDF of the distribution of frequencies:

\[ CCDF(x) = \frac{(f/c)^{-{1/s}}}{n} - \frac{1}{n} \]

To characterize the asymptotic behavior
for large $n$ we can ignore $c$ and $1/n$, which yields:

\[ CCDF(x) \sim f^{-{1/s}} \]

Which shows that if a set of words obeys Zipf's law, the
distribution of their frequencies is asymptotic to a
Pareto distribution with $\alpha = 1/s$.

So the three visual tests are mathematically equivalent; a dataset
that passes one test will pass all three.  But as a practical
matter, the power law plot is noisier than the other two, because
it is the derivative of the CCDF.
\index{noise}

The Zipf and CCDF plots are more robust, but Zipf's law is only
applicable to discrete data (like words), not continuous quantities.
CCDF plots work with both.
\index{robust}

For these reasons---robustness and generality---I recommend
using CCDFs.


\begin{exercise}

The Stanford Large Network Dataset Collection is a repository of
datasets from a variety of networks, including social networks,
communication and collaboration, Internet and road networks.
See \url{http://snap.stanford.edu/data/index.html}.
\index{Stanford Large Network Dataset Collection}

Download one of these datasets and explore.  Is there evidence
of small-world behavior?  Is the network scale-free?  What else
can you discover?

\end{exercise}


\section{Explanatory models}
\label{model1}

\begin{figure}
\centerline{\includegraphics[height=2in]{figs/model.pdf}}
\caption{The logical structure of an explanatory model.\label{fig.model}}
\end{figure}

We started the discussion of networks with Milgram's Small World
Experiment, which shows that path lengths in social
networks are surprisingly small; hence, ``six degrees of separation''.
\index{six degrees}

When we see something surprising, it is natural to ask ``Why?''  but
sometimes it's not clear what kind of answer we are looking for.  One
kind of answer is an {\bf explanatory model} (see
Figure~\ref{fig.model}).  The logical structure of an explanatory
model is: \index{explanatory model}

\begin{enumerate}

\item In a system, S, we see something observable, O, that warrants
  explanation.
\index{system}
\index{observable}

\item We construct a model, M, that is analogous to the system; that
  is, there is a correspondence between the elements of the model and
  the elements of the system.
\index{model}

\item By simulation or mathematical derivation, we show that the model
  exhibits a behavior, B, that is analogous to O.
\index{behavior}

\item We conclude that S exhibits O {\em because} S is similar to M, M
  exhibits B, and B is similar to O.

\end{enumerate}

At its core, this is an argument by analogy, which says that if two
things are similar in some ways, they are likely to be similar in
other ways.
\index{analogy}
\index{argument by analogy}

Argument by analogy can be useful, and explanatory models can be
satisfying, but they do not constitute a proof in the mathematical
sense of the word.
\index{proof}
\index{mathematical proof}

Remember that all models leave out, or ``abstract away''
details that we think are unimportant.  For any system there
are many possible models that include or ignore different features.
And there might be models that exhibit different behaviors,
B, B' and B'', that are similar to O in different ways.
In that case, which model explains O?
\index{abstract model}

The small world phenomenon is an example: the
Watts-Strogatz (WS) model and the \Barabasi-Albert (BA) model
both exhibit small world behavior, but they offer different
explanations:

\begin{itemize}

\item The WS model suggests that social networks are ``small'' because
  they include both strongly-connected clusters and ``weak ties'' that
  connect clusters (see \url{http://en.wikipedia.org/wiki/Mark_Granovetter}).
\index{weak ties}

\item The BA model suggests that social networks are small because
  they include nodes with high degree that act as hubs, and that
  hubs grow, over time, due to preferential attachment.
\index{preferential attachment}

\end{itemize}

As is often the case in young areas of science, the problem is
not that we have no explanations, but too many.

\begin{exercise}

Are these explanations compatible; that is, can they both be right?
Which do you find more satisfying as an explanation, and why?

Is there data you could collect, or experiments you could perform,
that would provide evidence in favor of one model over the other?

Choosing among competing models is the topic of Thomas Kuhn's
essay, ``Objectivity, Value Judgment, and Theory Choice.''
Kuhn was a historian of science who wrote {\em The
Structure of Scientific Revolutions} in 1962, and spent the rest of
his life explaining what he meant to say.
\index{Kuhn, Thomas}
\index{Structure of Scientific Revolutions@{\it The Structure of Scientific Revolutions}}
\index{theory choice}
\index{objectivity}

What criteria does Kuhn propose for choosing among competing models?
Do these criteria influence your opinion about the WS and BA models?
Are there other criteria you think should be considered?

\end{exercise}



\chapter{Cellular Automata}
\label{automata}

A {\bf cellular automaton} is a model of a world with very simple physics.
``Cellular'' means that the space is divided into discrete chunks,
called cells.  An ``automaton'' is a machine that performs
computations---it could be a real machine, but more often the
``machine'' is a mathematical abstraction or a computer simulation.
\index{cellular automaton}

Automata are governed by rules that determine how the system evolves
in time.  Time is divided into discrete steps, and the rules
specify how to compute the state of the world during the next time
step based on the current state.
\index{time step}

As a trivial example, consider a cellular automaton (CA) with
a single cell.  The state of the cell is an integer represented
with the variable $x_i$, where the subscript $i$ indicates
that $x_i$ is the state of the system during time step $i$.
As an initial condition, $x_0 = 0$.
\index{state}

Now all we need is a rule.  Arbitrarily, I'll pick $x_i = x_{i-1} + 1$,
which says that after each time step, the state of the CA gets
incremented by 1.  So far, we have a simple CA that performs
a simple calculation: it counts.
\index{rule}

But this CA is atypical; normally the number of
possible states is finite.  To bring it into line, I'll choose the
smallest interesting number of states, 2, and another simple rule,
$x_i = (x_{i-1} + 1) \% 2$, where $\%$ is the remainder (or
modulus) operator.

This CA performs a simple calculation: it blinks.  That is,
the state of the cell switches between 0 and 1 after every time step.

Most CAs are {\bf deterministic}, which means that rules do not
have any random elements; given the same initial state, they
always produce the same result.  There are also nondeterministic
CAs, but I will not address them here.
\index{deterministic}



\section{Stephen Wolfram}

The CA in the previous section was 0-dimensional and it wasn't very
interesting.  But 1-dimensional CAs turn out to be surprisingly
interesting.
\index{Wolfram, Stephen}
\index{1-D cellular automaton}

In the early 1980s Stephen Wolfram published a series of papers
presenting a systematic study of 1-dimensional CAs.  He identified
four general categories of behavior, each more interesting than
the last.

To say that a CA has dimensions is to say that the cells are
arranged in a contiguous space so that some of them are
considered ``neighbors.''  In one dimension, there are three
natural configurations:
\index{neighbor}

\begin{description}

\item[Finite sequence:] A finite number of cells arranged
in a row.  All cells except the first and last have two neighbors.

\item[Ring:] A finite number of cells arranged
in a ring.  All cells have two neighbors.

\item[Infinite sequence:] An infinite number of cells arranged
in a row.

\end{description}

The rules that determine how the system evolves in time are
based on the notion of a ``neighborhood,'' which is the set
of cells that determines the next state of a given cell.
Wolfram's experiments use a 3-cell neighborhood: the cell itself
and its left and right neighbors.
\index{neighborhoos}

In these experiments, the cells have two states, denoted 0 and 1,
so the rules can be summarized by a table that maps from the
state of the neighborhood (a tuple of 3 states) to the next state
for the center cell.
The following table shows an example:
\index{rule table}
\index{state}

\centerline{
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline
prev & 111 & 110 & 101 & 100 & 011 & 010 & 001 & 000 \\
\hline
next   & 0   & 0   & 1   & 1   & 0   & 0   & 1   & 0 \\
\hline
\end{tabular}}

The row first shows the eight states a
neighborhood can be in.  The second row shows the state of
the center cell during the next time step.  As a concise encoding
of this table, Wolfram suggested reading the bottom row
as a binary number.  Because 00110010 in binary is 50 in
decimal, Wolfram calls this CA ``Rule 50.''
\index{enumeration}

\begin{figure}
\centerline{\includegraphics[height=1.5in]{figs/rule-50-10.pdf}}
\caption{Rule 50 after 10 time steps.\label{fig.rule50}}
\end{figure}

Figure~\ref{fig.rule50} shows the effect of Rule 50 over 10
time steps.  The first row shows the state of the system during the first
time step; it starts with one cell ``on'' and the rest ``off''.
The second row shows the state of the system during the
next time step, and so on.

The triangular shape in the figure is typical of these CAs; is it a
consequence of the shape of the neighborhood.  In one time step, each
cell influences the state of one neighbor in either direction.  During
the next time step, that influence can propagate one more cell in each
direction.  So each cell in the past has a ``triangle of influence''
that includes all of the cells that can be affected by it.
\index{triangle of influence}


\section{Implementing CAs}

\begin{figure}
\centerline{\includegraphics[height=1.75in]{figs/array.pdf}}
\caption{A list of lists (left) and a Numpy array (right).\label{fig.array}}
\end{figure}

To generate the previous figure, I wrote a Python program that
implements and draws CAs.
You can download my code from \url{thinkcomplex.com/CA.py}.
and \url{thinkcomplex.com/CADrawer.py}.
\index{implementing cellular automata}

To store the state of the CA, I use a NumPy array.  An array is a
multi-dimensional data structure whose elements are all the same type.
It is similar to a nested list, but usually smaller and faster.  
Figure~\ref{fig.array} shows why.
The diagram on the left shows a list of lists of integers; each
dot represents a reference, which takes up 4--8 bytes.  To access
one of the integers, you have to follow two references.
\index{NumPy}
\index{array}
\index{nested list}
\index{reference}

The diagram on the right shows an array of the same integers.  Because
the elements are all the same size, they can be stored contiguously in
memory.  This arrangement saves space because it doesn't use
references, and it saves time because the location of an element can
be computed directly from the indices; there is no need to follow a
series of references.

Here is a CA object that uses a NumPy array:
\index{CA@\py{CA}}

\begin{code}
import numpy

class CA(object):

    def __init__(self, rule, n=100, ratio=2):
        self.table = make_table(rule)
        self.n = n
        self.m = ratio*n + 1
        self.array = numpy.zeros((n, self.m), dtype=numpy.int8)
        self.next = 0
\end{code}

\py{rule} is an integer in the range 0-255, which represents the
CA rule table using Wolfram's encoding.  \py{make_table} converts
the rule to a dictionary that maps from neighborhood states to cell
states.  For example, in Rule 50 the table maps from
$(1,1,1)$ to 0.

\py{n} is the number of rows in the array, which is the number
of time steps we will compute.  \py{m} is the number of columns,
which is the number of cells.  To get started, I'll implement
a finite array of cells.

\py{zeros} is provided by NumPy; it creates a new array with
the given dimensions, \py{n} by \py{m}; \py{dtype} stands
for ``data type,'' and it specifies the type of the array elements.
\py{int8} is an 8-bit integer, so we are limited to 256 states,
but that's no problem: we only need two.
\index{zeros@\py{zeros}}

\py{next} is the index of the next time step.

There are two common starting conditions for CAs: a single cell, or a row
of random cells.  \py{start_single} initializes the first row with
a single cell and increments \py{next}:

\begin{code}
    def start_single(self):
        """Starts with one cell in the middle of the top row."""
        self.array[0, self.m/2] = 1
        self.next += 1
\end{code}
%
The array index is a tuple that species the row and column of the cell,
in that order.
\index{array indexing}

\py{step} computes the next state of the CA:

\begin{code}
    def step(self):
        i = self.next
        self.next += 1

        a = self.array
        t = self.table
        for j in xrange(1,self.m-1):
            a[i,j] = t[tuple(a[i-1, j-1:j+2])]
\end{code}

\py{i} is the time step and the index of the row we are about to
compute.  \py{j} loops through the cells, skipping the first and
last, which are always off.

Arrays support slice operations, so \py{self.array[i-1, j-1:j+2]}
gets three elements from row \py{i-1}.
Then we look up the neighborhood tuple in the table, get
the next state, and store it in the array.
\index{array slice}

Array indexing is constant time, so \py{step} is linear in $n$.
Filling in the whole array is $O(nm)$.
\index{order of growth}

You can read more about NumPy and arrays at
\url{scipy.org/Tentative_NumPy_Tutorial}.


\section{CADrawer}

An {\bf abstract class} is a class definition that specifies the
interface for a set of methods without providing an implementation.
Child classes extend the abstract class and implement the incomplete
methods.  See \url{http://en.wikipedia.org/wiki/Abstract_type}.
\index{abstract class}

As an example, \py{CADrawer}
defines an interface for drawing CAs; here is the definition:
\index{implement}
\index{interface}
\index{CADrawer@\py{CADrawer}}

\begin{code}
class Drawer(object):
    """Drawer is an abstract class that should not be instantiated.
    It defines the interface for a CA drawer; child classes of Drawer
    should implement draw, show and save.

    If draw_array is not overridden, the child class should provide
    draw_cell.
    """
    def __init__(self):
        msg = 'CADrawer is an abstract type and should not be instantiated.'
        raise UnimplementedMethodException, msg

    def draw(self, ca):
        """Draws a representation of cellular automaton (CA).
        This function generally has no visible effect."""
        raise UnimplementedMethodException

    def draw_array(self, a):
        """Iterate through array (a) and draws any non-zero cells."""
        for i in xrange(self.rows):
            for j in xrange(self.cols):
                if a[i,j]:
                    self.draw_cell(j, self.rows-i-1)

    def draw_cell(self, ca):
        """Draws a single cell.
        Not required for all implementations."""
        raise UnimplementedMethodException

    def show(self):
        """Displays the representation on the screen, if possible."""
        raise UnimplementedMethodException

    def save(self, filename):
        """Saves the representation of the CA in filename."""
        raise UnimplementedMethodException
\end{code}

Abstract classes should not be instantiated; if you try, you
get an \py{UnimplementedMethodException}, which is a simple
extension of \py{Exception}:
\index{UnimplementedMethodException@\py{UnimplementedMethodException}}

\begin{code}
class UnimplementedMethodException(Exception):
    """Used to indicate that a child class has not implemented an
    abstract method."""
\end{code}

To instantiate a \py{CADrawer} you have to define a child class
that implements the methods, then instantiate the child.
\index{instantiate}

\py{CADrawer.py} provides three implementations, one that uses
\py{pyplot}, one that uses the Python Imaging Library (PIL), and
one that generates Encapsulated Postscript (EPS).
\index{pyplot}
\index{Python Imaging Library}
\index{PIL}
\index{Postscript}
\index{EPS}

Here is an example that uses \py{PyplotDrawer} to display a
CA on the screen:

\begin{code}
    ca = CA(rule, n)
    ca.start_single()
    ca.loop(n-1)

    drawer = CADrawer.PyplotDrawer()
    drawer.draw(ca)
    drawer.show()
\end{code}

\begin{exercise}

Download \url{thinkcomplex.com/CA.py} and \url{thinkcomplex.com/CADrawer.py}
and confirm that they run on your system; you might have to install
additional Python packages.

Create a new class called \py{CircularCA} that extends
\py{CA} so that the cells are arranged in a ring.
Hint: you might find it useful to add a column of ``ghost cells'' to
the array.
\index{CircularCA}
\index{ghost cells}

You can download my solution from
\url{thinkcomplex.com/CircularCA.py}

\end{exercise}


\section{Classifying CAs}

\begin{figure}
\centerline{\includegraphics[height=1.5in]{figs/rule-18-64.pdf}}
\caption{Rule 18 after 64 steps.\label{fig.rule18}}
\end{figure}

Wolfram proposes that the behavior of CAs can be grouped
into four classes.  Class 1 contains the simplest (and least
interesting) CAs, the ones that evolve from almost any starting
condition to the same uniform pattern.  As a trivial example,
Rule 0 always generates an empty pattern after one time step.
\index{classifying cellular automata}

Rule 50 is an example of Class 2.  It generates a simple pattern with
nested structure; that is, the pattern contains many smaller versions
of itself.  Rule 18 makes the nested structure even clearer;
Figure~\ref{fig.rule18} shows what it looks like after 64 steps.
\index{Rule 18}
 
This pattern resembles the Sierpi\'{n}ski triangle, which
you can read about at \url{http://en.wikipedia.org/wiki/Sierpinski_triangle}.
\index{Sierpi\'{n}ski triangle}

Some Class 2 CAs generate patterns that are intricate and
pretty, but compared to Classes 3 and 4, they are relatively
simple.


\section{Randomness}

\begin{figure}
\centerline{\includegraphics[height=2.5in]{figs/rule-30-100.pdf}}
\caption{Rule 30 after 100 time steps.\label{fig.rule30}}
\end{figure}

Class 3 contains CAs that generate randomness.
Rule 30 is an example; Figure~\ref{fig.rule30} shows what it looks like
after 100 time steps.
\index{randomness}
\index{Class 3 behavior}
\index{Rule 30}

Along the left side there is an apparent pattern, and on the right
side there are triangles in various sizes, but the center seems
quite random.  In fact, if you take the center column and treat it as a
sequence of bits, it is hard to distinguish from a truly random
sequence.  It passes many of the statistical tests people use
to test whether a sequence of bits is random.

Programs that produce random-seeming numbers are called
{\bf pseudo-random number generators} (PRNGs).  They are not considered
truly random because
\index{pseudo-random number generator}
\index{PRNG}

\begin{itemize}

\item Many of them produce sequences with regularities that
can be detected statistically.  For example, the original implementation
of \py{rand} in the C library used a linear congruential generator
that yielded sequences with easily detectable serial correlations.
\index{linear congruential generator}

\item Any PRNG that uses a finite amount
of state (that is, storage) will eventually repeat itself.  One of the
characteristics of a generator is the {\bf period} of this
repetition.
\index{period}

\item The underlying process is fundamentally deterministic,
unlike some physical processes, like radioactive decay and
thermal noise, that are considered to be fundamentally
random.
\index{deterministic}

\end{itemize}

Modern PRNGs produce sequences that are statistically
indistinguishable from random, and they can be implemented with with
periods so long that the universe will collapse before they repeat.
The existence of these generators raises the question of whether there
is any real difference between a good quality pseudo-random sequence
and a sequence generated by a ``truly'' random process.  In {\em A New
  Kind of Science}, Wolfram argues that there is not (pages 315--326).
\index{New Kind of Science@{\it A New Kind of Science}}

\begin{exercise}

This exercise asks you to implement and test several PRNGs.

\begin{enumerate}

\item Write a program that implements one of the linear congruential
generators described at
\url{http://en.wikipedia.org/wiki/Linear_congruential_generator}).

\item Download \py{DieHarder}, a random number test suite, from
\url{http://phy.duke.edu/~rgb/General/rand_rate.php} and use it to
test your PRNG.  How does it do?
\index{DieHarder}

\item Read the documentation of Python's \py{random} module.
What PRNG does it use?  Test it using DieHarder.
\index{random module@\py{random} module}

\item Implement a Rule 30 CA on a ring with a few hundred cells,
run it for as many time steps as you can in a reasonable amount
of time, and output the center column as a sequence of bits.
Test it using DieHarder.
\index{Rule 30}

\end{enumerate}

\end{exercise}


\section{Determinism}
\label{determinism}

The existence of Class 3 CAs is surprising.  To understand how
surprising, it is useful to consider philosophical
{\bf determinism} (see \url{http://en.wikipedia.org/wiki/Determinism}).
Most philosophical stances are hard to define precisely because
they come in a variety of flavors.  I often find it useful
to define them with a list of statements ordered from weak
to strong:
\index{determinism}

\begin{description}

\item[D1:] Deterministic models can make accurate predictions
for some physical systems.

\item[D2:] Many physical systems can be modeled by deterministic
processes, but some are intrinsically random.

\item[D3:] All events are caused by prior events, but many
physical systems are nevertheless fundamentally unpredictable.

\item[D4:] All events are caused by prior events, and can (at
least in principle) be predicted.
\index{causation}

\end{description}

My goal in constructing this range is to make D1 so weak that
virtually everyone would accept it, D4 so strong that almost no one
would accept it, with intermediate statements that some people accept.

The center of mass of world opinion swings along this range in
response to historical developments and scientific discoveries.  Prior
to the scientific revolution, many people regarded the working of the
universe as fundamentally unpredictable or controlled by supernatural
forces.  After the triumphs of Newtonian mechanics, some optimists
came to believe something like D4; for example, in 1814 Pierre-Simon
Laplace wrote
\index{Newtonian mechanics}
\index{Laplace, Pierre-Simon}

\begin{quote}
We may regard the present state of the universe as the effect of its
past and the cause of its future. An intellect which at a certain
moment would know all forces that set nature in motion, and all
positions of all items of which nature is composed, if this intellect
were also vast enough to submit these data to analysis, it would
embrace in a single formula the movements of the greatest bodies of
the universe and those of the tiniest atom; for such an intellect
nothing would be uncertain and the future just like the past would be
present before its eyes.
\end{quote}

This ``intellect'' came to be called ``Laplace's Demon''.
See \url{http://en.wikipedia.org/wiki/Laplace's_demon}.  The word
``demon'' in this context has the sense of ``spirit,'' with no
implication of evil.
\index{Laplace's Demon}

Discoveries in the 19th and 20th centuries gradually dismantled
this hope.  The thermodynamic concept of entropy, radioactive decay,
and quantum mechanics posed successive challenges to strong
forms of determinism.
\index{entropy}
\index{radioactive decay}
\index{quantum mechanics}

In the 1960s chaos theory showed that in some deterministic systems
prediction is only possible over short time scales,  limited by
the precision of measurement of initial conditions.
\index{chaos}

Most of these systems are continuous in space (if not time) and
nonlinear, so the complexity of their behavior is not entirely
surprising.  Wolfram's demonstration of complex behavior in simple
cellular automata is more surprising---and disturbing, at least to a
deterministic world view.
\index{complex behavior}
\index{simple rules}

So far I have focused on scientific challenges to determinism, but the
longest-standing objection is the conflict between
determinism and human free will.  Complexity science provides
a possible resolution of this apparent conflict; we come
back to this topic in Section~\ref{free.will}.
\index{free will}


\section{Structures}

\begin{figure}
\centerline{\includegraphics[height=2.5in]{figs/rule-110-100.pdf}}
\caption{Rule 110 after 100 time steps.\label{fig.rule110}}
\end{figure}

The behavior of Class 4 CAs is even more surprising.  Several 1-D CAs,
most notably Rule 110, are {\bf Turing complete}, which means that
they can compute any computable function.  This property, also called
{\bf universality}, was proved by Matthew Cook in 1998.  See
\url{http://en.wikipedia.org/wiki/Rule_110}.
\index{Turing complete}
\index{universality}
\index{Cook, Matthew}

Figure~\ref{fig.rule110} shows what Rule 110 looks like with an initial
condition of a single cell and 100 time steps.
At this time scale it is not apparent that anything special is
going on.  There are some regular patterns but also some features
that are hard to characterize.
\index{Rule 110}

Figure~\ref{rule110} shows a bigger picture, starting with a random
initial condition and 600 time steps:

\begin{figure}
\centerline{\includegraphics[width=5.5in,height=5.5in]{figs/rule-110-600-random.pdf}}
\caption{Rule 110 with random initial conditions and 600 time steps.\label{rule110}}
\end{figure}

After about 100 steps the background settles into a simple repeating
pattern, but there are a number of persistent structures that appear
as disturbances in the background.  Some of these structures
are stable, so they appear as vertical lines.  Others translate in
space, appearing as diagonals with different slopes, depending on
how many time steps they take to shift by one column.  These
structures are called {\bf spaceships}.
\index{spaceships}

Collisions between spaceships yield different results
depending on the types of the spaceships and the phase they are in
when they collide.  Some collisions annihilate both ships; others
leave one ship unchanged; still others yield one or more ships of
different types.

These collisions are the basis of computation in a Rule 110 CA.  If
you think of spaceships as signals that propagate on wires, and
collisions as gates that compute logical operations like AND and OR,
you can see what it means for a CA to perform a computation.

\begin{exercise}

This exercise asks you to experiment with Rule 110 and see how
many spaceships you can find.

\begin{enumerate}

\item Modify your program from the previous exercises so it starts
  with an initial condition that yields the stable background
  pattern.

\item Modify the initial condition by adding different patterns in the
  center of the row and see which ones yield spaceships.  You might
  want to enumerate all possible patterns of $n$ bits, for some
  reasonable value of $n$.  For each spaceship, can you find the
  period and rate of translation?  What is the biggest spaceship you
  can find?

\end{enumerate}

\end{exercise}


\section{Universality}

To understand universality, we have to understand computability
theory, which is about models of computation and what they compute.
\index{universality}

One of the most general models of computation is the Turing machine,
which is an abstract computer proposed by Alan Turing in 1936.  A
Turing machine is a 1-D CA, infinite in both directions, augmented
with a read-write head.  At any time, the head is positioned over a
single cell.  It can read the state of that cell (usually there are
only two states) and it can write a new value into the cell.
\index{Turing machine}
\index{Turing, Alan}

In addition, the machine has a register, which records the state
of the machine (one of a finite number of states), and a table
of rules.  For each machine state and cell state, the table
specifies an action.  Actions include modifying the cell
the head is over and moving one cell to the left or right.
\index{register}
\index{tape}
\index{read/write head}
\index{cell}

A Turing machine is not a practical design for a computer, but it
models common computer architectures.  For a given program running on
a real computer, it is possible (at least in principle) to construct a
Turing machine that performs an equivalent computation.

The Turing machine is useful because it is possible to characterize
the set of functions that can be computed by a Turing machine,
which is what Turing did.  Functions in this set are
called Turing computable.
\index{computable function}

To say that a Turing machine can compute any Turing-computable
function is a {\bf tautology}: it is true by definition.  But
Turing-computability is more interesting than that.
\index{tautology}

It turns out that just about every reasonable model of computation
anyone has come up with is Turing complete; that is, it can compute
exactly the same set of functions as the Turing machine.
Some of these models, like lamdba calculus, are very different
from a Turing machine, so their equivalence is surprising.
\index{lambda calculus}
\index{Church-Turing thesis}

This observation led to the Church-Turing Thesis, which is essentially
a definition of what it means to be computable.  The ``thesis'' is
that Turing-computability is the right, or at least natural,
definition of computability, because it describes the power of such a
diverse collection of models of computation.

The Rule 110 CA is yet another model of computation, and remarkable
for its simplicity.  That it, too, turns out to be universal lends
support to the Church-Turing Thesis.

In {\em A New Kind of Science}, Wolfram states a variation of this
thesis, which he calls the ``principle of computational equivalence:''
\index{principle of computational equivalence}
\index{New Kind of Science@{\it A New Kind of Science}}

\begin{quote}
Almost all processes that are not obviously simple can be viewed as
computations of equivalent sophistication.

More specifically, the principle of computational equivalence says
that systems found in the natural world can perform computations up to
a maximal (``universal'') level of computational power, and that most
systems do in fact attain this maximal level of computational
power. Consequently, most systems are computationally
equivalent (see
  \url{mathworld.wolfram.com/PrincipleofComputationalEquivalence.html}).
\end{quote}

Applying these definitions to CAs, Classes 1 and 2 are ``obviously
simple.''  It may be less obvious that Class 3 is simple, but in a way
perfect randomness is as simple as perfect order; complexity happens
in between.  So Wolfram's claim is that Class 4 behavior is common in
the natural world, and that almost all of the systems that manifest it
are computationally equivalent.
\index{Class 4 behavior}

\begin{exercise}

The goal of this exercise is to implement a Turing machine.
See \url{http://en.wikipedia.org/wiki/Turing_machine}.

\begin{enumerate}

\item Start with a copy of \py{CA.py} named \py{TM.py}.
Add attributes to represent the location of the head, the action
table and the state register.

\item Override \py{step} to implement a Turing machine update.

\item For the action table, use the rules for a 3-state busy beaver.
\index{busy beaver}

\item Write a class named \py{TMDrawer} that generates an
image that represents the state of the tape and the position and
state of the head.  For one example of what that might look like,
see \url{http://mathworld.wolfram.com/TuringMachine.html}.

\end{enumerate}

\end{exercise}


\section{Falsifiability}

Wolfram holds that his principle is a stronger claim than the
Church-Turing Thesis because it is about the natural world rather
than abstract models of computation.  But saying that natural processes
``can be viewed as computations'' strikes me as a statement about
theory choice more than a hypothesis about the natural world.
\index{falsifiability}

Also, with qualifications like
``almost'' and undefined terms like ``obviously simple,'' his
hypothesis may be {\bf unfalsifiable}.  Falsifiability is
an idea from the philosophy of science, proposed by Karl Popper
as a demarcation between scientific hypotheses and pseudoscience.
A hypothesis is falsifiable if there is an experiment, at least
in the realm of practicality, that would contradict the hypothesis
if it were false.
\index{Popper, Karl}

For example, the claim that all life on earth is descended
from a common ancestor is falsifiable because it makes specific
predictions about similarities in the genetics of modern species
(among other things).  If we discovered a new species whose
DNA was almost entirely different from ours, that would
contradict (or at least bring into question) the theory of
universal common descent.
\index{universal common descent}

On the other hand, ``special creation,'' the claim that all species
were created in their current form by a supernatural agent, is
unfalsifiable because there is nothing that we could observe about the
natural world that would contradict it.  Any outcome of any experiment
could be attributed to the will of the creator.
\index{special creation}

Unfalsifiable hypotheses can be appealing because
they are impossible to refute.  If your goal is never to be
proved wrong, you should choose hypotheses that are as
unfalsifiable as possible.

But if your goal is to make reliable predictions about the world---and
this is at least one of the goals of science---then unfalsifiable
hypotheses are useless.  The problem is that they have
no consequences (if they had consequences, they would be
falsifiable).
\index{prediction}

For example, if the theory of special creation were true, what good
would it do me to know it?  It wouldn't tell me anything about the
creator except that he has an ``inordinate fondness for beetles''
(attributed to J.~B.~S.~Haldane).  And unlike the
theory of common descent, which informs many areas of science
and bioengineering, it would be of no use for understanding
the world or acting in it.
\index{Haldane, J.~B.~S.}
\index{beetles}

\begin{exercise}

Falsifiability is an appealing and useful idea, but among
philosophers of science it is not generally accepted
as a solution to the demarcation problem, as Popper claimed.

Read \url{http://en.wikipedia.org/wiki/Falsifiability} and answer the following
questions.

\begin{enumerate}

\item What is the demarcation problem?
\index{demarcation problem}

\item How, according to Popper, does falsifiability solve the
demarcation problem?

\item Give an example of two theories, one considered scientific
and one considered unscientific, that are successfully distinguished
by the criterion of falsifiability.

\item Can you summarize one or more of the objections that
philosophers and historians of science have raised to Popper's
claim?

\item Do you get the sense that practicing philosophers think
highly of Popper's work?

\end{enumerate}

\end{exercise}



\section{What is this a model of?}
\label{model3}

\begin{figure}
\centerline{\includegraphics[height=2.5in]{figs/model3.pdf}}
\caption{The logical structure of a simple physical model.\label{fig.model3}}
\end{figure}

Some cellular automata are primarily mathematical artifacts.
They are interesting because they are surprising,
or useful, or pretty, or because they provide tools for
creating new mathematics (like the Church-Turing thesis).
\index{mathematics}

But it is not clear that they are models of physical systems.  And if
they are, they are highly abstracted, which is to say that they are
not very detailed or realistic.
\index{physical model}

For example, some species of cone
snail produce a
pattern on their shells that resembles the patterns generated by
cellular automata (seee \url{en.wikipedia.org/wiki/Cone_snail}).
So it is natural to suppose that a CA is a model of the mechanism that
produces patterns on shells as they grow.  But, at least initially, it
is not clear how the elements of the model (so-called cells,
communication between neighbors, rules) correspond to the elements of
a growing snail (real cells, chemical signals, protein interaction
networks).
\index{cone snail}
\index{abstract model}

For conventional physical models, being realistic is a virtue, at
least up to a point.  If the elements of a model correspond to the
elements of a physical system, there is an obvious analogy between the
model and the system.  In general, we expect a model that is more
realistic to make better predictions and to provide more believable
explanations.
\index{realistic model}

Of course, this is only true up to a point.  Models that are
more detailed are harder to work with, and usually less
amenable to analysis.  At some point, a model becomes so complex
that it is easier to experiment with the system.

At the other extreme, simple models can be compelling
exactly because they are simple.

Simple models offer a different kind of explanation than detailed
models.  With a detailed model, the argument goes something
like this: ``We are interested in physical system S, so we
construct a detailed model, M, and show by analysis and simulation
that M exhibits a behavior, B, that is similar (qualitatively
or quantitatively) to an observation of the real system, O.
So why does O happen?  Because S is similar to M, and
B is similar to O, and we can prove that M leads to B.''
\index{argument by analogy}

With simple models we can't claim that S is similar to M, because it
isn't.  Instead, the argument goes like this: ``There is a set of models
that share a common set of features.  Any model that has these
features exhibits behavior B.  If we make an observation, O, that
resembles B, one way to explain it is to show that the system, S, has
the set of features sufficient to produce B.''

For this kind of argument, adding more features doesn't help.  Making
the model more realistic doesn't make the model more reliable; it only
obscures the difference between the essential features that cause O
and the incidental features that are particular to S.

Figure~\ref{fig.model3} shows the logical structure of this kind of
model.  The features $x$ and $y$ are sufficient to produce the
behavior.  Adding more detail, like features $w$ and $z$, might make
the model more realistic, but that realism adds no explanatory power.


\chapter{Game of Life}
\label{life}

One of the first cellular automata to be studied, and probably the
most popular of all time, is a 2-D CA called ``The Game of Life,'' or GoL
for short.  It was developed by John H. Conway and popularized in 1970
in Martin Gardner's column in {\em Scientific American}.
See \url{http://en.wikipedia.org/wiki/Conway_Game_of_Life}.
\index{Game of Life}
\index{Conway, John H.}
\index{Gardner, Martin}

The cells in GoL are arranged in a 2-D {\bf grid},
either infinite in both
directions or wrapped around.  A grid wrapped
in both directions is called a {\bf torus} because it is topographically
equivalent to the surface of a doughnut.
See \url{http://en.wikipedia.org/wiki/Torus}.
\index{torus}
\index{grid}

Each cell has two states---live and dead---and 8 neighbors---north,
south, east, west, and the four diagonals.  This set of neighbors
is sometimes called a Moore neighborhood.
\index{Moore neighborhood}
\index{neighborhood}

The rules of GoL are {\bf totalistic}, which means that the next
state of a cell depends on the number of live neighbors only,
not on their arrangement.  The following table summarizes the
rules:
\index{totalistic}

\begin{tabular}{|l|c|c|}
\hline
Number of     &   Current      & Next \\
neighbors     &   state        & state \\
\hline
2--3          &   live           & live         \\
0--1,4--8     &   live           & dead         \\
3             &   dead           & live         \\
0--2,4--8     &   dead           & dead         \\
\hline
\end{tabular}

This behavior is loosely analogous to real cell growth: cells
that are isolated or overcrowded die; at moderate densities they
flourish.

GoL is popular because:

\begin{itemize}

\item There are simple initial conditions that yield
surprisingly complex behavior.
\index{complex behavior}

\item There are many interesting stable patterns: some
oscillate (with various periods) and some move like the
spaceships in Wolfram's Rule 110 CA.

\item Like Rule 110, GoL is Turing complete.
\index{Turing complete}
\index{universal}

\item Conway posed an intriguing conjecture---that there is
no initial condition that yields unbounded growth in the number
of live cells---and offered \$50 to anyone who could prove
or disprove it.
\index{unbounded}

\item The increasing availability of computers made it possible
to automate the computation and display the results graphically.
That turns out to be more fun than Conway's original implementation
using a checkerboard.

\end{itemize}


\section{Implementing Life}

To implement GoL efficiently, we can take advantage of the
multi-dimensional convolution function in SciPy.  SciPy is a Python
package that provides functions related to scientific computing.
You can read about it at \url{http://www.scipy.org/}; if it is not
already on your system, you might have to install it.
\index{implementing Game of Life}
\index{SciPy}

{\bf Convolution} is an operation common in digital image processing,
where an image is an array of pixels, and many operations involve
computing a function of a pixel and its neighbors.
\index{convolution}

The neighborhood is described by a smaller array, called a {\bf kernel}
that specifies the location and {\bf weight} of the neighbors.  For
example, this array:
\index{kernel}
\index{weight}

\begin{code}
    kernel = numpy.array([[1,1,1],
                          [1,0,1],
                          [1,1,1]])
\end{code}

represents a neighborhood with eight neighbors, all with weight 1.

Convolution computes the weighted sum of the neighbors for each
element of the array.  So this kernel computes the sum of the
neighbors (not including the center element).

For example, if \py{array} represents a GoL grid with 1s for live
cells and 0s for dead cells we can use convolution to compute the
number of neighbors for each cell.

\begin{code}
    import scipy.ndimage
    neighbors = scipy.ndimage.filters.convolve(array, kernel)
\end{code}

Here's an implementation of GoL using \py{convolve}:
\index{Life@\py{Life}}

\begin{code}
import numpy
import scipy.ndimage

class Life(object):

    def __init__(self, n, mode='wrap'):
        self.n = n
        self.mode = mode
        self.array = numpy.random.random_integers(0, 1, (n, n))
        self.weights = numpy.array([[1,1,1],
                                    [1,10,1],
                                    [1,1,1]])

    def step(self):
        con = scipy.ndimage.filters.convolve(self.array,
                                             self.weights,
                                             mode=self.mode)

        boolean = (con==3) | (con==12) | (con==13)
        self.array = numpy.int8(boolean)
\end{code}

The attributes of the \py{Life} object are \py{n}, the number
of rows and columns in the grid, \py{mode}, which controls the
behaviors of the boundary cells, \py{array}, which represents
the grid, and \py{weights} which is the kernel used to count
the neighbors.

The weight of the center cell is 10, so the number of neighbors
is 0-8 for dead cells and 10-18 for live cells.

In \py{step}, \py{boolean} is a boolean array with \py{True}
for live cells;  \py{numpy.int8} converts it to integers.

To display an animated sequence of grids, I use \py{pyplot}.
Animation in \py{pyplot} is a little awkward, but here's a class
that manages it:
\index{animation}
\index{pyplot}
\index{LifeViewer@\py{LifeViewer}}

\begin{code}
import matplotlib
matplotlib.use('TkAgg')
import matplotlib.pyplot as pyplot

class LifeViewer(object):

    def __init__(self, life, cmap=matplotlib.cm.gray_r):
        self.life = life
        self.cmap = cmap

        self.fig = pyplot.figure()
        pyplot.axis([0, life.n, 0, life.n])
        pyplot.xticks([])
        pyplot.yticks([])

        self.pcolor = None
        self.update()
\end{code}

\py{life} is a \py{Life} object.  \py{cmap} is a color map
provided by \py{matplotlib}; you can see the other
color maps at \url{http://www.scipy.org/Cookbook/Matplotlib/Show_colormaps}.
\index{color map}

\py{self.fig} is a reference to the \py{matplotlib} figure, and {\tt
  self.pcolor} is a reference to the {\bf pseudocolor plot} created by
\py{update}:
\index{pseudocolor plot}

\begin{code}
    def update(self):
        if self.pcolor:
            self.pcolor.remove()

        a = self.life.array
        self.pcolor = pyplot.pcolor(a, cmap=self.cmap)
        self.fig.canvas.draw()
\end{code}

If there is already a plot, we have to remove it; then we create
a new one and invoke \py{draw} to update the display.

To run the animation, we need two methods:

\begin{code}
    def animate(self, steps=10):
        self.steps = steps
        self.fig.canvas.manager.window.after(1000, self.animate_callback)
        pyplot.show()

    def animate_callback(self):
        for i in range(self.steps):
            self.life.step()
            self.update()
\end{code}

\py{animate} gets the animation started.  It invokes \py{pyplot.show},
which sets up the GUI and waits for user events, but {\em first} it has
to invoke \py{window.after} to set up a callback, so that
\py{animate_callback} gets invoked after the window is set up.
The first argument is the delay in milliseconds.  The second
argument is a bound method (see Chapter 19 of {\em Think Python}).
\index{Think Python@{\em Think Python}}

\py{animate_callback} invokes \py{step} to update the \py{Life}
object and \py{update} to update the display.

\begin{exercise}

Download my implementation of GoL from \url{thinkcomplex.com/Life.py}.

Start the CA in a random state and run it until it stabilizes.
What stable patterns can you identify?

\end{exercise}


\section{Life patterns}

If you run GoL from a random starting state, a number of stable
patterns are likely to appear.  Blocks, boats, beehives, blinkers and
gliders are among the most common.
\index{Game of Life patterns}
\index{glider}
\index{spaceship}

People have spent embarrassing
amounts of time finding and naming these patterns.  If you search
the web, you will find many collections.

From most initial conditions, GoL quickly reaches a stable
state where the number of live cells is nearly constant
(usually with a small amount of oscillation).

But there are a some simple starting conditions that take a
long time to settle down and yield a surprising
number of live cells.  These patterns are called ``Methuselahs''
because they are so long-lived.
\index{Methuselah}

One of the simplest is the
r-pentomino, which has only five cells in the shape of a ``r,'' hence
the name.  It runs for 1103 steps and yields 6 gliders, 8 blocks, 4
blinkers, 4 beehives, 1 boat, 1 ship, and 1 loaf.
One of the longest-lived small patterns is rabbits, which starts
with 9 live cells and takes 17 331 steps to stabilize.
\index{r-pentomino}

\begin{exercise}

Start with an r-pentomino as an initial condition and confirm
that the results are consistent with the description above.
You might have to adjust the size of the grid and the boundary behavior.

\end{exercise}


\section{Conway's conjecture}

The existence of long-lived patterns brings us back to Conway's
original question: are there initial patterns that never stabilize?
Conway thought not, but he described two kinds of pattern that would
prove him wrong, a ``gun'' and a ``puffer train.''  A gun is a stable
pattern that periodically produces a spaceship---as the stream of
spaceships moves out from the source, the number of live cells grows
indefinitely.  A puffer train is a translating pattern that leaves
live cells in its wake.
\index{glider gun}
\index{puffer train}

It turns out that both of these patterns exist.  A team led
by Bill Gosper discovered the first, a glider gun now called
Gosper's Gun.  Gosper also discovered the first puffer train.
You can find descriptions and animations of these patterns
in several places on the Web.
\index{Gosper, Bill}

There are many patterns of both types, but they are not easy to
design or find.  That is not a coincidence.  Conway chose the
rules of GoL so that his conjecture would not be obviously
true or false.  Of all the possible rules for a 2-D CA, most
yield simple behavior; most initial conditions stabilize quickly
or grow unboundedly.  By avoiding uninteresting CAs, Conway
was also avoiding Wolfram's Class 1 and Class 2 behavior, and
probably Class 3 as well.

If we believe Wolfram's Principle of Computational Equivalence, we
expect GoL to be in Class 4.  And it is.  The Game of Life was proved
Turing complete in 1982 (and again, independently, in 1983).
Since then several people have constructed GoL patterns that implement
a Turing machine or another machine known to be Turing complete.
\index{Class 4 behavior}
\index{Turing complete}
\index{universality}

\begin{exercise}

Many named patterns are available in portable file formats.
Modify \py{Life.py} to parse one of these formats and initialize
the grid.

\end{exercise}


\section{Realism}

Stable patterns in GoL are hard not to notice, especially the ones
that move.  It is natural to think of them as persistent entities, but
remember that a CA is made of cells; there is no such thing as a toad
or a loaf.  Gliders and other spaceships are even less real because
they are not even made up of the same cells over time.  So these
patterns are like constellations of stars.  We perceive them because
we are good at seeing patterns, or because we have active
imaginations, but they are not real.
\index{realism}

Right?

Well, not so fast.  Many entities that we consider ``real'' are also
persistent patterns of entities at a smaller scale.  Hurricanes are
just patterns of air flow, but we give them personal names.  And
people, like gliders, are not made up of the same cells over time.
But even if you replace every cell in your body, we consider you the
same person.
\index{hurricane}

This is not a new observation---about 2500 years ago Heraclitus
pointed out that you can't step in the same river twice---but the
entities that appear in the Game of Life are a useful test case for
thinking about {\bf philosophical realism}.
\index{philosophical realism}

In the context of philosophy, realism is the view that entities
in the world exist independent of human perception and conception.
By ``perception'' I mean the information that we get from
our senses, and by ``conception'' I mean the mental model
we form of the world.  For example, our vision systems perceive
something like a 2-D projection of a scene, and our brains
use that image to construct a 3-D model of the objects in the
scene.
\index{perception}
\index{conception}

{\bf Scientific realism} pertains to scientific theories and the
entities they postulate.
A theory postulates an entity if it is
expressed in terms of the properties and behavior of the entity.  For
example, Mendelian genetics postulates a ``gene'' as a unit that
controls a heritable characteristic.  Eventually we discovered that
genes are encoded in DNA, but for about 50 years, a gene was just a
postulated entity.  See \url{http://en.wikipedia.org/wiki/Gene}.
\index{gene}
\index{postulated entity}

Again, I find it useful to state philosophical positions in a range of
strengths, where SR1 is a weak form of scientific realism and SR4 is a
strong form:

\begin{description}

\item[SR1:] Scientific theories are true or false to the degree that
  they approximate reality, but no theory is exactly true.  Some
  postulated entities may be real, but there is no principled way to
  say which ones.

\item[SR2:] As science advances, our theories become better
  approximations of reality.  At least some postulated entities are
  known to be real.

\item[SR3:] Some theories are exactly true; others are approximately
  true.  Entities postulated by true theories, and some entities
  in approximate theories, are real.

\item[SR4:] A theory is true if it describes reality correctly, and
  false otherwise.  The entities postulated by true theories are real;
  others are not.

\end{description}

SR4 is so strong that it is probably untenable; by such a strict
criterion, almost all current theories are known to be false.
Most realists would accept something in the space
between SR1 and SR3.


\section{Instrumentalism}

But SR1 is so weak that it verges on {\bf instrumentalism}, which is
the view that we can't say whether a theory is true or false because
we can't know whether a theory corresponds to reality.  Theories are
instruments that we use for our purposes; a theory is useful, or not,
to the degree that it is fit for its purpose.
\index{instrumentalism}

To see whether you are comfortable with instrumentalism, consider
the following statements:

\begin{quote}
``Entities in the Game of Life aren't real; they are just patterns of
  cells that people have given cute names.''
\end{quote}

\begin{quote}
``A hurricane is just a pattern of air flow, but it is a useful
  description because it allows us to make predictions and communicate
  about the weather.''
\end{quote}
\index{hurricane}

\begin{quote}
``Freudian entities like the Id and the Superego aren't real, but they
  are useful tools for thinking and communicating about psychology (or
  at least some people think so).''
\end{quote}
\index{Id}
\index{Freud, Sigmund}
\index{Superego}

\begin{quote}
``Electrons are postulated entities in our best theories of
electro-magnetism, but they aren't real.  We could construct
other theories, without postulating electrons, that would be
just as useful.''
\end{quote}
\index{electron}

\begin{quote}
``Many of the things in the world that we identify as objects are
  arbitrary collections like constellations.  For example, a mushroom
  is just the fruiting body of a fungus, most of which grows
  underground as a barely-contiguous network of cells.  We focus
  on mushrooms for practical reasons like visibility and edibility.''
\end{quote}
\index{mushroom}

\begin{quote}
``Some objects have sharp boundaries, but many are fuzzy.  For
  example, which molecules are part of your body: Air in your lungs?
  Food in your stomach?  Nutrients in your blood?  Nutrients in a
  cell?  Water in a cell?  Structural parts of a cell?  Hair?  Dead
  skin?  Dirt?  Bacteria on your skin?  Bacteria in your gut?
  Mitochondria?  How many of those molecules do you include when you
  weigh yourself.  Conceiving the world in terms of discrete objects
  is useful, but the entities we identify are not real.''
\end{quote}

Give yourself one point for each statement you agree with.
If you score 4 or more, you might be an instrumentalist!

If you are more comfortable with some of these statements than
others, ask yourself why.  What are the differences in these
scenarios that influence your reaction?  Can you make
a principled distinction between them?

\begin{exercise}

Read \url{http://en.wikipedia.org/wiki/Instrumentalism}
and construct a sequence
of statements that characterize instrumentalism in a range of
strengths.

\end{exercise}


\section{Turmites}

If you generalize the Turing machine to two dimensions, or
add a read-write head to a 2-D CA, the result is a
cellular automaton called a Turmite.  It is named after a
termite because of the way the read-write head moves, but
spelled wrong as an homage to Alan Turing.
\index{turmite}
\index{Turing, Alan}

The most famous Turmite is Langton's Ant, discovered by Chris Langton
in 1986.  See \url{http://en.wikipedia.org/wiki/Langton_ant}.
\index{Langton's Ant}
\index{Langton, Chris}

The ant is a read-write head with
four states, which you can think of as facing north, south,
east or west.  The cells have two states, black and white.
\index{read-write head}

The rules are simple.  During each time step, the ant checks the color
of the cell is it on.  If it is black, the ant turns to the right,
changes the cell to white, and moves forward one space.  If the cell
is white, the ant turns left, changes the cell to black, and moves
forward.
\index{simple rules}

Given a simple world, a simple set of rules, and only one moving part,
you might expect to see simple behavior---but you should know
better by now.  Starting with all white cells, Langton's ant
moves in a seemingly random pattern for more than 10 000 steps
before it enters a cycle with a period of 104 steps.  After
each cycle, the ant is translated diagonally, so it leaves
a trail called the ``highway.''
\index{complex behavior}
\index{period}

If you start with multiple Turmites, they interact with each
other in seemingly complex ways.  If one Turmite is on the
highway, another can follow it, overtake it, and cause it to
reverse its pattern, moving back up the highway and leaving
only white cells behind.

\begin{exercise}

Write an implementation of Langton's Ant.

You can find a solution in \py{TurmiteWorld.py}, which is
part of Swampy.  See \url{http://thinkpython.com/swampy/}.
\index{Swampy}

\end{exercise}




\chapter{Fractals}
\label{fractals}

To understand fractals, we have to start with dimensions.  The
dimension of a space is the number of coordinates we need to specify a
point in a space.  A number line takes one coordinate, a Euclidean
plane takes 2, a solid takes 3, etc.  See
\url{http://en.wikipedia.org/wiki/Dimension}.
\index{fractals}
\index{dimension}
\index{geometric objects}

For simple geometric objects, dimension is defined in terms of scaling
behavior; that is, how ``size'' depends on length, $l$.  For example,
the area of a square is $l^2$; the exponent, 2, indicates that a
square is 2-dimensional.  Similarly, the volume of a cube is $l^3$ and
a cube is 3-dimensional.  A line has dimension 1 and, if we think of a
point as infinitesimally small, it has dimension 0.
\index{line}
\index{square}
\index{cube}

{\bf Fractal dimension} is a more precise and more general extension
of this definition.  There are several versions; the one I find
easiest to understand and apply is the {\bf box-counting dimension},
which is defined for a set, $S$, of points in a $d$-dimensional
space.  See \url{http://en.wikipedia.org/wiki/Box-counting_dimension}.
\index{fractal dimension}
\index{box-counting dimension}

\newcommand{\veps}{\varepsilon}

To compute the box-counting dimension, we divide the space into a grid
where the size of each cell is $\veps$.  Then we count $N(\veps)$,
the number of cells that contain at least one element of $S$.  As
$\veps$ gets smaller, $N(\veps)$ gets bigger.  For many objects
the relationship has the form:

\[ N(\veps) \sim \left( 1 / \veps \right)^D \]

The box counting dimension, $D_{box}$, is defined to be the exponent, $D$.
Taking the log of both sides and rearranging yields:

\[ D_{box} = \frac{log N(\veps)}{log \left( 1 / \veps \right)} \]

More formally, $D_{box}$ is the limit of this ratio as $\veps$ goes
to zero.
\index{log-log plot}


\section{Fractal CAs}

\begin{figure}
\centerline{\includegraphics[height=0.7in]{figs/fractal-254-4.pdf}}
\caption{Rule 254 after 4 timesteps.\label{fig.rule254.1}}
\end{figure}

\begin{figure}
\centerline{\includegraphics[height=1.4in]{figs/fractal-254-8.pdf}}
\caption{Rule 254 after 8 timesteps.\label{fig.rule254.2}}
\end{figure}

To investigate the behavior of fractal dimension, we'll apply it
to cellular automata.  Box-counting for CAs is simple; we
just count the number of ``on'' cells in each time step and
add them up.
\index{fractal cellular automaton}

As an example, consider Rule 254.  Figure~\ref{fig.rule254.1} shows
what it looks like after $t=4$ time steps.  And
Figure~\ref{fig.rule254.2} shows what it looks like after $t=8$ time
steps.  \index{Rule 254}

As $t$ increases, we can imagine the triangle getting bigger,
but for purposes of box-counting, it makes more sense to imagine
the cells getting smaller.  In that case the size of the cells,
$\veps$, is just $1/t$.

After 1 time step, there is 1 black cell.  After 2 time steps, there
are a total of 4, then 9, then 16, then 25.  As expected, the area
of the triangle goes up quadratically.  More formally,
$N(\veps) = \left( 1 / \veps \right)^2$, so $D_{box} = 2$.  And we conclude
that a triangle is 2-dimensional.

\begin{figure}
\centerline{\includegraphics[height=2.5in]{figs/rule-18-64.pdf}}
\caption{Rule 18 after 64 time steps.\label{fig.rule18.2}}
\end{figure}

\begin{figure}
\centerline{\includegraphics[width=3.5in]{figs/fractal_dim-18-64.pdf}}
\caption{$N(\veps)$ versus $1/\veps$ for Rule 18.\label{fig.fractal}}
\end{figure}

Rule 18 is more interesting.  Figure~\ref{fig.rule18.2} shows what it
looks like after 64 steps.  And Figure~\ref{fig.fractal} shows
$N(\veps)$ versus $1/\veps$ on a log-log scale.  \index{Rule 18}

To estimate $D_{box}$ I fit a line to this curve; its slope is 1.56.
$D_{box}$ is a non-integer, which means that this set of
points is a {\bf fractal}.  As $t$ increases, the slope approaches
$log 3 / log 2$, which is the fractal dimension of Sierpi\'{n}ski's
triangle.  See \url{http://en.wikipedia.org/wiki/Sierpinski_triangle}.
\index{fractal}
\index{Sierpi\'{n}ski's triangle}

\begin{exercise}

Write a function that takes a CA object, plots $N(\veps)$ versus
$1/\veps$, where $\veps = 1/t$, and estimates $D_{box}$.

Can you find other CAs with non-integer fractal dimensions?  Be
careful, you might have to run the CA for a while before
$D_{box}$ converges.

Here are some functions from \py{numpy} you might find useful:
\py{cumsum}, \py{log}, and \py{polyfit}.
\index{NumPy}

You can download my solution from \url{fractal.py}.

\end{exercise}


\begin{exercise}

In 1990 Bak, Chen and Tang proposed a cellular automaton that is
an abstract model of a forest fire.  Each cell is in one of three
states: empty, occupied by a tree, or on fire.
\index{Bak, Per}
\index{forest fire model}

The rules of the CA are:

\begin{enumerate}

\item An empty cell becomes occupied with probability $p$.

\item A cell with a tree burns if any of its neighbors
  is on fire.

\item A cell with a tree spontaneously burns, with
  probability $f$, even if none of its neighbors is on fire.

\item A cell with a burning tree becomes an empty cell in the next
  time step.

\end{enumerate}

Read about this model at
\url{http://en.wikipedia.org/wiki/Forest-fire_model} and write a
program that implements it.  You might want to start with a copy of
\url{thinkcomplex.com/Life.py}.

Starting from a random initial condition, run the CA until it reaches
a steady state where the number of trees no longer increases or
decreases consistently.  You might have to tune $p$ and $f$.

In steady state, is the geometry of the forest fractal?
What is its fractal dimension?

\end{exercise}


\section{Percolation}

Many of the CAs we have seen so far are not physical models; that is,
they are not intended to describe systems in the real world.  But some
CAs are designed explicitly as physical models.  In this section we
consider a simple grid-based model of percolation; in the next chapter
we see examples that model forest fires, avalanches, and earthquakes.
\index{physical model} \index{forest fire} \index{avalanche}
\index{earthquake}

Percolation is a process in which a fluid flows through a semi-porous
material.  Examples include oil in rock formations, water in paper,
and hydrogen gas in micropores.  Percolation models are also used to
study systems that are not literally percolation, including epidemics
and networks of electrical resistors.  See
\url{http://en.wikipedia.org/wiki/Percolation_theory}.
\index{percolation}

Percolation processes often exhibit a phase change; that is, an
abrupt transition from one behavior (low flow) to another
(high flow) with a small change in a continuous parameter (like
the porosity of the material).  This transition is sometimes
called a ``tipping point.''
\index{tipping point}

There are two common models of these systems: bond percolation
and site percolation.  Bond percolation is based on a grid
of sites, where each site is connected to four neighbors by
a bond.  Each bond is either porous or non-porous.  A set of sites
that are connected (directly or indirectly) by porous bonds is
called a cluster.  In the vocabulary of graphs, a site is a vertex,
a bond is an edge, and a cluster is a connected subgraph.
\index{bond percolation}
\index{site percolation}

Site percolation is based on a grid of cells, where each cell
represents a porous segment of the material or a non-porous segment.
If two porous cells are adjacent, they are considered connected; a set
of connected cells is considered a cluster.

The rate of flow in a percolation system is primarily determined by
whether or not the porous cells form a path all the way through the
material, so it is useful to know whether a set of cells (or bonds)
contains a ``spanning cluster.''  There are several
definitions of a spanning cluster; the choice depends on the system
you are trying to model. The simplest choice is a cluster that reaches
the top and bottom row of the grid.
\index{spanning cluster}
\index{grid}

To model the porosity of the material, it is common to define
a parameter, $p$, which the probability that any cell (or bond)
is porous.  For a given value of $p$, you can estimate
$R(p)$, which is the probability that there is a spanning cluster,
by generating a large number of random grids and computing the
fraction that contain a spanning cluster.  This way of estimating
probabilities is called a ``Monte Carlo simulation'' because
it a similar to a game of chance.
\index{porosity}
\index{Monte Carlo simulation}

Percolation models are often used to compute a critical value,
$p_c$, which is the fraction of porous segments where the phase
change occurs; that is, where the probability of a spanning cluster
increases quickly from near 0 to near 1.
\index{critical value}

\begin{exercise}

The paper ``Efficient Monte Carlo Algorithm and High-Precision Results
for Percolation,'' by Newman and Ziff, presents an efficient algorithm
for checking whether there is a path through a grid.  You can download
this paper from \url{http://arxiv.org/abs/cond-mat/0005264}.
Read it
paper, implement their algorithm, and see if you can reproduce their
Figure 2(a).

\begin{enumerate}

\item What is the difference between what Newman and Ziff call the
  ``microcanonical ensemble'' and the ``canonical
  ensemble''?  You might find it useful to read about the use of
    these terms in statistical mechanics at
    \url{http://en.wikipedia.org/wiki/Statistical_mechanics}.  Which one is
  easier to estimate by Monte Carlo simulation?
\index{microcanonical ensemble}
\index{canonical ensemble}

\item What algorithm do they use to merge two clusters efficiently?

\item What is the primary reason their algorithm is faster than the
  simpler alternative?

\end{enumerate}

\end{exercise}




\chapter{Self-organized criticality}
\label{soc}
 
\section{Sand piles}

In 1987 Bak, Tang and Wiesenfeld published a paper in Physical Review
Letters, ``Self-organized criticality: an explanation of $1/f$ noise.''
You can download it from \url{http://prl.aps.org/abstract/PRL/v59/i4/p381_1}.

The title takes some explaining.  A system is ``critical'' if it is
in transition between two phases; for example, water at
its freezing point is a critical system.
\index{Bak, Per}
\index{self-organized criticality}
\index{criticality}

A variety of critical systems demonstrate common behaviors:

\begin{itemize}

\item Long-tailed distributions of some physical quantities: for
  example, in freezing water the distribution of crystal sizes is
  characterized by a power law.
\index{long tail}
\index{power law}

\item Fractal geometries: freezing water tends to form fractal
  patterns---the canonical example is a snowflake.  Fractals
  are characterized by self-similarity; that is, parts of the
  pattern resemble scaled copies of the whole.
\index{fractal geometry}
\index{self-similarity}

\item Variations in time that exhibit pink noise: what we call
  ``noise'' is a time series with many frequency components.  In
  ``white'' noise, all of the components have equal power.  In
  ``pink'' noise, low-frequency components have more power than
  high-frequency components.  Specifically, the power at frequency $f$
  is proportional to $1/f$.  Visible light with this power spectrum
  looks pink, hence the name.
\index{pink noise}
\index{1/f noise@$1/f$ noise}

\end{itemize}

Critical systems are usually unstable.  For example, to keep
water in a partially frozen state requires active control of
the temperature.  If the system is near the critical
temperature, a small deviation tends to move the system
into one phase or the other.
\index{unstable}

Many natural systems exhibit characteristic behaviors of
criticality, but if critical points are unstable, they should
not be common in nature.  This is the puzzle Bak, Tang and
Wiesenfeld address.  Their solution is called self-organized
criticality (SOC), where ``self-organized'' means that from
any initial condition, the system tends to move toward a
critical state, and stay there, without external control.
\index{SOC}

As an example, they propose a model of a sand pile.  The model is not
realistic, but it has become the standard example of self-organized
criticality.
\index{sand pile model}
\index{abstract model}

The model is a 2-D cellular automaton where the state of each cell,
$z(i,j)$, represents the slope of a part of a sand pile.  During each
time step, each cell is checked to see whether it exceeds some
critical value, $K$.  If so, an ``avalanche'' occurs that transfers
sand to neighboring cells; specifically, $z(i,j)$ is decreased by 4,
and each of the 4 neighbors is increased by 1.
\index{2-D cellular automaton}
\index{state}

At the perimeter of the grid, all cells are kept at $z=0$, so
the excess spills over the edge.  To initialize the system,
Bak et al. start with all $z > K$ and evolve the system until
it stabilizes.  Then they observe the effect of small perturbations;
they choose a cell at random, increment its value
by 1, and evolve the system, again, until it stabilizes.
\index{grid}

For each perturbation, they measure $D$, the total number
of cells that are affected by the resulting avalanche.  Most of
the time, $D$ is small, usually 1.  But occasionally
a large avalanche affects a substantial fraction
of the grid.  The distribution of $D$ turns out to be long-tailed,
which supports the claim that the system is in a critical state.
\index{avalanche}

\begin{exercise}

Read the paper and write a program that implements their CA.
You might want to start with a copy of
\url{thinkcomplex.com/Life.py}.

See if you can reproduce their Figure 2(a), which shows the
distribution of cluster sizes.

After the system has been running for a while, compute its
fractal dimension.
\index{fractal dimension}

\end{exercise}


\section{Spectral density}

To understand $1/f$ noise, we have to take a detour to understand
spectral density.  If $h(t)$ is a signal that varies in time, it can
be described by its power spectral density, $P(f)$, which is a
function that maps from a frequency, $f$, to the amount of power the
signal contains at that frequency.
\index{spectral density}
\index{power}
\index{frequency}

This analysis applies to any varying signal, but I use sound as
an example.  The note we call ``middle A'' corresponds to a frequency
of 440 cycles per second, or Hertz (Hz).  If you strike a middle A
tuning fork, it produces a sound that is close to a pure sine wave at
440 Hz.  But if you play the same note on a piano, what you hear is
a complex sound that contains components at many different
frequencies.  The frequency with the most power is 440, which is why
we perceive the sound as a middle A, but there are also components at
880, 1320 and many higher frequencies.  These components are called
``harmonics.''
\index{pitch}
\index{Hertz}
\index{harmonics}

What we identify as the pitch of a sound is usually the dominant
frequency component.  But if a sound contains many different
components with roughly the same power, it has no particular pitch.
To our ears, it sounds like noise.
\index{noise}

Spectral analysis is the process of taking a signal and computing its
spectral density\footnote{The presentation here follows Press et al,
  {\em Numerical Recipes in C}.}.  The first step is to compute the
  Fourier transform of $h(t)$:
\index{Fourier transform}

\[ H(\omega) = \int_{-\infty}^{\infty} h(t) e^{i \omega t} dt \]

where $\omega = 2 \pi f$ is the angular frequency in
radians per second (rather than cycles per second).  The advantage
of working with angular frequency is that it reduces the number
of times the term $2 \pi$ appears.
\index{angular frequency}

$H(\omega)$ is written with a capital letter because it is a complex
number, which you can think of as a vector with a magnitude,
$|H(\omega)|$, and an angle.  The power spectral density is related to
the Fourier transform by the following relation:
\index{power spectral density}

\[ P(f) = |H(2 \pi f)|^2 \]

Depending on the application, we may not care about the difference
between $f$ and $-f$.  In that case, we would use the one-sided
power spectral density:

\[ P(f) = |H(2 \pi f)|^2 + |H(-2 \pi f)|^2 \]

So far we have assumed that $h(t)$ is a continuous function, but
often it is a series of values at discrete times.  In that
case we can replace the continuous Fourier transform with
the discrete Fourier transform (DFT).  Suppose that we have
$N$ values $h_k$ with $k$ in the range from 0 to $N-1$.  The
DFT is written $H_n$, where $n$ is an index related to frequency:
\index{discrete Fourier transform}
\index{DFT}

\begin{equation}
\label{dft}
H_n = \sum_{k=0}^{N-1} h_k e^{2 \pi i k n / N}
\end{equation}

Each element of this sequence corresponds to a particular frequency.
If the elements of $h_k$ are equally spaced in time, with time
step $d$, the frequency that corresponds to $H_n$ is

\[ f_n = \frac{n}{N d} \]

To get the one-sided power spectral density, you can compute $H_n$
with $n$ in the range $-N/2$ to $N/2$, and

\[ P_n = |H_n|^2 + |H_{-n}|^2 \]

To avoid negative indices, it is conventional to compute
$H_n$ with $n$ in the range $0$ to $N-1$ and use the relation
$H_{-n} = H_{N-n}$ to convert.

\begin{exercise}

Write a function named \py{dft} that takes $h$, a sequence of $N$
values, and returns the sequence $H_n$ with $n$ in the range $0$ to
$N-1$.

Python provides support for complex numbers as a built-in type.
The function \py{complex} takes two arguments, a real part
and an imaginary part, and returns a complex number:
\index{complex@\py{complex}}

\begin{code}
>>> complex(1, 1)
(1+1j)
\end{code}

The \py{cmath} module provides math functions that support
complex numbers:

\begin{code}
>>> import cmath
>>> i = complex(0, 1)
>>> N = 128
>>> cmath.exp(2 * math.pi * i / N)
(0.99879545620517241+0.049067674327418015j)
>>> abs(complex(3, 4))
5.0
\end{code}

What is the order of growth run time of \py{dft}?

{\bf Hoisting} is a way to speed up code by moving an
expression that does not change out of a loop.
See \url{http://en.wikipedia.org/wiki/Loop-invariant_code_motion}.
You can make your code easier to read and more efficient
by hoisting:
\index{hoisting}

\begin{equation}
W = e^{2 \pi i / N}
\end{equation}
%
What effect does hoisting have on the order of growth?
\index{order of growth}

\end{exercise}


\section{Fast Fourier Transform}

The Fast Fourier Transform (FFT) is an efficient algorithm for
computing the DFT.  It is often attributed to Cooley and Tukey,
but it was independently discovered several times earlier.
See \url{http://en.wikipedia.org/wiki/Fast_Fourier_transform}.
\index{Fast Fourier Transform}
\index{FFT}

The first step toward the FFT is to rewrite Equation~\ref{dft}
with the substitution $W = e^{2 \pi i/N}$:

\begin{equation}
H_n = \sum_{k=0}^{N-1} h_k W^{n k}
\end{equation}

The second step is the Danielson-Lanczos Lemma which states
\index{Danielson-Lanczos Lemma}

\[ H_n = H^e_n + W^k H^o_n \]

where $H^e$ is the DFT of the even-indexed elements
of $h$, and $H^o$ is the DFT of the odd-indexed elements.
This lemma follows naturally from the definition of $H_n$; you can see
a proof at \url{http://mathworld.wolfram.com/Danielson-LanczosLemma.html}.

This lemma suggests a recursive algorithm for evaluating the DFT
of a sequence $h$.  If  $h$ has only a single element, then $H=h$.
Otherwise:
\index{recursive algorithm}

\begin{enumerate}

\item Split $h$ into $h^e$ and $h^o$.

\item Compute $H^e$ and $H^o$ by making two recursive calls.

\item Use the lemma to combine $H^e$ and $H^o$ to form $H$.

\end{enumerate}

If $H$ has $2N$ elements, $H^e$ and $H^o$ have only $N$.
In order to merge them, you have to wrap around, but you
can do that because $H^e_{n+N} = H^e_{n}$.

This recursive algorithm is the Fast Fourier Transform.

\begin{exercise}

Write a function called \py{fft} that implements
the Fast Fourier Transform.  To check your function, you
can compare it to the function \py{fft} provided by
the module \py{numpy.fft}.

What is the order of growth run time of your implementation?
What is the order of growth for the space required?

Most FFT implementations use a clever indexing scheme to avoid copying
the sequence; instead, they transform the elements in place.  You can
read \url{http://en.wikipedia.org/wiki/Butterfly_diagram} to get the details.
\index{indexing}

Once your \py{fft} is working, write a function named
\py{psd} that takes a sequence, $h$, and returns its
one-sided power spectral density, $P$.

You can download my solution from \url{thinkcomplex.com/Fourier.py}.

\end{exercise}


\section{Pink noise}

In a followup paper in 1988, Bak, Tang and Wiesenfeld looked
at a time series $F(t)$, which is the number of cells that
exceed the threshold during each time step.  If I understand
their model, they seed avalanches by incrementing the state
of a random cell at random intervals; for example, there might
be a fixed probability during each time step that a cell
is incremented.  In this model (unlike the previous one) there
may be more than one avalanche at a time.
\index{pink noise}
\index{1/f noise@$1/f$ noise}

A plot of $F(t)$ shows that it is noisy, but not completely
random, which is consistent with pink, or $1/f$ noise.
As a stronger test, they plot the power spectral density of
$F$ on a log-log scale.  If $F$ is $1/f$ noise, then

\[ P_n \sim 1 / f_n = \frac{N d}{n} \]

Since the units of time in this model are arbitrary, we
can choose $d=1$.  Taking the log of both sides yields:

\[ \log P_n \sim \log N - \log n \]

So on a log-log scale, the PSD of $1/f$ noise is a straight
line with slope -1.
\index{log-log plot}

\begin{exercise}

Modify your implementation of the sand pile model to increment
a random cell at random intervals and record the number of cells
that exceed the threshold during each time step.

To estimate the average PSD, you can divide the time series into
chunks of 128 to 256 values, compute the PSD of each chunk, and
average together the PSDs.  Plot the result on a log-log scale
and estimate the slope.

\end{exercise}


\begin{exercise}

In a 1989 paper, ``Self-organized criticality in the 'Game of Life',''
Bak, Chen and Creutz present evidence that the Game of Life is a
self-organized critical system
(\url{http://www.nature.com/nature/journal/v342/n6251/abs/342780a0.html}).
\index{Game of Life} \index{self-organized criticality}

To replicate their tests, run the GoL CA until it stabilizes,
then choose a random cell and toggle it.  Run the CA until
it stabilizes again, keeping track of $t$, the number
of time steps it takes, and $s$, the number of cells affected.
Repeat for a large number of trials and plot the distributions
of $t$ and $s$.  Also, see if you can think of an effective
experiment to test for $1/f$ noise.

Some later work has called the conclusions of this paper into
question.  You might want to read Blok, ``Life without bounds,''
at \url{http://zoology.ubc.ca/~rikblok/lib/blok95b.html}.

\end{exercise}


\section{Reductionism and Holism}
\label{model2}
 
The original paper by Bak, Tang and Wiesenfeld is one of
the most frequently-cited papers in the last few decades.
Many new systems have been shown to be self-organized critical,
and the sand-pile model, in particular, has been studied
in detail.
\index{sand pile model}

As it turns out, the sand-pile model is not a very good model
of a sand pile.  Sand is dense and not very sticky, so momentum
has a non-negligible effect on the behavior of avalanches.  As
a result, there are fewer very large and very small avalanches
than the model predicts, and the distribution is not long tailed.

Bak has suggested that this observation misses the point.
The sand pile model is not meant to be a realistic model of a sand
pile; it is meant to be a simple example of a broad category of
models.

To understand this point, it is useful to think about two
kinds of models, {\bf reductionist} and {\bf holistic}.  A
reductionist model describes a system by describing its parts
and their interactions.  When a reductionist model is used
as an explanation, it depends on an analogy between the
components of the model and the components of the system.
\index{reductionist}
\index{holist}

For example, to explain why the ideal gas law holds, we can model the
molecules that make up a gas with point masses, and model their
interactions as elastic collisions.  If you simulate or analyze this
model you find that it obeys the ideal gas law.  This model is
satisfactory to the degree that molecules in a gas behave like
molecules in the model.  The analogy is between the parts of the
system and the parts of the model.
\index{analogy}

\begin{figure}
\centerline{\includegraphics[width=5in]{figs/model2.pdf}}
\caption{The logical structure of a holistic model.\label{fig.model2}}
\end{figure}

Holistic models are more focused on similarities between systems and
less interested in analogous parts.  A holistic approach to modeling
often consists of two steps, not necessarily in this order:
\index{holistic model}

\begin{itemize}

\item Identify a kind of behavior that appears in a variety of
systems.

\item Find the simplest model that demonstrates that behavior.

\end{itemize}

For example, in {\em The Selfish Gene}, Richard Dawkins suggests that
genetic evolution is just one example of an evolutionary system.  He
identifies the essential elements of the category---discrete
replicators, variability and differential reproduction---and proposes
that any system that has these elements displays similar
behavior, including complexity without design.
\index{Selfish Gene@{\em The Selfish Gene}}
\index{Dawkins, Richard}
\index{evolution}

As another example of an evolutionary system, he proposes memes, which
are thoughts or behaviors that are ``replicated'' by transmission from
person to person.  As memes compete for the resource of human
attention, they evolve in ways that are similar to genetic evolution.
\index{meme}
\index{replicator}

Critics of memetics have pointed out that memes are a poor analogy
for genes.  Memes differ from genes in many obvious ways.  But
Dawkins has argued that these differences are beside the point
because memes are not {\em supposed} to be analogous to genes.
Rather, memetics and genetics are examples of the same
category---evolutionary systems.  The differences between them
emphasize the real point, which is that evolution is a general model
that applies to many seemingly disparate systems.  The logical
structure of this argument is shown in Figure~\ref{fig.model2}.
\index{gene}
\index{genetics}

Bak has made a similar argument that self-organized criticality is a
general model for a broad category of systems.  According to
Wikipedia, ``SOC is typically observed in slowly-driven
non-equilibrium systems with extended degrees of freedom and a high
level of nonlinearity.''
\index{Bak, Per}

Many natural systems demonstrate behaviors characteristic of critical
systems.  Bak's explanation for this prevalence is that these systems
are examples of the broad category of self-organized criticality.
There are two ways to support this argument.  One is to build
a realistic model of a particular system and show that the model
exhibits SOC.  The second is to show that SOC is a feature of many
diverse models, and to identify the essential characteristics
those models have in common.

The first approach, which I characterize as reductionist,
can explain the behavior of a particular system.  The
second, holistic, approach, explains the prevalence of
criticality in natural systems.  They are different models
with different purposes.
\index{prevalence}

For reductionist models, realism is the primary virtue, and
simplicity is secondary.  For holistic models, it is the other
way around.

\begin{exercise}

Read \url{http://en.wikipedia.org/wiki/Reductionism}
and \url{http://en.wikipedia.org/wiki/Holism}.

\end{exercise}


\begin{exercise}

In a 1996 paper in Nature, Frette et al report the results of
experiments with rice piles
(\url{http://www.nature.com/nature/journal/v379/n6560/abs/379049a0.html}).
They find that some kinds of rice yield evidence of critical behavior,
but others do not.  \index{rice pile model}

Similarly, Pruessner and Jensen studied large-scale versions of the
forest fire model (using an algorithm similar to Newman and Ziff's).
In their 2004 paper, ``Efficient algorithm for the forest fire model,''
they present evidence that the system is not critical after all
(\url{http://pre.aps.org/abstract/PRE/v70/i6/e066707}).
\index{forest fire model}

How do these results bear on Bak's claim that SOC explains
the prevalence of critical phenomena in nature?

\end{exercise}


\begin{exercise}

In {\em The Fractal Geometry of Nature}, Benoit Mandelbrot proposes
what he calls a ``heretical'' explanation for the prevalence of
long-tailed distributions in natural systems (page 344).  It may not
be, as Bak suggests, that many systems can generate this behavior in
isolation.  Instead there may be only a few, but there may be
interactions between systems that cause the behavior to propagate.
\index{Fractal Geometry of Nature@{\em The Fractal Geometry of
    Nature}} \index{Mandelbrot, Benoit} \index{long-tailed
  distribution}

To support this argument, Mandelbrot points out:

\begin{itemize}

\item The distribution of observed data is often ``the joint
  effect of a fixed underlying 'true distribution' and a highly
  variable 'filter.'"

\item Long-tailed distributions are robust to filtering; that is,
  ``a wide variety of filters leave their asymptotic behavior
  unchanged.''

\end{itemize}

What do you think of this argument?  Would you characterize
it as reductionist or holist?

\end{exercise}


\section{SOC, causation and prediction}

If a stock market index drops by a fraction of a percent in a
day, there is no need for an explanation.  But if it drops 10\%,
people want to know why.  Pundits
on television are willing to offer explanations, but the real
answer may be that there is no explanation.
\index{stock market}

Day-to-day variability in the stock market shows evidence of
criticality: the distribution of value changes is long-tailed
and the time series exhibits $1/f$ noise.
If the stock market is a self-organized critical system, we
should expect occasional large changes as part of the ordinary
behavior of the market.
\index{1/f noise@$1/f$ noise}

The distribution of earthquake sizes is also long-tailed,
and there are simple models of the dynamics of geological faults
that might explain this behavior.  If these models are right,
they imply that large earthquakes are unexceptional; that is,
they do not require explanation any more than
small earthquakes do.
\index{earthquake}
\index{prediction}
\index{causation}

Similarly, Charles Perrow has suggested that failures in large
engineered systems, like nuclear power plants, are like avalanches
in the sand pile model.  Most failures are small, isolated and
harmless, but occasionally a coincidence of bad fortune yields a
catastrophe.  When big accidents occur, investigators go looking for
the cause, but if Perrow's ``normal accident theory'' is correct,
there may be no special cause of large failures.
\index{normal accident theory}
\index{Perrow, Charles}

These conclusions are not comforting.  Among other things, they
imply that large earthquakes and some kinds of accidents are
fundamentally unpredictable.  It is impossible to look at the
state of a critical system and say whether a large avalanche
is ``due.''  If the system is in a critical state, then a large
avalanche is always possible.  It just depends on the
next grain of sand.

In a sand-pile model, what is the cause of a large avalanche?
Philosophers sometimes distinguish the {\bf proximate} cause, which is
most immediately responsible, from the {\bf ultimate} cause, which is,
for whatever reason, considered the true cause.
\index{proximate cause}
\index{ultimate cause}

In the sand-pile model, the proximate cause of an avalanche is
a grain of sand, but the grain that causes a large avalanche
is identical to any other grain, so it offers no special explanation.
The ultimate cause of a large avalanche is the structure and
dynamics of the systems as a whole: large avalanches occur because
they are a property of the system.

Many social phenomena, including wars, revolutions, epidemics,
inventions and terrorist attacks, are characterized by long-tailed
distributions.  If the reason for these distributions is that
social systems are critical, that suggests that major historical
events may be fundamentally unpredictable and unexplainable.
\index{long-tailed distributions}

\begin{exercise}

Read about the ``Great Man'' theory of history at
\url{http://en.wikipedia.org/wiki/Great_man_theory}.  What implication
does self-organized criticality have for this theory?
\index{Great Man theory}

\end{exercise}


\chapter{Agent-based models}
\label{agent-based}

\section{Thomas Schelling}

In 1971 Thomas Schelling published ``Dynamic Models of Segregation,''
which proposes a simple model of racial segregation.  The Schelling
model of the world is a grid; each cell represents a
house.  The houses are occupied by two kinds of ``agents,''
labeled red and blue, in roughly equal numbers.  About 10\% of the
houses are empty.
\index{Schelling, Thomas}

At any point in time, an agent might be happy or unhappy, depending
on the other agents in the neighborhood.
The neighborhood of each house is the set of
eight adjacent cells.
In one version of the model, agents are happy if they have at least
two neighbors like themselves, and unhappy if they have one or zero.
\index{agent-based model}

The simulation proceeds by choosing an agent at random and checking
to see whether it is happy.  If so, then nothing happens; if not,
the agent chooses one of the unoccupied cells at
random and moves.

You might not be surprised to hear that this model leads to some
segregation, but you might be surprised by the degree.  Fairly
quickly, clusters of similar agents appear.  The clusters
grow and coalesce over time until there are a small number
of large clusters and most agents live in homogeneous
neighborhoods.
\index{segregation}

If you did not know the process and only saw the result, you might
assume that the agents were racist, but in fact all of them
would be perfectly happy in a mixed neighborhood.  Since they prefer
not to be greatly outnumbered, they might be considered xenophobic at
worst.  Of course, these agents are a wild simplification of real
people, so it may not be appropriate to apply these descriptions at
all.
\index{racism}
\index{xenophobia}

Racism is a complex human problem; it is hard to imagine that such a
simple model could shed light on it.  But in fact it provides a strong
argument about the relationship between a system and its parts: if you
observe segregation in a real city, you cannot conclude that
individual racism is the immediate cause, or even that the people in
the city are racists.
\index{causation}

But we have to keep in mind the limitations of this argument:
Schelling's model demonstrates a possible cause of segregation, but
says nothing about actual causes.

\begin{exercise}

Implement Schelling's model in a grid.  From random initial conditions,
how does the system evolve?

Define a statistic that measures the degree of segregation, and plot
this statistic over time.

Experiment with the parameters of the model.  What happens as the agents
become more tolerant?  What happens if the agents are only happy in
mixed neighborhoods; that is, if they are unhappy if too many of their
neighbors are like themselves?
\index{parameter}

\end{exercise}


\begin{exercise}

In a recent book, {\em The Big Sort}, Bill Bishop argues that
American society is increasingly segregated by political
opinion, as people choose to live among like-minded neighbors.
\index{Big Sort@{\em The Big Sort}}
\index{Bishop, Bill}

The mechanism Bishop hypothesizes is not that people, like the agents
in Schelling's model, are more likely to move if they are
isolated, but that when they move for any reason, they are
likely to choose a neighborhood with people like themselves.

Modify your implementation of Schelling's model to simulate
this kind of behavior and see if it yields similar degrees of
segregation.

\end{exercise}


\section{Agent-based models}

Schelling's model is one of the first, and one of the most
famous, agent-based models.  Since the 1970s, agent-based
modeling has become an important tool in economics and other
social sciences, and in some natural sciences.
\index{agent-based model}

The characteristics of agent-based models include:

\begin{itemize}

\item Agents that model intelligent behavior, usually with a simple
  set of rules.

\item The agents are usually situated in space (or in a network), and
  interact with each other locally.

\item They usually have imperfect, local information.

\item Often there is variability between agents.

\item Often there are random elements, either among the agents or in
  the world.

\end{itemize}

Agent-based models are useful for modeling the dynamics of systems
that are not in equilibrium (although they are also used to study
equilibrium).  And they are particularly useful for understanding
relationships between individual decisions and system behavior, or as
in the title of Schelling's book, {\em Micromotives and Macrobehavior}.
\index{Micromotives and Macrobehavior@{\em Micromotives and Macrobehavior}}

For more about agent-based modeling, see
\url{http://en.wikipedia.org/wiki/Agent-based_model}.


\section{Traffic jams}

What causes traffic jams?  In some cases there is an obvious cause,
like an accident, a speed trap, or something else that disturbs
the flow of traffic.  But other times traffic jams appear for no
apparent reason.
\index{traffic jam}

Agent-based models can help explain spontaneous traffic jams.
As an example, I implemented a simple highway simulation, based on
a model in Resnick, {\em Turtles, Termites and Traffic Jams}.
\index{Turtles, Termites and Traffic Jams@{\em Turtles, Termites and Traffic Jams}}
\index{Resnick, Mitchell}

You can download my program from \url{thinkcomplex.com/Highway.py}.
It uses \py{TurtleWorld}, which is part of Swampy.
See \url{thinkpython.com/swampy}.
\index{Swampy}

This module defines two classes: \py{Highway}, which inherits from
TurtleWorld, and \py{Driver}, which inherits from Turtle.

The Highway is a one-lane road that forms a circle, but it is displayed
as a series of rows that spiral down the canvas.

Each driver starts with a random position and speed.  At each time
step, each Driver accelerates or brakes based on the distance between
it and the Driver in front.  Here is an example:

\begin{code}
    def choose_acceleration(self, dist):
        if dist < self.safe_distance:
            return -1
        else:
            return 0.3
\end{code}

If the following distance is too short, the \py{Driver} brakes;
otherwise it accelerates.  \py{Highway.py} enforces two other
constraints: there is a speed limit for each driver, and if the
current speed would cause a collision, the \py{Driver} comes to a
complete stop.

If you run \py{Highway.py} you will probably see a traffic jam,
and the natural question is, ``Why?''  There is nothing about the
\py{Highway} or \py{Driver} behavior that obviously causes
traffic jams.

\begin{exercise}

Experiment with the parameters of the system to identify the factors
that are necessary and sufficient to cause traffic jams.  Some of
the factors to explore are:
\index{parameter}

\begin{description}

\item[Density:] What happens as the number of drivers increases (or
the length of the highway decreases)?

\item[Acceleration and braking:]  What happens if drivers accelerate
faster or brake more gently?

\item[Safe distance:] What happens as the safe distance between drivers
changes?

\item[Heterogeneity:] What if all drivers are not the same; for
example, if they have different speed limits or following distances?

\end{description}

\end{exercise}


\section{Boids}

In 1987 Craig Reynolds published ``Flocks, herds and
schools: A distributed behavioral model,'' which describes an
agent-based model of herd behavior.  You can download his
paper from \url{http://www.red3d.com/cwr/papers/1987/boids.html}.
\index{Reynolds, Craig}

Agents in this models are called ``boids,'' which is both a
contraction of ``bird-oid'' and an accented pronunciation of ``bird''
(although boids are also used to model fish and herding land animals).
\index{boid}

Each agent simulates three behaviors:

\begin{description}

\item[Collision avoidance:] avoid obstacles, including other birds.

\item[Flock centering:] move toward the center of the flock.

\item[Velocity matching:] align velocity with neighboring birds.

\end{description}

Boids make decisions based on local information only; each boid
only sees (or pays attention to) other boids in its field of
vision and range.
\index{local information}

The \py{Visual} package, also known as VPython, is well-suited
for implementing boids.  It provides simple 3-D graphics as
well as vector objects and operations that are useful for the
computations.
\index{Visual package}
\index{VPython}

You can download my implementation from
\url{thinkcomplex.com/Boids.py}.  It is based in part
on the description of boids in Flake, {\em The Computational
Beauty of Nature}.

The program defines two classes: \py{Boid}, which implements the boid
algorithm, and \py{World}, which contains a list of Boids and a
``carrot'' the Boids are attracted to.
\index{carrot}

The boid algorithm uses \py{get_neighbors} to find other
boids in the field of view:

\begin{code}
    def get_neighbors(self, others, radius, angle):
        boids = []
        for other in others:
            if other is self:
               continue

            offset = other.pos - self.pos

            # if not in range, skip it
            if offset.mag > radius:
                continue

            # if not within viewing angle, skip it
            if self.vel.diff_angle(offset) > angle:
                continue

            # otherwise add it to the list
            boids.append(other)

        return boids
\end{code}

\py{get_neighbors} uses vector subtraction to compute the
vector from \py{self} to \py{other}.  The magnitude of
this vector is the distance to the other boid.  \py{diff_angle}
computes the angle between the velocity of \py{self}, which
is also the line of sight, and the other boid.

\py{center} finds the center of mass of the boids in the
field of view and returns a vector pointing toward it:

\begin{code}
    def center(self, others):
        close = self.get_neighbors(others, r_center, a_center)
        t = [other.pos for other in close]
        if t:
            center = sum(t)/len(t)
            toward = vector(center - self.pos)
            return limit_vector(toward)
        else:
            return null_vector
\end{code}

Similarly, \py{avoid} finds the center of mass of any obstacles
in range and returns a vector pointing away from it,
\py{copy} returns the difference between the current heading
and the average heading of the neighbors, and \py{love}
computes the heading toward the carrot.

\py{set_goal} computes the weighed sum of these goals and
sets the overall goal:

\begin{code}
    def set_goal(self, boids, carrot):
        self.goal = (w_avoid * self.avoid(boids, carrot) +
                     w_center * self.center(boids) +
                     w_copy * self.copy(boids) +
                     w_love * self.love(carrot))
\end{code}

Finally, \py{move} updates the velocity, position and
attitude of the boid:

\begin{code}
    def move(self, mu=0.1):
        self.vel = (1-mu) * self.vel + mu * self.goal
        self.vel.mag = 1

        self.pos += dt * self.vel
        self.axis = b_length * self.vel.norm()
\end{code}

The new velocity is the weighted sum of the old velocity
and the goal.  The parameter \py{mu} determines how quickly
the birds can change speed and direction.  The time step, \py{dt}
determines how far the boids move.
\index{weighted sum}

Many parameters influence flock behavior, including the range, angle
and weight for each behavior, and the maneuverability, \py{mu}.
\index{flock behavior}

These parameters determine the ability of the boids to form and
maintain a flock and the patterns of motion and organization in the
flock.  For some settings, the boids resemble a flock of birds; other
settings resemble a school of fish or a cloud flying insects.

\begin{exercise}

% sudo apt-get install python-visual libgtkglextmm-x11-1.2-dev

Run my implementation of the boid algorithm and experiment with
different parameters.  What happens if you ``turn off'' one
of the behaviors by setting the weight to 0?
\index{parameter}

To generate more bird-like behavior, Flake suggests adding a fourth
behavior to maintain a clear line of sight; in other words, if there
is another bird directly ahead, the boid should move away
laterally.  What effect do you expect this rule to have on the
behavior of the flock?  Implement it and see.

\end{exercise}


\section{Prisoner's Dilemma}
\label{prisoners}

The Prisoner's Dilemma is a topic of study in game theory, but
it's not the fun kind of game.  Instead, it is the kind of game
that sheds light on human motivation and behavior.
\index{Prisoner's Dilemma}

Here is the presentation of the dilemma from
\url{http://en.wikipedia.org/wiki/Prisoner's_dilemma}.

\begin{quote}
Two suspects [Alice and Bob] are arrested by the police.  The police
have insufficient evidence for a conviction, and, having separated the
prisoners, visit each of them to offer the same deal.  If one
testifies against the other (defects) and the
other remains silent (cooperates), the defector goes free and the
silent accomplice receives the full one-year sentence.  If both remain
silent, both prisoners are sentenced to only one month in jail for a
minor charge. If each betrays the other, each receives a three-month
sentence. Each prisoner must choose to betray the other or to remain
silent. Each one is assured that the other would not know about the
betrayal before the end of the investigation.  How should the
prisoners act?
\end{quote}

Notice that in this context, ``cooperate'' means to keep silent,
not to cooperate with police.

It is tempting to say that the players should cooperate with each
other, since they would both be better off.  But neither player knows
what the other will do.  Looking at it from Bob's point of view:

\begin{itemize}

\item If Alice remains silent, Bob is better off defecting.

\item If Alice defects, Bob is better off defecting.

\end{itemize}

Either way, Bob is better off defecting.  And from her point of view,
Alice reaches the same conclusion.  So if both players do the math,
and no other factors come into play, we expect them to defect and be
worse off for it.
\index{game theory}

This result is saddening because it is an example of how good intentions
can lead to bad outcomes, and, unfortunately, it applies to other
scenarios in real life, not just hypothetical prisoners.

But in real scenarios, the game is often iterated; that is, the same
players face each other over and over, so they have the opportunity
to learn, react, and communicate, at least implicitly.
\index{iterated Prisoner's dilemma}

The iterated version of the game is not as easy to analyze; it
is not obvious what the optimal strategy is or even whether one
exists.
\index{strategy}

So in the late 1970s Robert Axelrod organized a tournament to compare
strategies.  He invited participants to submit strategies in the
form of computer programs, then played the programs against each other
and kept score.
\index{Axelrod, Robert}

I won't tell you the outcome, and if you don't know you should resist
the temptation to look it up.  Instead, I encourage you to run your
own tournament.  I'll provide the referee; you provide the players.

\begin{exercise}

Download \url{thinkcomplex.com/Referee.py}, which runs the tournament,
and \url{thinkcomplex.com/PlayerFlipper.py}, which implements a simple
player strategy.

Here is the code from \py{PlayerFlipper.py}:

\begin{code}
def move(history):
    mine, theirs = history
    if len(mine) % 2 == 0:
        return 'C'
    else:
        return 'D'
\end{code}

Any file that matches the pattern \py{Player*.py} is recognized
as a player.  The file should contain a definition for \py{move},
which takes the history of the match so far and returns a string:
\py{'D'} for defect and \py{'C'} for cooperate.

\py{history} is a pair of lists: the first list contains the
player's previous responses in order; the second contains the
opponent's responses.

\py{PlayerFlipper} checks whether the number of previous rounds
is even or odd and returns \py{'C'} or \py{'D'} respectively.

Write a \py{move} function in a file like \py{PlayerFlipper.py}, but
replace ``Flipper'' with a name that summarizes your strategy.

Run \py{Referee.py} and see how your strategy does.

After you run your own tournament, you can read about the results
of Axelrod's tournament in his book, {\it The Evolution of Cooperation}.
\end{exercise}


\section{Emergence}

The examples in this chapter have something in common: emergence.  An
{\bf emergent property} is a characteristic of a system that results
from the interaction of its components, not from their properties.
\index{emergence}
\index{emergent property}

To clarify what emergence is, it helps to consider what it isn't.  For
example, a brick wall is hard because bricks and mortar are hard, so
that's not an emergent property.  As another example, some rigid
structures are built from flexible components, so that seems like a
kind of emergence.  But it is at best a weak kind, because structural
properties follow from well-understood laws of mechanics.
\index{brick wall}

Emergent properties are surprising: it is hard to predict the behavior
of the system even if we know all the rules.  That difficulty is not
an accident; it may be the defining characteristic of emergence.

As Wolfram discusses in {\em A New Kind of Science}, conventional science
is based on the axiom that if you know the rules that govern a system,
you can predict its behavior.  What we call ``laws'' are often
computational shortcuts that allow us to predict the outcome of a
system without building or observing it.
\index{New Kind of Science@{\it A New Kind of Science}}
\index{Wolfram, Stephen}
\index{natural law}

But many cellular automata are {\bf computationally irreducible},
which means that there are no shortcuts.  The only way to get the
outcome is to implement the system.
\index{computationally irreducible}
\index{shortcut}

The same may be true of complex systems in general.  For physical
systems with more than a few components, there is usually no model
that yields an analytic solution.  Numerical methods provide a kind of
computational shortcut, but there is still a qualitative difference.
Analytic solutions often provide a constant-time algorithm for
prediction; that is, the run time of the computation does not depend
on $t$, the time scale of prediction.  But numerical methods,
simulation, analog computation, and similar methods take time
proportional to $t$.  And for many systems, there is a bound on $t$
beyond which we can't compute reliable predictions at all.

These observations suggest that emergent properties are fundamentally
unpredictable, and that for complex systems we should not expect to
find natural laws in the form of computational shortcuts.

To some people, ``emergence'' is another name for ignorance; by this
reckoning, a property is emergent if we don't have a reductionist
explanation for it, but if we come to understand it better in the
future, it would no longer be emergent.

The status of emergent properties is a topic of debate, so it is
appropriate to be skeptical.
When we see an apparently emergent property,
we should not assume that there can never be a reductionist explanation.
But neither should we assume that there has to be one.  The examples
in this book and the principle of computational
equivalence give good reasons to believe that at least some
emergent properties can never be ``explained'' by a
classical reductionist model.

You can read more about emergence at
\url{http://en.wikipedia.org/wiki/Emergence}.


\section{Free will}
\label{free.will}

Many complex systems have properties, as a whole, that their
components do not:

\begin{itemize}

\item The Rule 30 cellular automaton is deterministic, and the rules
  that govern its evolution are completely known.  Nevertheless, it
  generates a sequence that is statistically indistiguishable from
  random.

\item The agents in Schelling's model are not racist, but the outcome
  of their interactions is as if they were.

\item Traffic jams move backward even though the cars in them are
  moving forward.

\item The behavior of flocks and herds emerges from local interactions
  between their members.

\item As Axelrod says about the iterated prisoner's dilemma: ``The
  emergence of cooperation can be explained as a consequence of
  individual[s] pursuing their own interests.''

\end{itemize}

These examples suggest an approach to several old and challenging
questions, including the problems of consciousness and free will.

Free will is the ability to make choices, but if our bodies and brains
are governed by deterministic physical laws, our actions would be
determined.  Arguments about free will are innumerable; I will
only mention two:

\begin{itemize}

\item William James proposed a two-stage model in which
  possible actions are generated by a random process and then selected
  by a deterministic process.  In that case our actions are
  fundamentally unpredictable because the process that generates them
  includes a random element.

\item David Hume suggested that our perception of making choices
  is an illusion; in that case, our actions are deterministic because
  the system that produces them is deterministic.

\end{itemize}

These arguments reconcile the conflict in opposite ways, but they
agree that there is a conflict: the system cannot have free will if
the parts are deterministic.

The complex systems in this book suggest the alternative
that free will, at the level of options and decisions, is compatible
with determinism at the level of neurons (or some lower level).  In
the same way that a traffic jam moves backward while the cars move
forward, a person can have free will even though neurons don't.


\begin{exercise}

Read more about free will at
\url{http://en.wikipedia.org/wiki/Free_will}.  The view that free will
is compatible with determinism is called {\bf compatibilism}.  One of
the strongest challenges to compatibilism is the ``consequence
argument.''  What is the consequence argument?  What response can you
give to the consequence argument based on what you have read in this
book?
\end{exercise}


\begin{exercise}

In the philosophy of mind, {\em Strong AI} is the position that an
appropriately-programmed computer could have a mind in the same sense
that humans have minds.

John Searle presented a thought experiment called ``The Chinese Room,''
intended to show that {\em Strong AI} is false.  You can read about
it at \url{http://en.wikipedia.org/wiki/Chinese_room}.

What is the {\bf system reply} to the Chinese Room argument?
How does what you have learned about complexity science influence
your reaction to the system response?

\end{exercise}

You have reached the end of the book.  Congratulations!  When you
first read Chapter~\ref{overview}, some of the topics might not have
made sense.  You might find it helpful to read that chapter again now.
Then get to work on your case study!  See Appendix~\ref{submissions}.


\appendix

\chapter{Analysis of algorithms}

Analysis of algorithms is the branch of computer science that studies
the performance of algorithms, especially their run time and space
requirements.  See \url{http://en.wikipedia.org/wiki/Analysis_of_algorithms}.
\index{algorithm}
\index{analysis of algorithms}

%\url{http://en.wikipedia.org/wiki/Run-time_analysis}

The practical goal of algorithm analysis is to predict the performance
of different algorithms in order to guide design decisions.

During the 2008 United States Presidential Campaign, candidate
Barack Obama was asked to perform an impromptu analysis when
he visited Google.  Chief executive Eric Schmidt jokingly asked him
for ``the most efficient way to sort a million 32-bit integers.''
Obama had apparently been tipped off, because he quickly
replied, ``I think the bubble sort would be the wrong way to go.''
See \url{http://www.youtube.com/watch?v=k4RRi_ntQc8}.
\index{Obama, Barack}
\index{Schmidt, Eric}
\index{bubble sort}

This is true: bubble sort is conceptually simple but slow for
large datasets.  The answer Schmidt was probably looking for is
``radix sort'' (see \url{http://en.wikipedia.org/wiki/Radix_sort})\footnote{
But if you get a question like this in an interview, I think
a better answer is, ``The fastest way to sort a million integers
is to use whatever sort function is provided by the language
I'm using.  Its performance is good enough for the vast majority
of applications, but if it turned out that my application was too
slow, I would use a profiler to see where the time was being
spent.  If it looked like a faster sort algorithm would have
a significant effect on performance, then I would look
around for a good implementation of radix sort.''}.
\index{radix sort}

So the goal of algorithm analysis is to make meaningful
comparisons between algorithms, but there are some problems:
\index{comparing algorithms}

\begin{itemize}

\item The relative performance of the algorithms might
depend on characteristics of the hardware, so one algorithm
might be faster on Machine A, another on Machine B.
The general solution to this problem is to specify a
{\bf machine model} and analyze the number of steps, or
operations, an algorithm requires under a given model.
\index{machine model}

\item Relative performance might depend on the details of
the dataset.  For example, some sorting
algorithms run faster if the data are already partially sorted;
other algorithms run slower in this case.
A common way to avoid this problem is to analyze the
{\bf worst case} scenario.  It is also sometimes useful to
analyze average case performance, but it is usually harder,
and sometimes it is not clear what set of cases to average over.
\index{worst case}
\index{average case}

\item Relative performance also depends on the size of the
problem.  A sorting algorithm that is fast for small lists
might be slow for long lists.
The usual solution to this problem is to express run time
(or number of operations) as a function of problem size,
and to compare the functions {\bf asymptotically} as the problem
size increases.
\index{asymptotic analysis}

\end{itemize}

The good thing about this kind of comparison that it lends
itself to simple classification of algorithms.  For example,
if I know that the run time of Algorithm A tends to be
proportional to the size of the input, $n$, and Algorithm B
tends to be proportional to $n^2$, then I
expect A to be faster than B for large values of $n$.

This kind of analysis comes with some caveats, but we'll get
to that later.


\section{Order of growth}

Suppose you have analyzed two algorithms and expressed
their run times in terms of the size of the input:
Algorithm A takes $100 n + 1$ steps to solve a problem with
size $n$; Algorithm B takes $n^2 + n + 1$ steps.
\index{order of growth}

The following table shows the run time of these algorithms
for different problem sizes:

\begin{tabular}{|r|r|r|}
\hline
Input     &   Run time of     & Run time of \\
size      &   Algorithm A     & Algorithm B \\
\hline
10        &   1 001           & 111         \\
100       &   10 001          & 10 101         \\
1 000     &   100 001         & 1 001 001         \\
10 000    &   1 000 001       & $> 10^{10}$         \\
\hline
\end{tabular}

At $n=10$, Algorithm A looks pretty bad; it takes almost 10 times
longer than Algorithm B.  But for $n=100$ they are about the same, and
for larger values A is much better.

The fundamental reason is that for large values of $n$, any function
that contains an $n^2$ term will grow faster than a function whose
leading term is $n$.  The {\bf leading term} is the term with the
highest exponent.
\index{leading term}
\index{exponent}

For Algorithm A, the leading term has a large coefficient, 100, which
is why B does better than A for small $n$.  But regardless of the
coefficients, there will always be some value of $n$ where $a n^2 > b
n$.
\index{leading coefficient}

The same argument applies to the non-leading terms.  Even if the run
time of Algorithm A were $n + 1000000$, it would still be better than
Algorithm B for sufficiently large $n$.

In general, we expect an algorithm with a smaller leading term to be a
better algorithm for large problems, but for smaller problems, there
may be a {\bf crossover point} where another algorithm is better.  The
location of the crossover point depends on the details of the
algorithms, the inputs, and the hardware, so it is usually ignored for
purposes of algorithmic analysis.  But that doesn't mean you can forget
about it.
\index{crossover point}

If two algorithms have the same leading order term, it is hard to say
which is better; again, the answer depends on the details.  So for
algorithmic analysis, functions with the same leading term
are considered equivalent, even if they have different coefficients.

An {\bf order of growth} is a set of functions whose asymptotic growth
behavior is considered equivalent.  For example, $2n$, $100n$ and $n +
1$ belong to the same order of growth, which is written $O(n)$ in
{\bf Big-Oh notation} and often called {\bf linear} because every function
in the set grows linearly with $n$.
\index{big-oh notation}
\index{linear growth}

All functions with the leading term $n^2$ belong to $O(n^2)$; they are
{\bf quadratic}, which is a fancy word for functions with the
leading term $n^2$.
\index{quadratic growth}

The following table shows some of the orders of growth that
appear most commonly in algorithmic analysis,
in increasing order of badness.
\index{badness}

\begin{tabular}{|r|r|r|}
\hline
Order of     &   Name      \\
growth       &               \\
\hline
$O(1)$             & constant \\
$O(\log_b n)$      & logarithmic (for any $b$) \\
$O(n)$             & linear \\
$O(n \log_b n)$    & ``en log en'' \\
$O(n^2)$           & quadratic     \\
$O(n^3)$           & cubic     \\
$O(c^n)$           & exponential (for any $c$)    \\
\hline
\end{tabular}

For the logarithmic terms, the base of the logarithm doesn't matter;
changing bases is the equivalent of multiplying by a constant, which
doesn't change the order of growth.  Similarly, all exponential
functions belong to the same order of growth regardless of the base of
the exponent.
Exponential functions grow very quickly, so exponential algorithms are
only useful for small problems.
\index{logarithmic growth}
\index{exponential growth}


\begin{exercise}

Read the Wikipedia page on Big-Oh notation at
\url{http://en.wikipedia.org/wiki/Big_O_notation} and
answer the following questions:

\begin{enumerate}

\item What is the order of growth of $n^3 + n^2$?
What about $1000000 n^3 + n^2$?
What about $n^3 + 1000000 n^2$?

\item What is the order of growth of $(n^2 + n) \cdot (n + 1)$?  Before
  you start multiplying, remember that you only need the leading term.

\item If $f$ is in $O(g)$, for some unspecified function $g$, what can
  we say about $a f + b$?

\item If $f_1$ and $f_2$ are in $O(g)$, what can we say about $f_1 + f_2$?

\item If  $f_1$ is in $O(g)$
and $f_2$ is in $O(h)$,
what can we say about  $f_1 + f_2$?

\item If  $f_1$ is in $O(g)$ and $f_2$ is $O(h)$,
what can we say about  $f_1 * f_2$?

\end{enumerate}

\end{exercise}


Programmers who care about performance often find this kind of
analysis hard to swallow.  They have a point: sometimes the
coefficients and the non-leading terms make a real difference.  And
sometimes the details of the hardware, the programming language, and
the characteristics of the input make a big difference.  And for small
problems asymptotic behavior is irrelevant.
\index{practical analysis of algorithms}

But if you keep those caveats in mind, algorithmic analysis is a
useful tool.  At least for large problems, the ``better'' algorithms
is usually better, and sometimes it is {\em much} better.  The
difference between two algorithms with the same order of growth is
usually a constant factor, but the difference between a good algorithm
and a bad algorithm is unbounded!
\index{unbounded}


\section{Analysis of basic Python operations}

Most arithmetic operations are constant time; multiplication
usually takes longer than addition and subtraction, and division
takes even longer, but these run times don't
depend on the magnitude of the operands.  Very large integers
are an exception; in that case the run time increases
with the number of digits.
\index{analysis of primitives}

Indexing operations---reading or writing elements in a sequence
or dictionary---are also constant time, regardless of the size
of the data structure.
\index{indexing}

A \py{for} loop that traverses a sequence or dictionary is
usually linear, as long as all of the operations in the body
of the loop are constant time.  For example, adding up the
elements of a list is linear:

\begin{code}
    total = 0
    for x in t:
        total += x
\end{code}

The built-in function \py{sum} is also linear because it does
the same thing, but it tends to be faster because it is a more
efficient implementation; in the language of algorithmic analysis,
it has a smaller leading coefficient.

If you use the same loop to ``add'' a list of strings, the
run time is quadratic
because string concatenation is linear.
\index{string concatenation}

The string method \py{join} is usually faster because it is
linear in the total length of the strings.
\index{join@\py{join}}

As a rule of thumb, if the body of a loop is in $O(n^a)$ then
the whole loop is in $O(n^{a+1})$.  The exception is if you can
show that the loop exits after a constant number of iterations.
If a loop runs $k$ times regardless of $n$, then
the loop is in $O(n^a)$, even for large $k$.

Multiplying by $k$ doesn't change the order of growth, but neither
does dividing.  So if the body of a loop is in $O(n^a)$ and it runs $n
/ k$ times, the loop is in $O(n^{a+1})$, even for large $k$.

Most string and tuple operations are linear, except indexing and {\tt
  len}, which are constant time.  The built-in functions \py{min} and
\py{max} are linear.  The run-time of a slice operation is
proportional to the length of the output, but independent of the size
of the input.
\index{string methods}
\index{tuple methods}

All string methods are linear, but if the lengths of
the strings are bounded by a constant---for example, operations on single
characters---they are considered constant time.

Most list methods are linear, but there are some exceptions:
\index{list methods}

\begin{itemize}

\item Adding an element to the end of a list is constant time on
average; when it runs out of room it occasionally gets copied
to a bigger location, but the total time for $n$ operations
is $O(n)$, so we say that the ``amortized'' time for one
operation is $O(1)$.

\item Removing an element from the end of a list is constant time.

\item Sorting is $O(n \log n)$.
\index{sorting}

\end{itemize}

Most dictionary operations and methods are constant time, but
there are some exceptions:
\index{dictionary methods}

\begin{itemize}

\item The run time of \py{copy} is proportional to the number of
  elements, but not the size of the elements (it copies references,
  not the elements themselves).

\item The run time of \py{update} is
  proportional to the size of the dictionary passed as a parameter,
  not the dictionary being updated.

\item \py{keys}, \py{values} and \py{items} are linear because they
  return new lists; \py{iterkeys}, \py{itervalues} and {\tt
    iteritems} are constant time because they return iterators.  But
  if you loop through the iterators, the loop will be linear.  Using
  the ``iter'' functions saves some overhead, but it doesn't change
  the order of growth unless the number of items you access is
  bounded.

\end{itemize}

The performance of dictionaries is one of the minor miracles of
computer science.  We will see how they work in
Section~\ref{hashtable}.


\begin{exercise}

Read the Wikipedia page on sorting algorithms at
\url{http://en.wikipedia.org/wiki/Sorting_algorithm} and answer
the following questions:
\index{sorting}

\begin{enumerate}

\item What is a ``comparison sort?'' What is the best worst-case order
  of growth for a comparison sort?  What is the best worst-case order
  of growth for any sort algorithm?
\index{comparison sort}

\item What is the order of growth of bubble sort, and why does Barack
  Obama think it is ``the wrong way to go?''

\item What is the order of growth of radix sort?  What preconditions
  do we need to use it?

\item What is a stable sort and why might it matter in practice?
\index{stable sort}

\item What is the worst sorting algorithm (that has a name)?

\item What sort algorithm does the C library use?  What sort algorithm
  does Python use?  Are these algorithms stable?  You might have to
  Google around to find these answers.

\item Many of the non-comparison sorts are linear, so why does does
  Python use an $O(n \log n)$ comparison sort?

\end{enumerate}

\end{exercise}


\section{Analysis of search algorithms}

A {\bf search} is an algorithm that takes a collection and a target
item and determines whether the target is in the collection, often
returning the index of the target.
\index{search}

The simplest search algorithm is a ``linear search,'' which traverses
the items of the collection in order, stopping if it finds the target.
In the worst case it has to traverse the entire collection, so the run
time is linear.
\index{linear search}

The \py{in} operator for sequences uses a linear search; so do string
methods like \py{find} and \py{count}.
\index{in@\py{in} operator}

If the elements of the sequence are in order, you can use a {\bf
  bisection search}, which is $O(\log n)$.  Bisection search is
similar to the algorithm you probably use to look a word up in a
dictionary (a real dictionary, not the data structure).  Instead of starting at
the beginning and checking each item in order, you start with the item
in the middle and check whether the word you are looking for comes
before or after.  If it comes before, then you search the first half
of the sequence.  Otherwise you search the second half.  Either way,
you cut the number of remaining items in half.  \index{bisection
  search}

If the sequence has 1,000,000 items, it will take about 20 steps to
find the word or conclude that it's not there.  So that's about 50,000
times faster than a linear search.

\begin{exercise}

Write a function called \py{bisection} that takes a sorted list
and a target value and returns the index of the value
in the list, if it's there, or \py{None} if it's not.
\index{bisect@\py{bisect} module}

\index{bisect module}
\index{module!bisect}

Or you could read the documentation of the \py{bisect} module
and use that!

\end{exercise}

Bisection search can be much faster than linear search, but
it requires the sequence to be in order, which might require
extra work.

There is another data structure, called a {\bf hashtable} that
is even faster---it can do a search in constant time---and it
doesn't require the items to be sorted.  Python dictionaries
are implemented using hashtables, which is why most dictionary
operations, including the \py{in} operator, are constant time.


\section{Hashtables}
\label{hashtable}

To explain how hashtables work and why their performance is so
good, I start with a simple implementation of a map and
gradually improve it until it's a hashtable.
\index{hashtable}

I use Python to demonstrate these implementations, but in real
life you wouldn't write code like this in Python; you would just use a
dictionary!  So for the rest of this chapter, you have to imagine that
dictionaries don't exist and you want to implement a data structure
that maps from keys to values.  The operations you have to
implement are:

\begin{description}

\item[\py{add(k, v)}:] Add a new item that maps from key \py{k}
to value \py{v}.  With a Python dictionary, \py{d}, this operation
is written \py{d[k] = v}.

\item[\py{get(target)}:] Look up and return the value that corresponds
to key \py{target}.  With a Python dictionary, \py{d}, this operation
is written \py{d[target]} or \py{d.get(target)}.

\end{description}

For now, I assume that each key only appears once.
The simplest implementation of this interface uses a list of
tuples, where each tuple is a key-value pair.
\index{LinearMap@\py{LinearMap}}

\begin{code}
class LinearMap(object):

    def __init__(self):
        self.items = []

    def add(self, k, v):
        self.items.append((k, v))

    def get(self, k):
        for key, val in self.items:
            if key == k:
                return val
        raise KeyError
\end{code}

\py{add} appends a key-value tuple to the list of items, which
takes constant time.

\py{get} uses a \py{for} loop to search the list:
if it finds the target key it returns the corresponding value;
otherwise it raises a \py{KeyError}.
So \py{get} is linear.
\index{KeyError@\py{KeyError}}

An alternative is to keep the list sorted by key.  Then \py{get}
could use a bisection search, which is $O(\log n)$.  But inserting a
new item in the middle of a list is linear, so this might not be the
best option.  There are other data structures (see
  \url{http://en.wikipedia.org/wiki/Red-black_tree})  that can implement {\tt
  add} and \py{get} in log time, but that's still not as good as
constant time, so let's move on.
\index{red-black tree}

One way to improve \py{LinearMap} is to break the list of key-value
pairs into smaller lists.  Here's an implementation called
\py{BetterMap}, which is a list of 100 LinearMaps.  As we'll see
in a second, the order of growth for \py{get} is still linear,
but \py{BetterMap} is a step on the path toward hashtables:
\index{BetterMap@\py{BetterMap}}

\begin{code}
class BetterMap(object):

    def __init__(self, n=100):
        self.maps = []
        for i in range(n):
            self.maps.append(LinearMap())

    def find_map(self, k):
        index = hash(k) % len(self.maps)
        return self.maps[index]

    def add(self, k, v):
        m = self.find_map(k)
        m.add(k, v)

    def get(self, k):
        m = self.find_map(k)
        return m.get(k)
\end{code}

\py{__init__} makes a list of \py{n} \py{LinearMap}s.

\py{find_map} is used by
\py{add} and \py{get}
to figure out which map to put the
new item in, or which map to search.

\py{find_map} uses the built-in function \py{hash}, which takes
almost any Python object and returns an integer.  A limitation of this
implementation is that it only works with hashable keys.  Mutable
types like lists and dictionaries are unhashable.
\index{hash function}

Hashable objects that are considered equal return the same hash value,
but the converse is not necessarily true: two different objects
can return the same hash value.

\py{find_map} uses the modulus operator to wrap the hash values
into the range from 0 to \py{len(self.maps)}, so the result is a legal
index into the list.  Of course, this means that many different
hash values will wrap onto the same index.  But if the hash function
spreads things out pretty evenly (which is what hash functions
are designed to do), then we expect $n/100$ items per LinearMap.

Since the run time of \py{LinearMap.get} is proportional to the
number of items, we expect BetterMap to be about 100 times faster
than LinearMap.  The order of growth is still linear, but the
leading coefficient is smaller.  That's nice, but still not
as good as a hashtable.

Here (finally) is the crucial idea that makes hashtables fast: if you
can keep the maximum length of the LinearMaps bounded, {\tt
  LinearMap.get} is constant time.  All you have to do is keep track
of the number of items and when the number of
items per LinearMap exceeds a threshold, resize the hashtable by
adding more LinearMaps.
\index{bounded}

Here is an implementation of a hashtable:
\index{HashMap}

\begin{code}
class HashMap(object):

    def __init__(self):
        self.maps = BetterMap(2)
        self.num = 0

    def get(self, k):
        return self.maps.get(k)

    def add(self, k, v):
        if self.num == len(self.maps.maps):
            self.resize()

        self.maps.add(k, v)
        self.num += 1

    def resize(self):
        new_maps = BetterMap(self.num * 2)

        for m in self.maps.maps:
            for k, v in m.items:
                new_maps.add(k, v)

        self.maps = new_maps
\end{code}

Each \py{HashMap} contains a \py{BetterMap}; \py{__init__} starts
with just 2 LinearMaps and initializes \py{num}, which keeps track of
the number of items.

\py{get} just dispatches to \py{BetterMap}.  The real work happens
in \py{add}, which checks the number of items and the size of the
\py{BetterMap}: if they are equal, the average number of items per
LinearMap is 1, so it calls \py{resize}.

\py{resize} make a new \py{BetterMap}, twice as big as the previous
one, and then ``rehashes'' the items from the old map to the new.

Rehashing is necessary because changing the number of LinearMaps
changes the denominator of the modulus operator in
\py{find_map}.  That means that some objects that used
to wrap into the same LinearMap will get split up (which is
what we wanted, right?).
\index{rehashing}

Rehashing is linear, so
\py{resize} is linear, which might seem bad, since I promised
that \py{add} would be constant time.  But remember that
we don't have to resize every time, so \py{add} is usually
constant time and only occasionally linear.  The total amount
of work to run \py{add} $n$ times is proportional to $n$,
so the average time of each \py{add} is constant time!
\index{constant time}

To see how this works, think about starting with an empty
HashTable and adding a sequence of items.  We start with 2 LinearMaps,
so the first 2 adds are fast (no resizing required).  Let's
say that they take one unit of work each.  The next add
requires a resize, so we have to rehash the first two
items (let's call that 2 more units of work) and then
add the third item (one more unit).  Adding the next item
costs 1 unit, so the total so far is
6 units of work for 4 items.

The next \py{add} costs 5 units, but the next three
are only one unit each, so the total is 14 units for the
first 8 adds.

The next \py{add} costs 9 units, but then we can add 7 more
before the next resize, so the total is 30 units for the
first 16 adds.

After 32 adds, the total cost is 62 units, and I hope you are starting
to see a pattern.  After $n$ adds, where $n$ is a power of two, the
total cost is $2n - 2$ units, so the average work per add is
a little less than 2 units.  When $n$ is a power of two, that's
the best case; for other values of $n$ the average work is a little
higher, but that's not important.  The important thing is that it
is $O(1)$.
\index{average cost}

Figure~\ref{fig.hash} shows how this works graphically.  Each
block represents a unit of work.  The columns show the total
work for each add in order from left to right: the first two
\py{adds} cost 1 units, the third costs 3 units, etc.

\begin{figure}
\centerline{\includegraphics[width=5.5in]{figs/towers.pdf}}
\caption{The cost of a hashtable add.\label{fig.hash}}
\end{figure}

The extra work of rehashing appears as a sequence of increasingly
tall towers with increasing space between them.  Now if you knock
over the towers, amortizing the cost of resizing over all
adds, you can see graphically that the total cost after $n$
adds is $2n - 2$.

An important feature of this algorithm is that when we resize the
HashTable it grows geometrically; that is, we multiply the size by a
constant.  If you increase the size
arithmetically---adding a fixed number each time---the average time
per \py{add} is linear.
\index{geometric resizing}

You can download my implementation of HashMap from
\url{thinkcomplex.com/Map.py}, but remember that there
is no reason to use it; if you want a map, just use a Python dictionary.

\begin{exercise}

My implementation of \py{HashMap} accesses the attributes of
\py{BetterMap} directly, which shows poor object-oriented design.
\index{object-oriented design}

\begin{enumerate}

\item The special method \py{__len__} is invoked by the built-in
function \py{len}.  Write a \py{__len__} method for \py{BetterMap}
and use it in \py{add}.
\index{len@\py{\_\_len\_\_}}

\item Use a generator to write \py{BetterMap.iteritems}, and use it
in \py{resize}.
\index{generator}

\end{enumerate}

\end{exercise}


\begin{exercise}

A drawbacks of hashtables is that the elements have to be hashable,
which usually means they have to be immutable.  That's why, in Python,
you can use tuples but not lists as keys in a dictionary.  An
alternative is to use a tree-based map.

Write an implementation of the map interface called
\py{TreeMap} that uses a red-black tree to perform \py{add}
and \py{get} in log time.
\index{red-black tree}
\index{TreeMap@\py{TreeMap}}

\end{exercise}



\section{Summing lists}
\label{growth_experiment}

Suppose you have a bunch of lists and you want to join them up
into a single list.  There are three ways you might do that
in Python:
\index{summing lists}

\begin{itemize}

\item You could use the \py{+=} operator:

\begin{code}
    total = []
    for x in t:
        total += x
\end{code}

\item Or the \py{extend} method:

\begin{code}
    total = []
    for x in t:
        total.extend(x)
\end{code}

\item Or the built-in function \py{sum}:

\begin{code}
    total = sum(t, [])
\end{code}

The second argument to \py{sum} is the initial value for the total.

\end{itemize}

Without knowing how \py{+=} and \py{extend} and \py{sum} are
implemented, it is hard to analyze their performance.  For example,
if \py{total += x} creates a new list every time, the loop
is quadratic; but if it modifies \py{total}, it's linear.
\index{+=@\py{+=} operator}
\index{extend@\py{extend}}
\index{sum@\py{sum}}

To find out, we could read the source code, but as an exercise, let's see
if we can figure it out by measuring run times.

A simple way to measure the run time of a program is to use
the function \py{times} in the \py{os} module, which returns
a tuple of floats indicating the time your process has used
(see the documentation for details).  I use a function, \py{etime},
which returns the sum of ``user time'' and ``system time'' which
is usually what we care about for performance measurement:
\index{os module@\py{os} module}
\index{user time}
\index{system time}

\begin{code}
import os

def etime():
    """See how much user and system time this process has used
    so far and return the sum."""

    user, sys, chuser, chsys, real = os.times()
    return user+sys
\end{code}

To measure the elapsed time of a function you can call
\py{etime} twice and compute the difference:

\begin{code}
    start = etime()

    # put the code you want to measure here

    end = etime()
    elapsed = end - start
\end{code}

Alternatively, if you use IPython, you can use the
\py{timeit} command. See \url{ipython.scipy.org}.
\index{IPython}
\index{timeit@\py{timeit}}

If an algorithm is quadratic, we expect the run time, $t$
as a function of input size, $n$, to look like this:
\index{quadratic}

\[ t = a n^2 + b n + c \]

Where $a$, $b$ and $c$ are unknown coefficients.  If you take
the log of both sides you get:

\[ \log t \sim \log a + 2 \log n \]

For large values of $n$, the non-leading terms are insignificant
and this approximation is pretty good.  So if we plot $t$
versus $n$ on a log-log scale, we expect a straight line
with slope 2.
\index{log-log scale}

Similarly if the algorithm is linear, we expect a line with
slope 1.
\index{linear}

I wrote three functions that concatenate lists: \py{sum_plus} uses
\py{+=}; \py{sum_extend} uses \py{list.extend}; and \py{sum_sum}
uses \py{sum}.  I timed them for a range of \py{n} and plotted the
results on a log-log scale.  Figures~\ref{listsum1} and \ref{listsum2}
show the results.

\begin{figure}
\centerline{\includegraphics[height=2.5in]{figs/listsum1.pdf}}
\caption{Runtime versus \py{n}.  The dashed lines have slope 1.\label{listsum1}}
\end{figure}

\begin{figure}
\centerline{\includegraphics[height=2.5in]{figs/listsum2.pdf}}
\caption{Runtime versus \py{n}.  The dashed line has slope 2.\label{listsum2}}
\end{figure}

In Figure~\ref{listsum1} I fit a line with slope 1 to the curves.
The data fit this line well, so we conclude
that these implementations are linear.  The implementation for \py{+=}
is faster by a constant factor because it takes some time
to look up the \py{extend} method each time through the loop.

In Figure~\ref{listsum2} the data fit a line with slope 2, so the
implementation of \py{sum} is quadratic.
\index{quadratic}


\section{\py{pyplot}}
\index{pyplot@\py{pyplot}}
\label{pyplot}

To make the figures in this section I used \py{pyplot}, which is part of
\py{matplotlib}.  If \py{matplotlib} is not part of your Python
installation, you might have to install it, or you can use another
library to make plots.
\index{pyplot}
\index{matplotlib}

Here's an example that makes a simple plot:

\begin{code}
import matplotlib.pyplot as pyplot

pyplot.plot(xs, ys)
scale = 'log'
pyplot.xscale(scale)
pyplot.yscale(scale)
pyplot.title('')
pyplot.xlabel('n')
pyplot.ylabel('run time (s)')
pyplot.show()
\end{code}

The import statement makes \py{matplotlib.pyplot} accessible
with the shorter name \py{pyplot}.

\py{plot} takes a list of $x$-values and a list of $y$-values and
plots them.  The lists have to have the same length.
\py{xscale} and \py{yscale} make the axes either linear or logarithmic.

\py{title}, \py{xlabel} and \py{ylabel} are self-explanatory.
Finally, \py{show} displays the plot on the screen.  You could also
use \py{savefig} to save the plot in a file.

Documentation of \py{pyplot} is at \url{http://matplotlib.sourceforge.net/}.


\begin{exercise}

Test the performance of
\py{LinearMap}, \py{BetterMap} and \py{HashMap}; see if you
can characterize their order of growth.

You can download my map implementations from
\url{thinkcomplex.com/Map.py}, and the code I used in this section
from \url{thinkcomplex.com/listsum.py}.

You will have to find a range
of \py{n} that is big enough to show asymptotic behavior, but small
enough to run quickly.

\end{exercise}




\chapter{Call for submissions}
\label{submissions}

The case studies in this book were written by students at Olin
College, and edited by Lisa Downey and Allen Downey.  They
were reviewed by a program committee of faculty at Olin College
who chose the ones that met the criteria of interest and quality.
I am grateful to the program committee and the students.

I invite readers to submit additional case studies.  Reports
that meet the criteria will be published in an online supplement
to this book, and the best of them will be included in future
print editions.

The criteria are:

\begin{itemize}

\item The case study should be relevant to complexity.  For an
  overview of possible topics, see
  \url{http://en.wikipedia.org/wiki/Complexity} and
  \url{http://en.wikipedia.org/wiki/Complex_systems}.  Topics
  not already covered in the book are particularly welcome.

\item A good case study might present a seminal paper, reimplement
  an important experiment, discuss the results, and explain their
  context.  Original research is not necessary, and might not
  be appropriate for this format, but you could extend existing
  results.

\item A good case study should invite the reader to participate
  by including exercises, references to further reading, and
  topics for discussion.

\item The case study should present enough technical detail that
   reader could implement the model in a language like Python.

\item If you use an algorithm or data structure that deserves comment,
  you should discuss it.  Topics not covered in the book, including
  tree algorithms and dynamic programming, are particularly welcome.

\item If you use a feature of Python or a module that you think
  will interest readers, you should discuss it.  But you should
  stick to modules that are in widespread use and either included
  in Python distributions or easy to install.

\end{itemize}

For more details, see \url{thinkcomplex.com/case_studies}.


\chapter{Reading list}
\label{reading}

In my class we start the semester by reading popular books about
complexity science.  They provide a pleasant way to get a big picture
of the field and start to see connections between topics.

One problem with these books is that
they are written for a non-technical audience, so after a while
the students get frustrated by the vagueness and hand-waving.
That's what {\em this} book is for.

The other problem is that students can't read 30 books in a week, or
even a semester.  Instead, I provide
one copy of each book and ask the students to pick one, read
the first chapter, write a summary, and post it on the class web page.

During the next class session, the students swap books.  Each student
reads the summary written by the previous student, then reads the
next chapter, and writes a summary.

After a few iterations, we have a class discussion where students
report what they have read so far and we look for connections.  For
example, one student might present a topic, then another student
suggests a related topic and explains the relationship.  I draw the
topics and the relationships between them on the board (see
\url{http://en.wikipedia.org/wiki/Concept_map}).

We repeat this exercise until we have seen what there is to see, or we
are impatient to get down to business.

You can see the list of books, and read the summaries my students
wrote, at \url{https://sites.google.com/site/compmodolin}.

Here are the books:

\begin{itemize}

\item Axelrod, {\it  Complexity of Cooperation}.

\item Axelrod, {\it  The Evolution of Cooperation}.

\item Bak, {\it  How Nature Works}.

\item Barabasi, {\it  Linked}.

\item Buchanan, {\it  Nexus}.

\item Epstein and Axtell, {\it  Growing Artificial Societies: Social Science from the Bottom Up}.

\item Fisher, {\it  The Perfect Swarm}.

\item Flake, {\it  The Computational Beauty of Nature}.

\item Goodwin, {\it  How the Leopard Changed Its Spots}.

\item Holland, {\it  Hidden Order}.

\item Johnson, {\it  Emergence}.

\item Kelly, {\it  Out of Control}.

\item Kluger, {\it  Simplexity}.

\item Levy, {\it  Artificial Life}.

\item Lewin, {\it  Complexity: Life at the Edge of Chaos}.

\item Mitchell, {\it  Complexity: A Guided Tour}.

\item Mitchell Waldrop: Complexity, {\it  the emerging science at the edge of order and chaos}.

\item Resnick, {\it  Turtles, Termites, and Traffic Jams}.

\item Rucker, {\it  The Lifebox, The Seashell, and the Soul}.

\item Sawyer, {\it  Social Emergence: Societies As Complex Systems}.

\item Schelling, {\it  Micromotives and Macrobehaviors}.

\item Schiff, {\it  Cellular Automata: A Discrete View of the World}.

\item Strogatz, {\it  Sync}.

\item Watts, {\it  Six Degrees}.

\item Wolfram, {\it  A New Kind Of Science}.

\end{itemize}



\backmatter
\printindex

\end{document}
